{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhesu157/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train.txt'\n",
    "valid_path = 'valid.txt'\n",
    "test_path = 'test.txt'\n",
    "\n",
    "# Load data\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            data.append(line.strip())\n",
    "        return data\n",
    "\n",
    "train_data = load_data(train_path)\n",
    "valid_data = load_data(valid_path)\n",
    "test_data = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tokens: 42068\n",
      "Number of validation tokens: 3370\n",
      "Number of test tokens: 3761\n",
      "Sample: the N <unk> funds increased $ N billion to $ N billion while N broker-dealer funds increased $ N billion to $ N billion\n",
      "Sample: about N N of picop is publicly traded and other shareholders own the rest of the equity\n",
      "Sample: a financial adviser for revco bondholders david <unk> of <unk> partners had mixed reactions to the offer\n",
      "Sample: jacobs is an international engineering and construction concern\n",
      "Sample: on the <unk> streets of <unk> mahfouz 's cairo life is nasty <unk> and <unk> entertaining\n",
      "First 10 training tokens: ['aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter', 'pierre <unk> N years old will join the board as a nonexecutive director nov. N', 'mr. <unk> is chairman of <unk> n.v. the dutch publishing group', 'rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate', 'a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported', 'the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said', '<unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N', \"although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem\", 'a <unk> <unk> said this is an old story', \"we 're talking about years ago before anyone heard of asbestos having any questionable properties\"]\n"
     ]
    }
   ],
   "source": [
    "print('Number of training tokens:', len(train_data))\n",
    "print('Number of validation tokens:', len(valid_data))\n",
    "print('Number of test tokens:', len(test_data))\n",
    "\n",
    "for i in range(5):\n",
    "    random_index = np.random.randint(0, len(train_data))\n",
    "    print('Sample:', train_data[random_index])\n",
    "    \n",
    "print('First 10 training tokens:', train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training tokens: ['aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos>', 'pierre <unk> N years old will join the board as a nonexecutive director nov. N <eos>', 'mr. <unk> is chairman of <unk> n.v. the dutch publishing group <eos>', 'rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate <eos>', 'a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported <eos>', 'the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said <eos>', '<unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N <eos>', \"although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem <eos>\", 'a <unk> <unk> said this is an old story <eos>', \"we 're talking about years ago before anyone heard of asbestos having any questionable properties <eos>\"]\n"
     ]
    }
   ],
   "source": [
    "#1.3\n",
    "def add_eos(data):\n",
    "    return [line + ' <eos>' for line in data]\n",
    "\n",
    "train_data = add_eos(train_data)\n",
    "valid_data = add_eos(valid_data)\n",
    "test_data = add_eos(test_data)\n",
    "\n",
    "print('First 10 training tokens:', train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training words: 929589\n",
      "Number of validation words: 73760\n",
      "Number of test words: 82430\n"
     ]
    }
   ],
   "source": [
    "#1.4\n",
    "def calculate_words_number(data):\n",
    "    return len(' '.join(data).split(' '))\n",
    "\n",
    "print('Number of training words:', calculate_words_number(train_data))\n",
    "print('Number of validation words:', calculate_words_number(valid_data))\n",
    "print('Number of test words:', calculate_words_number(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 1085779\n"
     ]
    }
   ],
   "source": [
    "total_number = calculate_words_number(train_data) + calculate_words_number(valid_data) + calculate_words_number(test_data)\n",
    "print('Total number of words:', total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 10000\n"
     ]
    }
   ],
   "source": [
    "#1.5\n",
    "def freq_dict(data):\n",
    "    words = ' '.join(data).split(' ')\n",
    "    unique, counts = np.unique(words, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "dict_all = freq_dict(train_data+valid_data+test_data)\n",
    "print('Number of unique words:', len(dict_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ' '.join(train_data).split(' ')\n",
    "valid_data = ' '.join(valid_data).split(' ')\n",
    "test_data = ' '.join(test_data).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in train: 10000\n",
      "Number of unique words in valid: 6022\n",
      "Number of unique words in test: 6049\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words in train:', len(freq_dict(train_data)))\n",
    "print('Number of unique words in valid:', len(freq_dict(valid_data)))\n",
    "print('Number of unique words in test:', len(freq_dict(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(data):\n",
    "    vocab = {}\n",
    "    for word in data:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Convert data to indices\n",
    "def convert_to_indices(data, vocab):\n",
    "    return [vocab[word] for word in data]\n",
    "\n",
    "# Create batches\n",
    "def create_batches(data, batch_size, seq_len):\n",
    "    num_batches = len(data) // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = np.array(data)\n",
    "\n",
    "    return data.reshape(batch_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "data_all = train_data + valid_data + test_data\n",
    "vocab = build_vocab(data_all)\n",
    "int_to_word = dict((i, c) for i, c in enumerate(vocab))\n",
    "\n",
    "# Convert data to indices\n",
    "train_data = convert_to_indices(train_data, vocab)\n",
    "valid_data = convert_to_indices(valid_data, vocab)\n",
    "test_data = convert_to_indices(test_data, vocab)\n",
    "\n",
    "# Create batches\n",
    "batch_size = 20\n",
    "seq_len = 30\n",
    "train_data = create_batches(train_data, batch_size, seq_len)\n",
    "valid_data = create_batches(valid_data, batch_size, seq_len)\n",
    "test_data = create_batches(test_data, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def train(model, model_type, train_data, valid_data, vocab_size, seq_len, batch_size, num_epochs, learning_rate, device, clip=0):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_perplexities = []\n",
    "    valid_perplexities = []\n",
    "\n",
    "    num_batches = len(train_data[0]) // seq_len\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = []\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for i in range(0, len(train_data[0,:]) - seq_len, seq_len):\n",
    "            inputs = torch.from_numpy(train_data[:,i:i+seq_len]).to(device)\n",
    "            targets = torch.from_numpy(train_data[:,(i+1):(i+1)+seq_len]).to(device)\n",
    "\n",
    "            if model_type == 'LSTM':\n",
    "                hidden = [h.data.to(device) for h in hidden]\n",
    "            else:\n",
    "                hidden = hidden.data.to(device)\n",
    "\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip != 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())  \n",
    "            \n",
    "            step = (i+1) // seq_len\n",
    "            if step % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                        .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
    "                \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        train_perplexities.append(np.exp(train_loss))\n",
    "        \n",
    "        valid_loss = []\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(valid_data[0,:]) - seq_len, seq_len):\n",
    "                inputs = torch.from_numpy(valid_data[:,i:i+seq_len]).to(device)\n",
    "                targets = torch.from_numpy(valid_data[:,(i+1):(i+1)+seq_len]).to(device)\n",
    "\n",
    "                if model_type == 'LSTM':\n",
    "                    hidden = [h.data.to(device) for h in hidden]\n",
    "                else:\n",
    "                    hidden = hidden.data.to(device)\n",
    "                    \n",
    "                outputs, hidden = model(inputs, hidden)\n",
    "                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                valid_loss.append(loss.item())\n",
    "                \n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_perplexities.append(np.exp(valid_loss))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'train_loss =', '{:.4f}'.format(train_loss),\n",
    "                'valid_loss =', '{:.4f}'.format(valid_loss),\n",
    "                'time =', '{:.4f}'.format(end_time - start_time))\n",
    "        \n",
    "    return train_losses, valid_losses, train_perplexities, valid_perplexities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate\n",
    "def evaluate(model, model_type, test_data, vocab_size, batch_size, seq_len, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = []\n",
    "    test_perplexity = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_data[0]) - seq_len, seq_len):\n",
    "            inputs = torch.from_numpy(train_data[:,i:i+seq_len]).to(device)\n",
    "            targets = torch.from_numpy(train_data[:,(i+1):(i+1)+seq_len]).to(device)\n",
    "\n",
    "            if model_type == 'LSTM':\n",
    "                hidden = [h.data.to(device) for h in hidden]\n",
    "            else:\n",
    "                hidden = hidden.data.to(device)\n",
    "\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "    return test_loss, test_perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "def predict(model, hidden, model_type, character, int_to_word, vocab, stategy, device):\n",
    "\n",
    "    character = np.array([[vocab[c] for c in character]])\n",
    "    character = torch.from_numpy(character).to(device)\n",
    "    \n",
    "    out, hidden = model(character, hidden)\n",
    "\n",
    "    if stategy == 'greedy':\n",
    "        prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "        # Taking the class with the highest probability score from the output\n",
    "        char_ind = torch.max(prob, dim=1)[-1][-1].item()\n",
    "    elif stategy == 'sampling':\n",
    "        word_weights = out[-1].squeeze().exp().cpu()\n",
    "        char_ind = torch.multinomial(word_weights, num_samples=1)[0].item()\n",
    "\n",
    "    return int_to_word[char_ind], hidden\n",
    "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
    "def sample(model, model_type, int_to_word, out_len, vocab, device, start='however director'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start.split()]\n",
    "    size = out_len - len(chars)\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "        hidden = [h.data.to(device) for h in hidden]\n",
    "    else:\n",
    "        hidden = hidden.data.to(device)\n",
    "\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "\n",
    "        char, hidden = predict(model, hidden, model_type, chars, int_to_word, vocab, device)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ' '.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMAN_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super(ELMAN_RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[0/1549], Loss: 9.2240, Perplexity: 10137.49\n",
      "Epoch [1/10], Step[100/1549], Loss: 6.4339, Perplexity: 622.60\n",
      "Epoch [1/10], Step[200/1549], Loss: 6.4812, Perplexity: 652.73\n",
      "Epoch [1/10], Step[300/1549], Loss: 6.5048, Perplexity: 668.32\n",
      "Epoch [1/10], Step[400/1549], Loss: 6.3203, Perplexity: 555.76\n",
      "Epoch [1/10], Step[500/1549], Loss: 6.0273, Perplexity: 414.59\n",
      "Epoch [1/10], Step[600/1549], Loss: 5.9497, Perplexity: 383.62\n",
      "Epoch [1/10], Step[700/1549], Loss: 6.2516, Perplexity: 518.85\n",
      "Epoch [1/10], Step[800/1549], Loss: 5.9787, Perplexity: 394.91\n",
      "Epoch [1/10], Step[900/1549], Loss: 5.9935, Perplexity: 400.81\n",
      "Epoch [1/10], Step[1000/1549], Loss: 5.9739, Perplexity: 393.04\n",
      "Epoch [1/10], Step[1100/1549], Loss: 6.1279, Perplexity: 458.47\n",
      "Epoch [1/10], Step[1200/1549], Loss: 5.9351, Perplexity: 378.07\n",
      "Epoch [1/10], Step[1300/1549], Loss: 6.0056, Perplexity: 405.71\n",
      "Epoch [1/10], Step[1400/1549], Loss: 5.7784, Perplexity: 323.24\n",
      "Epoch [1/10], Step[1500/1549], Loss: 5.9811, Perplexity: 395.89\n",
      "Epoch: 0001 train_loss = 6.1665 valid_loss = 5.8134 time = 39.6110\n",
      "Epoch [2/10], Step[0/1549], Loss: 6.4187, Perplexity: 613.18\n",
      "Epoch [2/10], Step[100/1549], Loss: 5.6896, Perplexity: 295.76\n",
      "Epoch [2/10], Step[200/1549], Loss: 5.7850, Perplexity: 325.37\n",
      "Epoch [2/10], Step[300/1549], Loss: 5.9156, Perplexity: 370.79\n",
      "Epoch [2/10], Step[400/1549], Loss: 5.8260, Perplexity: 338.98\n",
      "Epoch [2/10], Step[500/1549], Loss: 5.4214, Perplexity: 226.18\n",
      "Epoch [2/10], Step[600/1549], Loss: 5.4903, Perplexity: 242.32\n",
      "Epoch [2/10], Step[700/1549], Loss: 5.8093, Perplexity: 333.40\n",
      "Epoch [2/10], Step[800/1549], Loss: 5.5747, Perplexity: 263.66\n",
      "Epoch [2/10], Step[900/1549], Loss: 5.5075, Perplexity: 246.54\n",
      "Epoch [2/10], Step[1000/1549], Loss: 5.5726, Perplexity: 263.12\n",
      "Epoch [2/10], Step[1100/1549], Loss: 5.7984, Perplexity: 329.76\n",
      "Epoch [2/10], Step[1200/1549], Loss: 5.6015, Perplexity: 270.82\n",
      "Epoch [2/10], Step[1300/1549], Loss: 5.6173, Perplexity: 275.15\n",
      "Epoch [2/10], Step[1400/1549], Loss: 5.4471, Perplexity: 232.08\n",
      "Epoch [2/10], Step[1500/1549], Loss: 5.6803, Perplexity: 293.03\n",
      "Epoch: 0002 train_loss = 5.6143 valid_loss = 5.5707 time = 39.3782\n",
      "Epoch [3/10], Step[0/1549], Loss: 5.9812, Perplexity: 395.93\n",
      "Epoch [3/10], Step[100/1549], Loss: 5.4066, Perplexity: 222.87\n",
      "Epoch [3/10], Step[200/1549], Loss: 5.5018, Perplexity: 245.14\n",
      "Epoch [3/10], Step[300/1549], Loss: 5.6068, Perplexity: 272.26\n",
      "Epoch [3/10], Step[400/1549], Loss: 5.5792, Perplexity: 264.87\n",
      "Epoch [3/10], Step[500/1549], Loss: 5.1511, Perplexity: 172.61\n",
      "Epoch [3/10], Step[600/1549], Loss: 5.2349, Perplexity: 187.72\n",
      "Epoch [3/10], Step[700/1549], Loss: 5.5487, Perplexity: 256.90\n",
      "Epoch [3/10], Step[800/1549], Loss: 5.3714, Perplexity: 215.16\n",
      "Epoch [3/10], Step[900/1549], Loss: 5.2670, Perplexity: 193.83\n",
      "Epoch [3/10], Step[1000/1549], Loss: 5.3399, Perplexity: 208.50\n",
      "Epoch [3/10], Step[1100/1549], Loss: 5.5757, Perplexity: 263.93\n",
      "Epoch [3/10], Step[1200/1549], Loss: 5.4031, Perplexity: 222.09\n",
      "Epoch [3/10], Step[1300/1549], Loss: 5.3501, Perplexity: 210.62\n",
      "Epoch [3/10], Step[1400/1549], Loss: 5.2214, Perplexity: 185.20\n",
      "Epoch [3/10], Step[1500/1549], Loss: 5.4591, Perplexity: 234.89\n",
      "Epoch: 0003 train_loss = 5.3605 valid_loss = 5.4442 time = 39.6055\n",
      "Epoch [4/10], Step[0/1549], Loss: 5.6241, Perplexity: 277.03\n",
      "Epoch [4/10], Step[100/1549], Loss: 5.2294, Perplexity: 186.68\n",
      "Epoch [4/10], Step[200/1549], Loss: 5.3043, Perplexity: 201.21\n",
      "Epoch [4/10], Step[300/1549], Loss: 5.4109, Perplexity: 223.84\n",
      "Epoch [4/10], Step[400/1549], Loss: 5.3586, Perplexity: 212.42\n",
      "Epoch [4/10], Step[500/1549], Loss: 4.9441, Perplexity: 140.35\n",
      "Epoch [4/10], Step[600/1549], Loss: 5.0419, Perplexity: 154.77\n",
      "Epoch [4/10], Step[700/1549], Loss: 5.3733, Perplexity: 215.57\n",
      "Epoch [4/10], Step[800/1549], Loss: 5.2140, Perplexity: 183.82\n",
      "Epoch [4/10], Step[900/1549], Loss: 5.0786, Perplexity: 160.54\n",
      "Epoch [4/10], Step[1000/1549], Loss: 5.1683, Perplexity: 175.61\n",
      "Epoch [4/10], Step[1100/1549], Loss: 5.4009, Perplexity: 221.60\n",
      "Epoch [4/10], Step[1200/1549], Loss: 5.2541, Perplexity: 191.34\n",
      "Epoch [4/10], Step[1300/1549], Loss: 5.1372, Perplexity: 170.23\n",
      "Epoch [4/10], Step[1400/1549], Loss: 5.0371, Perplexity: 154.03\n",
      "Epoch [4/10], Step[1500/1549], Loss: 5.2733, Perplexity: 195.05\n",
      "Epoch: 0004 train_loss = 5.1729 valid_loss = 5.3588 time = 39.7764\n",
      "Epoch [5/10], Step[0/1549], Loss: 5.3892, Perplexity: 219.03\n",
      "Epoch [5/10], Step[100/1549], Loss: 5.0431, Perplexity: 154.95\n",
      "Epoch [5/10], Step[200/1549], Loss: 5.1367, Perplexity: 170.16\n",
      "Epoch [5/10], Step[300/1549], Loss: 5.2326, Perplexity: 187.27\n",
      "Epoch [5/10], Step[400/1549], Loss: 5.1667, Perplexity: 175.33\n",
      "Epoch [5/10], Step[500/1549], Loss: 4.7945, Perplexity: 120.84\n",
      "Epoch [5/10], Step[600/1549], Loss: 4.9038, Perplexity: 134.80\n",
      "Epoch [5/10], Step[700/1549], Loss: 5.1995, Perplexity: 181.19\n",
      "Epoch [5/10], Step[800/1549], Loss: 5.0833, Perplexity: 161.31\n",
      "Epoch [5/10], Step[900/1549], Loss: 4.9198, Perplexity: 136.98\n",
      "Epoch [5/10], Step[1000/1549], Loss: 5.0231, Perplexity: 151.88\n",
      "Epoch [5/10], Step[1100/1549], Loss: 5.2654, Perplexity: 193.53\n",
      "Epoch [5/10], Step[1200/1549], Loss: 5.1215, Perplexity: 167.58\n",
      "Epoch [5/10], Step[1300/1549], Loss: 4.9543, Perplexity: 141.78\n",
      "Epoch [5/10], Step[1400/1549], Loss: 4.8778, Perplexity: 131.35\n",
      "Epoch [5/10], Step[1500/1549], Loss: 5.1212, Perplexity: 167.53\n",
      "Epoch: 0005 train_loss = 5.0146 valid_loss = 5.2944 time = 39.8056\n",
      "Epoch [6/10], Step[0/1549], Loss: 5.1912, Perplexity: 179.69\n",
      "Epoch [6/10], Step[100/1549], Loss: 4.8860, Perplexity: 132.43\n",
      "Epoch [6/10], Step[200/1549], Loss: 4.9758, Perplexity: 144.86\n",
      "Epoch [6/10], Step[300/1549], Loss: 5.0855, Perplexity: 161.67\n",
      "Epoch [6/10], Step[400/1549], Loss: 5.0044, Perplexity: 149.07\n",
      "Epoch [6/10], Step[500/1549], Loss: 4.6119, Perplexity: 100.67\n",
      "Epoch [6/10], Step[600/1549], Loss: 4.7965, Perplexity: 121.09\n",
      "Epoch [6/10], Step[700/1549], Loss: 5.0435, Perplexity: 155.01\n",
      "Epoch [6/10], Step[800/1549], Loss: 4.9764, Perplexity: 144.95\n",
      "Epoch [6/10], Step[900/1549], Loss: 4.7824, Perplexity: 119.39\n",
      "Epoch [6/10], Step[1000/1549], Loss: 4.8980, Perplexity: 134.03\n",
      "Epoch [6/10], Step[1100/1549], Loss: 5.1425, Perplexity: 171.15\n",
      "Epoch [6/10], Step[1200/1549], Loss: 5.0115, Perplexity: 150.12\n",
      "Epoch [6/10], Step[1300/1549], Loss: 4.8017, Perplexity: 121.71\n",
      "Epoch [6/10], Step[1400/1549], Loss: 4.7346, Perplexity: 113.81\n",
      "Epoch [6/10], Step[1500/1549], Loss: 4.9844, Perplexity: 146.11\n",
      "Epoch: 0006 train_loss = 4.8773 valid_loss = 5.2488 time = 39.9122\n",
      "Epoch [7/10], Step[0/1549], Loss: 5.0172, Perplexity: 150.99\n",
      "Epoch [7/10], Step[100/1549], Loss: 4.7536, Perplexity: 116.01\n",
      "Epoch [7/10], Step[200/1549], Loss: 4.8573, Perplexity: 128.68\n",
      "Epoch [7/10], Step[300/1549], Loss: 4.9628, Perplexity: 143.00\n",
      "Epoch [7/10], Step[400/1549], Loss: 4.8631, Perplexity: 129.42\n",
      "Epoch [7/10], Step[500/1549], Loss: 4.4813, Perplexity: 88.35\n",
      "Epoch [7/10], Step[600/1549], Loss: 4.7072, Perplexity: 110.74\n",
      "Epoch [7/10], Step[700/1549], Loss: 4.9193, Perplexity: 136.91\n",
      "Epoch [7/10], Step[800/1549], Loss: 4.8718, Perplexity: 130.55\n",
      "Epoch [7/10], Step[900/1549], Loss: 4.6625, Perplexity: 105.90\n",
      "Epoch [7/10], Step[1000/1549], Loss: 4.7897, Perplexity: 120.27\n",
      "Epoch [7/10], Step[1100/1549], Loss: 5.0354, Perplexity: 153.77\n",
      "Epoch [7/10], Step[1200/1549], Loss: 4.8983, Perplexity: 134.06\n",
      "Epoch [7/10], Step[1300/1549], Loss: 4.6776, Perplexity: 107.51\n",
      "Epoch [7/10], Step[1400/1549], Loss: 4.6187, Perplexity: 101.37\n",
      "Epoch [7/10], Step[1500/1549], Loss: 4.8804, Perplexity: 131.68\n",
      "Epoch: 0007 train_loss = 4.7576 valid_loss = 5.2186 time = 42.2598\n",
      "Epoch [8/10], Step[0/1549], Loss: 4.8648, Perplexity: 129.65\n",
      "Epoch [8/10], Step[100/1549], Loss: 4.6320, Perplexity: 102.72\n",
      "Epoch [8/10], Step[200/1549], Loss: 4.7385, Perplexity: 114.27\n",
      "Epoch [8/10], Step[300/1549], Loss: 4.8429, Perplexity: 126.84\n",
      "Epoch [8/10], Step[400/1549], Loss: 4.7377, Perplexity: 114.18\n",
      "Epoch [8/10], Step[500/1549], Loss: 4.3604, Perplexity: 78.29\n",
      "Epoch [8/10], Step[600/1549], Loss: 4.6121, Perplexity: 100.70\n",
      "Epoch [8/10], Step[700/1549], Loss: 4.7983, Perplexity: 121.31\n",
      "Epoch [8/10], Step[800/1549], Loss: 4.7862, Perplexity: 119.84\n",
      "Epoch [8/10], Step[900/1549], Loss: 4.5456, Perplexity: 94.22\n",
      "Epoch [8/10], Step[1000/1549], Loss: 4.7005, Perplexity: 110.01\n",
      "Epoch [8/10], Step[1100/1549], Loss: 4.9318, Perplexity: 138.63\n",
      "Epoch [8/10], Step[1200/1549], Loss: 4.7858, Perplexity: 119.80\n",
      "Epoch [8/10], Step[1300/1549], Loss: 4.5534, Perplexity: 94.96\n",
      "Epoch [8/10], Step[1400/1549], Loss: 4.4949, Perplexity: 89.56\n",
      "Epoch [8/10], Step[1500/1549], Loss: 4.7767, Perplexity: 118.71\n",
      "Epoch: 0008 train_loss = 4.6449 valid_loss = 5.1975 time = 42.3399\n",
      "Epoch [9/10], Step[0/1549], Loss: 4.7387, Perplexity: 114.28\n",
      "Epoch [9/10], Step[100/1549], Loss: 4.5216, Perplexity: 91.98\n",
      "Epoch [9/10], Step[200/1549], Loss: 4.6226, Perplexity: 101.76\n",
      "Epoch [9/10], Step[300/1549], Loss: 4.7253, Perplexity: 112.77\n",
      "Epoch [9/10], Step[400/1549], Loss: 4.6263, Perplexity: 102.13\n",
      "Epoch [9/10], Step[500/1549], Loss: 4.2497, Perplexity: 70.09\n",
      "Epoch [9/10], Step[600/1549], Loss: 4.5325, Perplexity: 92.99\n",
      "Epoch [9/10], Step[700/1549], Loss: 4.6911, Perplexity: 108.98\n",
      "Epoch [9/10], Step[800/1549], Loss: 4.6984, Perplexity: 109.77\n",
      "Epoch [9/10], Step[900/1549], Loss: 4.4343, Perplexity: 84.29\n",
      "Epoch [9/10], Step[1000/1549], Loss: 4.6012, Perplexity: 99.60\n",
      "Epoch [9/10], Step[1100/1549], Loss: 4.8272, Perplexity: 124.86\n",
      "Epoch [9/10], Step[1200/1549], Loss: 4.6846, Perplexity: 108.26\n",
      "Epoch [9/10], Step[1300/1549], Loss: 4.4393, Perplexity: 84.72\n",
      "Epoch [9/10], Step[1400/1549], Loss: 4.3780, Perplexity: 79.68\n",
      "Epoch [9/10], Step[1500/1549], Loss: 4.6744, Perplexity: 107.17\n",
      "Epoch: 0009 train_loss = 4.5399 valid_loss = 5.1967 time = 43.2027\n",
      "Epoch [10/10], Step[0/1549], Loss: 4.6396, Perplexity: 103.50\n",
      "Epoch [10/10], Step[100/1549], Loss: 4.4128, Perplexity: 82.50\n",
      "Epoch [10/10], Step[200/1549], Loss: 4.5288, Perplexity: 92.64\n",
      "Epoch [10/10], Step[300/1549], Loss: 4.6298, Perplexity: 102.50\n",
      "Epoch [10/10], Step[400/1549], Loss: 4.5219, Perplexity: 92.01\n",
      "Epoch [10/10], Step[500/1549], Loss: 4.1591, Perplexity: 64.01\n",
      "Epoch [10/10], Step[600/1549], Loss: 4.4688, Perplexity: 87.25\n",
      "Epoch [10/10], Step[700/1549], Loss: 4.5899, Perplexity: 98.48\n",
      "Epoch [10/10], Step[800/1549], Loss: 4.6179, Perplexity: 101.28\n",
      "Epoch [10/10], Step[900/1549], Loss: 4.3334, Perplexity: 76.20\n",
      "Epoch [10/10], Step[1000/1549], Loss: 4.5349, Perplexity: 93.22\n",
      "Epoch [10/10], Step[1100/1549], Loss: 4.7302, Perplexity: 113.31\n",
      "Epoch [10/10], Step[1200/1549], Loss: 4.5999, Perplexity: 99.47\n",
      "Epoch [10/10], Step[1300/1549], Loss: 4.3364, Perplexity: 76.43\n",
      "Epoch [10/10], Step[1400/1549], Loss: 4.2656, Perplexity: 71.20\n",
      "Epoch [10/10], Step[1500/1549], Loss: 4.5735, Perplexity: 96.88\n",
      "Epoch: 0010 train_loss = 4.4431 valid_loss = 5.2047 time = 44.7209\n"
     ]
    }
   ],
   "source": [
    "# train elman rnn\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "model = ELMAN_RNN(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "train_losses, valid_losses, train_perplexities, valid_perplexities = train(model=model,\n",
    "                                                                            model_type='ELMANRNN',\n",
    "                                                                            train_data=train_data,\n",
    "                                                                            valid_data=valid_data,\n",
    "                                                                            vocab_size=vocab_size, \n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            seq_len=seq_len,\n",
    "                                                                            num_epochs=num_epochs, \n",
    "                                                                            learning_rate=learning_rate, \n",
    "                                                                            device=device\n",
    "                                                                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 4.4092 test_perplexity = 82.2073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'however director thomas timing lynn to especially a future at chairman which management r. in of replacement to that both recently an system of in allegations during <eos> <eos> however ultimately of he some the that described the a the for accounting mary phenomenon have that more of it trucks'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_loss, test_perplexity = evaluate(model, \"ELMANRNN\", test_data, vocab_size, batch_size, seq_len, device)\n",
    "print('test_loss =', '{:.4f}'.format(test_loss), 'test_perplexity =', '{:.4f}'.format(test_perplexity))\n",
    "\n",
    "# Generate text\n",
    "sample(model, 'ELMANRNN', int_to_word, 50, vocab, device, start='however director thomas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excise 3.1 gradient clipping\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "model_1 = ELMAN_RNN(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "train_losses_1, valid_losses_1, train_perplexities_1, valid_perplexities_1 = train(model=model_1,\n",
    "                                                                            model_type='ELMANRNN',\n",
    "                                                                            train_data=train_data,\n",
    "                                                                            valid_data=valid_data,\n",
    "                                                                            vocab_size=vocab_size, \n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            seq_len=seq_len,\n",
    "                                                                            num_epochs=num_epochs, \n",
    "                                                                            learning_rate=learning_rate, \n",
    "                                                                            device=device,\n",
    "                                                                            clip=5\n",
    "                                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 4.3876 test_perplexity = 80.4460\n",
      "constitution natural gas consolidating said it intends from the december computers of the problem \n",
      "\n",
      "analysts has lived at texas national that he sent the salesman job in the fierce and standard compared at home credit on bloomingdale of whether \n",
      "\n",
      "the treasury began yesterday the association in a u.s. appeals yesterday 's offerings in five years compared with august locations \n",
      "\n",
      "a rash of the <unk> services concern was at columbia vice two norfolk 's <unk> theatre fund increase beginning \n",
      "\n",
      "rhetoric many cos. firm prompted earlier holding college formation N a year delivered \n",
      "\n",
      "third-quarter net income at\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_loss_1, test_perplexity_1 = evaluate(model_1, \"ELMANRNN\", test_data, vocab_size, batch_size, seq_len, device)\n",
    "print('test_loss =', '{:.4f}'.format(test_loss_1), 'test_perplexity =', '{:.4f}'.format(test_perplexity_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.embedding(inputs)\n",
    "        output, hidden = self.lstm(embeds, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excise 3.2 TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[0/1549], Loss: 9.2084, Perplexity: 9981.05\n",
      "Epoch [1/10], Step[100/1549], Loss: 6.0080, Perplexity: 406.68\n",
      "Epoch [1/10], Step[200/1549], Loss: 5.9508, Perplexity: 384.05\n",
      "Epoch [1/10], Step[300/1549], Loss: 5.7628, Perplexity: 318.23\n",
      "Epoch [1/10], Step[400/1549], Loss: 5.6997, Perplexity: 298.77\n",
      "Epoch [1/10], Step[500/1549], Loss: 5.1125, Perplexity: 166.09\n",
      "Epoch [1/10], Step[600/1549], Loss: 5.1551, Perplexity: 173.31\n",
      "Epoch [1/10], Step[700/1549], Loss: 5.3598, Perplexity: 212.67\n",
      "Epoch [1/10], Step[800/1549], Loss: 5.1513, Perplexity: 172.66\n",
      "Epoch [1/10], Step[900/1549], Loss: 5.0623, Perplexity: 157.96\n",
      "Epoch [1/10], Step[1000/1549], Loss: 5.1000, Perplexity: 164.03\n",
      "Epoch [1/10], Step[1100/1549], Loss: 5.3101, Perplexity: 202.37\n",
      "Epoch [1/10], Step[1200/1549], Loss: 5.1864, Perplexity: 178.82\n",
      "Epoch [1/10], Step[1300/1549], Loss: 5.0875, Perplexity: 161.98\n",
      "Epoch [1/10], Step[1400/1549], Loss: 4.8028, Perplexity: 121.85\n",
      "Epoch [1/10], Step[1500/1549], Loss: 5.1476, Perplexity: 172.01\n",
      "Epoch: 0001 train_loss = 5.3487 valid_loss = 4.9148 time = 55.1257\n",
      "Epoch [2/10], Step[0/1549], Loss: 5.4013, Perplexity: 221.69\n",
      "Epoch [2/10], Step[100/1549], Loss: 4.5752, Perplexity: 97.05\n",
      "Epoch [2/10], Step[200/1549], Loss: 4.6560, Perplexity: 105.21\n",
      "Epoch [2/10], Step[300/1549], Loss: 4.6890, Perplexity: 108.75\n",
      "Epoch [2/10], Step[400/1549], Loss: 4.5036, Perplexity: 90.34\n",
      "Epoch [2/10], Step[500/1549], Loss: 4.1433, Perplexity: 63.01\n",
      "Epoch [2/10], Step[600/1549], Loss: 4.4198, Perplexity: 83.08\n",
      "Epoch [2/10], Step[700/1549], Loss: 4.4125, Perplexity: 82.47\n",
      "Epoch [2/10], Step[800/1549], Loss: 4.3979, Perplexity: 81.28\n",
      "Epoch [2/10], Step[900/1549], Loss: 4.2135, Perplexity: 67.59\n",
      "Epoch [2/10], Step[1000/1549], Loss: 4.2922, Perplexity: 73.13\n",
      "Epoch [2/10], Step[1100/1549], Loss: 4.4841, Perplexity: 88.60\n",
      "Epoch [2/10], Step[1200/1549], Loss: 4.4948, Perplexity: 89.55\n",
      "Epoch [2/10], Step[1300/1549], Loss: 4.2851, Perplexity: 72.61\n",
      "Epoch [2/10], Step[1400/1549], Loss: 3.9222, Perplexity: 50.51\n",
      "Epoch [2/10], Step[1500/1549], Loss: 4.3783, Perplexity: 79.70\n",
      "Epoch: 0002 train_loss = 4.3723 valid_loss = 4.8520 time = 56.1389\n",
      "Epoch [3/10], Step[0/1549], Loss: 4.3941, Perplexity: 80.97\n",
      "Epoch [3/10], Step[100/1549], Loss: 3.8736, Perplexity: 48.11\n",
      "Epoch [3/10], Step[200/1549], Loss: 4.0072, Perplexity: 54.99\n",
      "Epoch [3/10], Step[300/1549], Loss: 3.9527, Perplexity: 52.07\n",
      "Epoch [3/10], Step[400/1549], Loss: 3.8089, Perplexity: 45.10\n",
      "Epoch [3/10], Step[500/1549], Loss: 3.3805, Perplexity: 29.39\n",
      "Epoch [3/10], Step[600/1549], Loss: 3.8341, Perplexity: 46.25\n",
      "Epoch [3/10], Step[700/1549], Loss: 3.7156, Perplexity: 41.08\n",
      "Epoch [3/10], Step[800/1549], Loss: 3.7924, Perplexity: 44.36\n",
      "Epoch [3/10], Step[900/1549], Loss: 3.5108, Perplexity: 33.48\n",
      "Epoch [3/10], Step[1000/1549], Loss: 3.5558, Perplexity: 35.01\n",
      "Epoch [3/10], Step[1100/1549], Loss: 3.7408, Perplexity: 42.13\n",
      "Epoch [3/10], Step[1200/1549], Loss: 3.8480, Perplexity: 46.90\n",
      "Epoch [3/10], Step[1300/1549], Loss: 3.5200, Perplexity: 33.78\n",
      "Epoch [3/10], Step[1400/1549], Loss: 3.1642, Perplexity: 23.67\n",
      "Epoch [3/10], Step[1500/1549], Loss: 3.6717, Perplexity: 39.32\n",
      "Epoch: 0003 train_loss = 3.6828 valid_loss = 5.0072 time = 55.9523\n",
      "Epoch [4/10], Step[0/1549], Loss: 3.6025, Perplexity: 36.69\n",
      "Epoch [4/10], Step[100/1549], Loss: 3.3356, Perplexity: 28.10\n",
      "Epoch [4/10], Step[200/1549], Loss: 3.5156, Perplexity: 33.63\n",
      "Epoch [4/10], Step[300/1549], Loss: 3.3978, Perplexity: 29.90\n",
      "Epoch [4/10], Step[400/1549], Loss: 3.2664, Perplexity: 26.22\n",
      "Epoch [4/10], Step[500/1549], Loss: 2.8696, Perplexity: 17.63\n",
      "Epoch [4/10], Step[600/1549], Loss: 3.4040, Perplexity: 30.08\n",
      "Epoch [4/10], Step[700/1549], Loss: 3.2449, Perplexity: 25.66\n",
      "Epoch [4/10], Step[800/1549], Loss: 3.2707, Perplexity: 26.33\n",
      "Epoch [4/10], Step[900/1549], Loss: 3.0075, Perplexity: 20.24\n",
      "Epoch [4/10], Step[1000/1549], Loss: 3.1488, Perplexity: 23.31\n",
      "Epoch [4/10], Step[1100/1549], Loss: 3.2313, Perplexity: 25.31\n",
      "Epoch [4/10], Step[1200/1549], Loss: 3.3345, Perplexity: 28.06\n",
      "Epoch [4/10], Step[1300/1549], Loss: 3.0472, Perplexity: 21.06\n",
      "Epoch [4/10], Step[1400/1549], Loss: 2.7041, Perplexity: 14.94\n",
      "Epoch [4/10], Step[1500/1549], Loss: 3.1213, Perplexity: 22.68\n",
      "Epoch: 0004 train_loss = 3.1691 valid_loss = 5.2129 time = 56.0208\n",
      "Epoch [5/10], Step[0/1549], Loss: 3.1287, Perplexity: 22.84\n",
      "Epoch [5/10], Step[100/1549], Loss: 2.9642, Perplexity: 19.38\n",
      "Epoch [5/10], Step[200/1549], Loss: 3.1116, Perplexity: 22.46\n",
      "Epoch [5/10], Step[300/1549], Loss: 2.9237, Perplexity: 18.61\n",
      "Epoch [5/10], Step[400/1549], Loss: 2.9364, Perplexity: 18.85\n",
      "Epoch [5/10], Step[500/1549], Loss: 2.5472, Perplexity: 12.77\n",
      "Epoch [5/10], Step[600/1549], Loss: 3.0886, Perplexity: 21.95\n",
      "Epoch [5/10], Step[700/1549], Loss: 2.9983, Perplexity: 20.05\n",
      "Epoch [5/10], Step[800/1549], Loss: 2.9719, Perplexity: 19.53\n",
      "Epoch [5/10], Step[900/1549], Loss: 2.7401, Perplexity: 15.49\n",
      "Epoch [5/10], Step[1000/1549], Loss: 2.8568, Perplexity: 17.41\n",
      "Epoch [5/10], Step[1100/1549], Loss: 2.8613, Perplexity: 17.48\n",
      "Epoch [5/10], Step[1200/1549], Loss: 3.0132, Perplexity: 20.35\n",
      "Epoch [5/10], Step[1300/1549], Loss: 2.7199, Perplexity: 15.18\n",
      "Epoch [5/10], Step[1400/1549], Loss: 2.3964, Perplexity: 10.98\n",
      "Epoch [5/10], Step[1500/1549], Loss: 2.7964, Perplexity: 16.39\n",
      "Epoch: 0005 train_loss = 2.8391 valid_loss = 5.4044 time = 55.8963\n",
      "Epoch [6/10], Step[0/1549], Loss: 2.8688, Perplexity: 17.62\n",
      "Epoch [6/10], Step[100/1549], Loss: 2.8003, Perplexity: 16.45\n",
      "Epoch [6/10], Step[200/1549], Loss: 2.8578, Perplexity: 17.42\n",
      "Epoch [6/10], Step[300/1549], Loss: 2.7578, Perplexity: 15.77\n",
      "Epoch [6/10], Step[400/1549], Loss: 2.7411, Perplexity: 15.50\n",
      "Epoch [6/10], Step[500/1549], Loss: 2.3212, Perplexity: 10.19\n",
      "Epoch [6/10], Step[600/1549], Loss: 2.8581, Perplexity: 17.43\n",
      "Epoch [6/10], Step[700/1549], Loss: 2.6748, Perplexity: 14.51\n",
      "Epoch [6/10], Step[800/1549], Loss: 2.8201, Perplexity: 16.78\n",
      "Epoch [6/10], Step[900/1549], Loss: 2.5398, Perplexity: 12.68\n",
      "Epoch [6/10], Step[1000/1549], Loss: 2.6981, Perplexity: 14.85\n",
      "Epoch [6/10], Step[1100/1549], Loss: 2.7637, Perplexity: 15.86\n",
      "Epoch [6/10], Step[1200/1549], Loss: 2.8925, Perplexity: 18.04\n",
      "Epoch [6/10], Step[1300/1549], Loss: 2.5448, Perplexity: 12.74\n",
      "Epoch [6/10], Step[1400/1549], Loss: 2.1687, Perplexity:  8.75\n",
      "Epoch [6/10], Step[1500/1549], Loss: 2.5991, Perplexity: 13.45\n",
      "Epoch: 0006 train_loss = 2.6339 valid_loss = 5.5848 time = 56.1607\n",
      "Epoch [7/10], Step[0/1549], Loss: 2.6201, Perplexity: 13.74\n",
      "Epoch [7/10], Step[100/1549], Loss: 2.5622, Perplexity: 12.96\n",
      "Epoch [7/10], Step[200/1549], Loss: 2.8084, Perplexity: 16.58\n",
      "Epoch [7/10], Step[300/1549], Loss: 2.6534, Perplexity: 14.20\n",
      "Epoch [7/10], Step[400/1549], Loss: 2.5956, Perplexity: 13.40\n",
      "Epoch [7/10], Step[500/1549], Loss: 2.2142, Perplexity:  9.15\n",
      "Epoch [7/10], Step[600/1549], Loss: 2.6675, Perplexity: 14.40\n",
      "Epoch [7/10], Step[700/1549], Loss: 2.6671, Perplexity: 14.40\n",
      "Epoch [7/10], Step[800/1549], Loss: 2.6504, Perplexity: 14.16\n",
      "Epoch [7/10], Step[900/1549], Loss: 2.3250, Perplexity: 10.23\n",
      "Epoch [7/10], Step[1000/1549], Loss: 2.5576, Perplexity: 12.90\n",
      "Epoch [7/10], Step[1100/1549], Loss: 2.6195, Perplexity: 13.73\n",
      "Epoch [7/10], Step[1200/1549], Loss: 2.7132, Perplexity: 15.08\n",
      "Epoch [7/10], Step[1300/1549], Loss: 2.3739, Perplexity: 10.74\n",
      "Epoch [7/10], Step[1400/1549], Loss: 2.1256, Perplexity:  8.38\n",
      "Epoch [7/10], Step[1500/1549], Loss: 2.5271, Perplexity: 12.52\n",
      "Epoch: 0007 train_loss = 2.5002 valid_loss = 5.7359 time = 56.7749\n",
      "Epoch [8/10], Step[0/1549], Loss: 2.5341, Perplexity: 12.61\n",
      "Epoch [8/10], Step[100/1549], Loss: 2.3821, Perplexity: 10.83\n",
      "Epoch [8/10], Step[200/1549], Loss: 2.6407, Perplexity: 14.02\n",
      "Epoch [8/10], Step[300/1549], Loss: 2.5152, Perplexity: 12.37\n",
      "Epoch [8/10], Step[400/1549], Loss: 2.4820, Perplexity: 11.97\n",
      "Epoch [8/10], Step[500/1549], Loss: 2.1400, Perplexity:  8.50\n",
      "Epoch [8/10], Step[600/1549], Loss: 2.6355, Perplexity: 13.95\n",
      "Epoch [8/10], Step[700/1549], Loss: 2.5320, Perplexity: 12.58\n",
      "Epoch [8/10], Step[800/1549], Loss: 2.6005, Perplexity: 13.47\n",
      "Epoch [8/10], Step[900/1549], Loss: 2.3309, Perplexity: 10.29\n",
      "Epoch [8/10], Step[1000/1549], Loss: 2.4931, Perplexity: 12.10\n",
      "Epoch [8/10], Step[1100/1549], Loss: 2.5586, Perplexity: 12.92\n",
      "Epoch [8/10], Step[1200/1549], Loss: 2.6205, Perplexity: 13.74\n",
      "Epoch [8/10], Step[1300/1549], Loss: 2.3628, Perplexity: 10.62\n",
      "Epoch [8/10], Step[1400/1549], Loss: 2.0355, Perplexity:  7.66\n",
      "Epoch [8/10], Step[1500/1549], Loss: 2.4459, Perplexity: 11.54\n",
      "Epoch: 0008 train_loss = 2.4124 valid_loss = 5.8684 time = 56.2574\n",
      "Epoch [9/10], Step[0/1549], Loss: 2.6084, Perplexity: 13.58\n",
      "Epoch [9/10], Step[100/1549], Loss: 2.3395, Perplexity: 10.38\n",
      "Epoch [9/10], Step[200/1549], Loss: 2.5181, Perplexity: 12.40\n",
      "Epoch [9/10], Step[300/1549], Loss: 2.4076, Perplexity: 11.11\n",
      "Epoch [9/10], Step[400/1549], Loss: 2.4387, Perplexity: 11.46\n",
      "Epoch [9/10], Step[500/1549], Loss: 2.0563, Perplexity:  7.82\n",
      "Epoch [9/10], Step[600/1549], Loss: 2.5555, Perplexity: 12.88\n",
      "Epoch [9/10], Step[700/1549], Loss: 2.4475, Perplexity: 11.56\n",
      "Epoch [9/10], Step[800/1549], Loss: 2.6189, Perplexity: 13.72\n",
      "Epoch [9/10], Step[900/1549], Loss: 2.2885, Perplexity:  9.86\n",
      "Epoch [9/10], Step[1000/1549], Loss: 2.3830, Perplexity: 10.84\n",
      "Epoch [9/10], Step[1100/1549], Loss: 2.5356, Perplexity: 12.62\n",
      "Epoch [9/10], Step[1200/1549], Loss: 2.5474, Perplexity: 12.77\n",
      "Epoch [9/10], Step[1300/1549], Loss: 2.3068, Perplexity: 10.04\n",
      "Epoch [9/10], Step[1400/1549], Loss: 1.9397, Perplexity:  6.96\n",
      "Epoch [9/10], Step[1500/1549], Loss: 2.3687, Perplexity: 10.68\n",
      "Epoch: 0009 train_loss = 2.3556 valid_loss = 5.9920 time = 56.3046\n",
      "Epoch [10/10], Step[0/1549], Loss: 2.5403, Perplexity: 12.68\n",
      "Epoch [10/10], Step[100/1549], Loss: 2.4285, Perplexity: 11.34\n",
      "Epoch [10/10], Step[200/1549], Loss: 2.3989, Perplexity: 11.01\n",
      "Epoch [10/10], Step[300/1549], Loss: 2.4222, Perplexity: 11.27\n",
      "Epoch [10/10], Step[400/1549], Loss: 2.4032, Perplexity: 11.06\n",
      "Epoch [10/10], Step[500/1549], Loss: 2.0120, Perplexity:  7.48\n",
      "Epoch [10/10], Step[600/1549], Loss: 2.4648, Perplexity: 11.76\n",
      "Epoch [10/10], Step[700/1549], Loss: 2.4397, Perplexity: 11.47\n",
      "Epoch [10/10], Step[800/1549], Loss: 2.5939, Perplexity: 13.38\n",
      "Epoch [10/10], Step[900/1549], Loss: 2.2740, Perplexity:  9.72\n",
      "Epoch [10/10], Step[1000/1549], Loss: 2.4442, Perplexity: 11.52\n",
      "Epoch [10/10], Step[1100/1549], Loss: 2.5288, Perplexity: 12.54\n",
      "Epoch [10/10], Step[1200/1549], Loss: 2.5902, Perplexity: 13.33\n",
      "Epoch [10/10], Step[1300/1549], Loss: 2.3213, Perplexity: 10.19\n",
      "Epoch [10/10], Step[1400/1549], Loss: 1.9024, Perplexity:  6.70\n",
      "Epoch [10/10], Step[1500/1549], Loss: 2.4338, Perplexity: 11.40\n",
      "Epoch: 0010 train_loss = 2.3199 valid_loss = 6.0904 time = 56.3688\n"
     ]
    }
   ],
   "source": [
    "# excise 3.3\n",
    "learning_rate = 0.002\n",
    "num_epochs = 10\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "\n",
    "# Train model\n",
    "model_3 = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "train_losses_3, valid_losses_3, train_perplexities_3, valid_perplexities_3 = train(model=model_3,\n",
    "                                                                           model_type='LSTM',\n",
    "                                                                            train_data=train_data,\n",
    "                                                                            valid_data=valid_data,\n",
    "                                                                            vocab_size=vocab_size, \n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            seq_len=seq_len,\n",
    "                                                                            num_epochs=num_epochs, \n",
    "                                                                            learning_rate=learning_rate, \n",
    "                                                                            device=device,\n",
    "                                                                            clip=0.5\n",
    "                                                                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_loss_3, test_perplexity_3 = evaluate(model_3, 'LSTM', test_data, vocab_size, batch_size, seq_len, device)\n",
    "print('test_loss =', '{:.4f}'.format(test_loss_3), 'test_perplexity =', '{:.4f}'.format(test_perplexity_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[0/1549], Loss: 9.2163, Perplexity: 10059.46\n",
      "Epoch [1/10], Step[100/1549], Loss: 5.9393, Perplexity: 379.66\n",
      "Epoch [1/10], Step[200/1549], Loss: 5.8815, Perplexity: 358.33\n",
      "Epoch [1/10], Step[300/1549], Loss: 5.6454, Perplexity: 282.99\n",
      "Epoch [1/10], Step[400/1549], Loss: 5.6037, Perplexity: 271.42\n",
      "Epoch [1/10], Step[500/1549], Loss: 5.0157, Perplexity: 150.76\n",
      "Epoch [1/10], Step[600/1549], Loss: 5.1945, Perplexity: 180.28\n",
      "Epoch [1/10], Step[700/1549], Loss: 5.2539, Perplexity: 191.30\n",
      "Epoch [1/10], Step[800/1549], Loss: 5.1535, Perplexity: 173.03\n",
      "Epoch [1/10], Step[900/1549], Loss: 5.0163, Perplexity: 150.86\n",
      "Epoch [1/10], Step[1000/1549], Loss: 5.1104, Perplexity: 165.73\n",
      "Epoch [1/10], Step[1100/1549], Loss: 5.3256, Perplexity: 205.53\n",
      "Epoch [1/10], Step[1200/1549], Loss: 5.2438, Perplexity: 189.38\n",
      "Epoch [1/10], Step[1300/1549], Loss: 5.0817, Perplexity: 161.04\n",
      "Epoch [1/10], Step[1400/1549], Loss: 4.8724, Perplexity: 130.63\n",
      "Epoch [1/10], Step[1500/1549], Loss: 5.1361, Perplexity: 170.05\n",
      "Epoch: 0001 train_loss = 5.3108 valid_loss = 5.0344 time = 55.5159\n",
      "Epoch [2/10], Step[0/1549], Loss: 6.0013, Perplexity: 403.95\n",
      "Epoch [2/10], Step[100/1549], Loss: 4.6026, Perplexity: 99.74\n",
      "Epoch [2/10], Step[200/1549], Loss: 4.7333, Perplexity: 113.67\n",
      "Epoch [2/10], Step[300/1549], Loss: 4.4683, Perplexity: 87.21\n",
      "Epoch [2/10], Step[400/1549], Loss: 4.4495, Perplexity: 85.58\n",
      "Epoch [2/10], Step[500/1549], Loss: 3.9804, Perplexity: 53.54\n",
      "Epoch [2/10], Step[600/1549], Loss: 4.2737, Perplexity: 71.78\n",
      "Epoch [2/10], Step[700/1549], Loss: 4.2982, Perplexity: 73.57\n",
      "Epoch [2/10], Step[800/1549], Loss: 4.2708, Perplexity: 71.58\n",
      "Epoch [2/10], Step[900/1549], Loss: 4.0787, Perplexity: 59.07\n",
      "Epoch [2/10], Step[1000/1549], Loss: 4.1794, Perplexity: 65.32\n",
      "Epoch [2/10], Step[1100/1549], Loss: 4.3046, Perplexity: 74.04\n",
      "Epoch [2/10], Step[1200/1549], Loss: 4.3419, Perplexity: 76.86\n",
      "Epoch [2/10], Step[1300/1549], Loss: 4.0493, Perplexity: 57.36\n",
      "Epoch [2/10], Step[1400/1549], Loss: 3.8464, Perplexity: 46.82\n",
      "Epoch [2/10], Step[1500/1549], Loss: 4.2349, Perplexity: 69.06\n",
      "Epoch: 0002 train_loss = 4.2617 valid_loss = 5.2447 time = 56.3295\n",
      "Epoch [3/10], Step[0/1549], Loss: 4.6153, Perplexity: 101.01\n",
      "Epoch [3/10], Step[100/1549], Loss: 3.9672, Perplexity: 52.84\n",
      "Epoch [3/10], Step[200/1549], Loss: 4.1170, Perplexity: 61.38\n",
      "Epoch [3/10], Step[300/1549], Loss: 3.9088, Perplexity: 49.84\n",
      "Epoch [3/10], Step[400/1549], Loss: 3.9634, Perplexity: 52.64\n",
      "Epoch [3/10], Step[500/1549], Loss: 3.3603, Perplexity: 28.80\n",
      "Epoch [3/10], Step[600/1549], Loss: 3.8515, Perplexity: 47.07\n",
      "Epoch [3/10], Step[700/1549], Loss: 3.7443, Perplexity: 42.28\n",
      "Epoch [3/10], Step[800/1549], Loss: 3.8688, Perplexity: 47.88\n",
      "Epoch [3/10], Step[900/1549], Loss: 3.5442, Perplexity: 34.61\n",
      "Epoch [3/10], Step[1000/1549], Loss: 3.6578, Perplexity: 38.78\n",
      "Epoch [3/10], Step[1100/1549], Loss: 3.7905, Perplexity: 44.28\n",
      "Epoch [3/10], Step[1200/1549], Loss: 3.8331, Perplexity: 46.20\n",
      "Epoch [3/10], Step[1300/1549], Loss: 3.5987, Perplexity: 36.55\n",
      "Epoch [3/10], Step[1400/1549], Loss: 3.3138, Perplexity: 27.49\n",
      "Epoch [3/10], Step[1500/1549], Loss: 3.7382, Perplexity: 42.02\n",
      "Epoch: 0003 train_loss = 3.7294 valid_loss = 5.4806 time = 55.8752\n",
      "Epoch [4/10], Step[0/1549], Loss: 4.2532, Perplexity: 70.33\n",
      "Epoch [4/10], Step[100/1549], Loss: 3.5676, Perplexity: 35.43\n",
      "Epoch [4/10], Step[200/1549], Loss: 3.7551, Perplexity: 42.74\n",
      "Epoch [4/10], Step[300/1549], Loss: 3.6333, Perplexity: 37.84\n",
      "Epoch [4/10], Step[400/1549], Loss: 3.7105, Perplexity: 40.87\n",
      "Epoch [4/10], Step[500/1549], Loss: 3.0461, Perplexity: 21.03\n",
      "Epoch [4/10], Step[600/1549], Loss: 3.6572, Perplexity: 38.75\n",
      "Epoch [4/10], Step[700/1549], Loss: 3.4449, Perplexity: 31.34\n",
      "Epoch [4/10], Step[800/1549], Loss: 3.7566, Perplexity: 42.80\n",
      "Epoch [4/10], Step[900/1549], Loss: 3.3145, Perplexity: 27.51\n",
      "Epoch [4/10], Step[1000/1549], Loss: 3.5300, Perplexity: 34.12\n",
      "Epoch [4/10], Step[1100/1549], Loss: 3.5952, Perplexity: 36.42\n",
      "Epoch [4/10], Step[1200/1549], Loss: 3.6409, Perplexity: 38.13\n",
      "Epoch [4/10], Step[1300/1549], Loss: 3.3441, Perplexity: 28.33\n",
      "Epoch [4/10], Step[1400/1549], Loss: 3.0409, Perplexity: 20.92\n",
      "Epoch [4/10], Step[1500/1549], Loss: 3.5859, Perplexity: 36.09\n",
      "Epoch: 0004 train_loss = 3.4738 valid_loss = 5.7334 time = 55.7669\n",
      "Epoch [5/10], Step[0/1549], Loss: 4.7116, Perplexity: 111.23\n",
      "Epoch [5/10], Step[100/1549], Loss: 3.3536, Perplexity: 28.61\n",
      "Epoch [5/10], Step[200/1549], Loss: 3.5097, Perplexity: 33.44\n",
      "Epoch [5/10], Step[300/1549], Loss: 3.4919, Perplexity: 32.85\n",
      "Epoch [5/10], Step[400/1549], Loss: 3.6256, Perplexity: 37.55\n",
      "Epoch [5/10], Step[500/1549], Loss: 3.0360, Perplexity: 20.82\n",
      "Epoch [5/10], Step[600/1549], Loss: 3.5895, Perplexity: 36.21\n",
      "Epoch [5/10], Step[700/1549], Loss: 3.3894, Perplexity: 29.65\n",
      "Epoch [5/10], Step[800/1549], Loss: 3.6242, Perplexity: 37.49\n",
      "Epoch [5/10], Step[900/1549], Loss: 3.2066, Perplexity: 24.70\n",
      "Epoch [5/10], Step[1000/1549], Loss: 3.3288, Perplexity: 27.91\n",
      "Epoch [5/10], Step[1100/1549], Loss: 3.5315, Perplexity: 34.18\n",
      "Epoch [5/10], Step[1200/1549], Loss: 3.5070, Perplexity: 33.35\n",
      "Epoch [5/10], Step[1300/1549], Loss: 3.2999, Perplexity: 27.11\n",
      "Epoch [5/10], Step[1400/1549], Loss: 2.9300, Perplexity: 18.73\n",
      "Epoch [5/10], Step[1500/1549], Loss: 3.4488, Perplexity: 31.46\n",
      "Epoch: 0005 train_loss = 3.3554 valid_loss = 5.9521 time = 56.1378\n",
      "Epoch [6/10], Step[0/1549], Loss: 5.0828, Perplexity: 161.23\n",
      "Epoch [6/10], Step[100/1549], Loss: 3.3962, Perplexity: 29.85\n",
      "Epoch [6/10], Step[200/1549], Loss: 3.4302, Perplexity: 30.88\n",
      "Epoch [6/10], Step[300/1549], Loss: 3.3631, Perplexity: 28.88\n",
      "Epoch [6/10], Step[400/1549], Loss: 3.4766, Perplexity: 32.35\n",
      "Epoch [6/10], Step[500/1549], Loss: 2.8802, Perplexity: 17.82\n",
      "Epoch [6/10], Step[600/1549], Loss: 3.5518, Perplexity: 34.87\n",
      "Epoch [6/10], Step[700/1549], Loss: 3.4017, Perplexity: 30.02\n",
      "Epoch [6/10], Step[800/1549], Loss: 3.5154, Perplexity: 33.63\n",
      "Epoch [6/10], Step[900/1549], Loss: 3.1703, Perplexity: 23.81\n",
      "Epoch [6/10], Step[1000/1549], Loss: 3.3384, Perplexity: 28.18\n",
      "Epoch [6/10], Step[1100/1549], Loss: 3.5635, Perplexity: 35.29\n",
      "Epoch [6/10], Step[1200/1549], Loss: 3.4342, Perplexity: 31.01\n",
      "Epoch [6/10], Step[1300/1549], Loss: 3.2229, Perplexity: 25.10\n",
      "Epoch [6/10], Step[1400/1549], Loss: 2.8781, Perplexity: 17.78\n",
      "Epoch [6/10], Step[1500/1549], Loss: 3.4734, Perplexity: 32.25\n",
      "Epoch: 0006 train_loss = 3.3092 valid_loss = 6.1934 time = 58.0371\n",
      "Epoch [7/10], Step[0/1549], Loss: 5.5261, Perplexity: 251.16\n",
      "Epoch [7/10], Step[100/1549], Loss: 3.2613, Perplexity: 26.08\n",
      "Epoch [7/10], Step[200/1549], Loss: 3.4252, Perplexity: 30.73\n",
      "Epoch [7/10], Step[300/1549], Loss: 3.3674, Perplexity: 29.00\n",
      "Epoch [7/10], Step[400/1549], Loss: 3.4499, Perplexity: 31.50\n",
      "Epoch [7/10], Step[500/1549], Loss: 2.8837, Perplexity: 17.88\n",
      "Epoch [7/10], Step[600/1549], Loss: 3.6693, Perplexity: 39.22\n",
      "Epoch [7/10], Step[700/1549], Loss: 3.3941, Perplexity: 29.79\n",
      "Epoch [7/10], Step[800/1549], Loss: 3.5753, Perplexity: 35.70\n",
      "Epoch [7/10], Step[900/1549], Loss: 3.1767, Perplexity: 23.97\n",
      "Epoch [7/10], Step[1000/1549], Loss: 3.4117, Perplexity: 30.32\n",
      "Epoch [7/10], Step[1100/1549], Loss: 3.5155, Perplexity: 33.63\n",
      "Epoch [7/10], Step[1200/1549], Loss: 3.4643, Perplexity: 31.95\n",
      "Epoch [7/10], Step[1300/1549], Loss: 3.2693, Perplexity: 26.29\n",
      "Epoch [7/10], Step[1400/1549], Loss: 2.7652, Perplexity: 15.88\n",
      "Epoch [7/10], Step[1500/1549], Loss: 3.4901, Perplexity: 32.79\n",
      "Epoch: 0007 train_loss = 3.2990 valid_loss = 6.3646 time = 55.7840\n",
      "Epoch [8/10], Step[0/1549], Loss: 5.8265, Perplexity: 339.18\n",
      "Epoch [8/10], Step[100/1549], Loss: 3.2136, Perplexity: 24.87\n",
      "Epoch [8/10], Step[200/1549], Loss: 3.2837, Perplexity: 26.67\n",
      "Epoch [8/10], Step[300/1549], Loss: 3.3418, Perplexity: 28.27\n",
      "Epoch [8/10], Step[400/1549], Loss: 3.4129, Perplexity: 30.35\n",
      "Epoch [8/10], Step[500/1549], Loss: 2.9895, Perplexity: 19.87\n",
      "Epoch [8/10], Step[600/1549], Loss: 3.6114, Perplexity: 37.02\n",
      "Epoch [8/10], Step[700/1549], Loss: 3.3680, Perplexity: 29.02\n",
      "Epoch [8/10], Step[800/1549], Loss: 3.5448, Perplexity: 34.63\n",
      "Epoch [8/10], Step[900/1549], Loss: 3.2573, Perplexity: 25.98\n",
      "Epoch [8/10], Step[1000/1549], Loss: 3.3445, Perplexity: 28.35\n",
      "Epoch [8/10], Step[1100/1549], Loss: 3.4826, Perplexity: 32.54\n",
      "Epoch [8/10], Step[1200/1549], Loss: 3.3899, Perplexity: 29.66\n",
      "Epoch [8/10], Step[1300/1549], Loss: 3.2821, Perplexity: 26.63\n",
      "Epoch [8/10], Step[1400/1549], Loss: 2.9254, Perplexity: 18.64\n",
      "Epoch [8/10], Step[1500/1549], Loss: 3.4496, Perplexity: 31.49\n",
      "Epoch: 0008 train_loss = 3.3020 valid_loss = 6.5348 time = 55.6207\n",
      "Epoch [9/10], Step[0/1549], Loss: 6.0216, Perplexity: 412.24\n",
      "Epoch [9/10], Step[100/1549], Loss: 3.1961, Perplexity: 24.44\n",
      "Epoch [9/10], Step[200/1549], Loss: 3.4711, Perplexity: 32.17\n",
      "Epoch [9/10], Step[300/1549], Loss: 3.3756, Perplexity: 29.24\n",
      "Epoch [9/10], Step[400/1549], Loss: 3.5798, Perplexity: 35.87\n",
      "Epoch [9/10], Step[500/1549], Loss: 2.8325, Perplexity: 16.99\n",
      "Epoch [9/10], Step[600/1549], Loss: 3.5552, Perplexity: 35.00\n",
      "Epoch [9/10], Step[700/1549], Loss: 3.4806, Perplexity: 32.48\n",
      "Epoch [9/10], Step[800/1549], Loss: 3.6416, Perplexity: 38.15\n",
      "Epoch [9/10], Step[900/1549], Loss: 3.2944, Perplexity: 26.96\n",
      "Epoch [9/10], Step[1000/1549], Loss: 3.3540, Perplexity: 28.62\n",
      "Epoch [9/10], Step[1100/1549], Loss: 3.5322, Perplexity: 34.20\n",
      "Epoch [9/10], Step[1200/1549], Loss: 3.2996, Perplexity: 27.10\n",
      "Epoch [9/10], Step[1300/1549], Loss: 3.0582, Perplexity: 21.29\n",
      "Epoch [9/10], Step[1400/1549], Loss: 2.9265, Perplexity: 18.66\n",
      "Epoch [9/10], Step[1500/1549], Loss: 3.3674, Perplexity: 29.00\n",
      "Epoch: 0009 train_loss = 3.3154 valid_loss = 6.6899 time = 55.6875\n",
      "Epoch [10/10], Step[0/1549], Loss: 6.2248, Perplexity: 505.12\n",
      "Epoch [10/10], Step[100/1549], Loss: 3.2020, Perplexity: 24.58\n",
      "Epoch [10/10], Step[200/1549], Loss: 3.3387, Perplexity: 28.18\n",
      "Epoch [10/10], Step[300/1549], Loss: 3.3920, Perplexity: 29.73\n",
      "Epoch [10/10], Step[400/1549], Loss: 3.4955, Perplexity: 32.97\n",
      "Epoch [10/10], Step[500/1549], Loss: 2.9419, Perplexity: 18.95\n",
      "Epoch [10/10], Step[600/1549], Loss: 3.5415, Perplexity: 34.52\n",
      "Epoch [10/10], Step[700/1549], Loss: 3.4200, Perplexity: 30.57\n",
      "Epoch [10/10], Step[800/1549], Loss: 3.5599, Perplexity: 35.16\n",
      "Epoch [10/10], Step[900/1549], Loss: 3.1289, Perplexity: 22.85\n",
      "Epoch [10/10], Step[1000/1549], Loss: 3.2682, Perplexity: 26.26\n",
      "Epoch [10/10], Step[1100/1549], Loss: 3.4800, Perplexity: 32.46\n",
      "Epoch [10/10], Step[1200/1549], Loss: 3.4757, Perplexity: 32.32\n",
      "Epoch [10/10], Step[1300/1549], Loss: 3.2797, Perplexity: 26.57\n",
      "Epoch [10/10], Step[1400/1549], Loss: 2.9674, Perplexity: 19.44\n",
      "Epoch [10/10], Step[1500/1549], Loss: 3.5207, Perplexity: 33.81\n",
      "Epoch: 0010 train_loss = 3.3400 valid_loss = 6.8385 time = 55.6918\n"
     ]
    }
   ],
   "source": [
    "# excise 3.3 gated recurrent unit (GRU)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(GRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.embedding(inputs)\n",
    "        output, hidden = self.gru(embeds, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "learning_rate = 0.002\n",
    "num_epochs = 10\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "\n",
    "# Train model\n",
    "model_4 = GRU(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "train_losses_4, valid_losses_4, train_perplexities_4, valid_perplexities_4 = train(model=model_4,\n",
    "                                                                           model_type='GRU',\n",
    "                                                                            train_data=train_data,\n",
    "                                                                            valid_data=valid_data,\n",
    "                                                                            vocab_size=vocab_size, \n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            seq_len=seq_len,\n",
    "                                                                            num_epochs=num_epochs, \n",
    "                                                                            learning_rate=learning_rate, \n",
    "                                                                            device=device,\n",
    "                                                                            clip=0.5\n",
    "                                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 3.4085 test_perplexity = 30.2212\n",
      "turbulence quoted as referring to treat victims while the rate above the risk that serves in particular suggest in funding the risks if it seemed not their spread \n",
      "\n",
      "an a.p. that tends to sell the franchise to concentrate what ford eventually trimmed it will acquire any but if overall words trades anyway lackluster deep makers \n",
      "\n",
      "bank indeed increasingly exchange rate would gradually decline in N real delivery had n't yet bought investments \n",
      "\n",
      "every posted <unk> labor high for the end were tied to five times below the predecessor showed a return to purchase N \n",
      "\n",
      "deutsche bank\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_loss_4, test_perplexity_4 = evaluate(model_4, 'GRU', test_data, vocab_size, batch_size, seq_len, device)\n",
    "print('test_loss =', '{:.4f}'.format(test_loss_4), 'test_perplexity =', '{:.4f}'.format(test_perplexity_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excise 3.4 bidirectional recurrent neural network (RNN)\n",
    "class BIDI_RECTIONAL_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(BIDI_RECTIONAL_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size*2, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.embedding(inputs)\n",
    "        output, hidden = self.lstm(embeds, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers*2, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers*2, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step[0/1549], Loss: 9.2111, Perplexity: 10007.68\n",
      "Epoch [1/2], Step[100/1549], Loss: 2.7893, Perplexity: 16.27\n",
      "Epoch [1/2], Step[200/1549], Loss: 1.6171, Perplexity:  5.04\n",
      "Epoch [1/2], Step[300/1549], Loss: 0.9806, Perplexity:  2.67\n",
      "Epoch [1/2], Step[400/1549], Loss: 0.6914, Perplexity:  2.00\n",
      "Epoch [1/2], Step[500/1549], Loss: 0.5841, Perplexity:  1.79\n",
      "Epoch [1/2], Step[600/1549], Loss: 0.5129, Perplexity:  1.67\n",
      "Epoch [1/2], Step[700/1549], Loss: 0.6230, Perplexity:  1.86\n",
      "Epoch [1/2], Step[800/1549], Loss: 0.5302, Perplexity:  1.70\n",
      "Epoch [1/2], Step[900/1549], Loss: 0.4640, Perplexity:  1.59\n",
      "Epoch [1/2], Step[1000/1549], Loss: 0.4213, Perplexity:  1.52\n",
      "Epoch [1/2], Step[1100/1549], Loss: 0.5089, Perplexity:  1.66\n",
      "Epoch [1/2], Step[1200/1549], Loss: 0.4832, Perplexity:  1.62\n",
      "Epoch [1/2], Step[1300/1549], Loss: 0.4398, Perplexity:  1.55\n",
      "Epoch [1/2], Step[1400/1549], Loss: 0.4334, Perplexity:  1.54\n",
      "Epoch [1/2], Step[1500/1549], Loss: 0.4310, Perplexity:  1.54\n",
      "Epoch: 0001 train_loss = 0.9725 valid_loss = 0.4534 time = 124.4175\n",
      "Epoch [2/2], Step[0/1549], Loss: 0.7846, Perplexity:  2.19\n",
      "Epoch [2/2], Step[100/1549], Loss: 0.3581, Perplexity:  1.43\n",
      "Epoch [2/2], Step[200/1549], Loss: 0.3830, Perplexity:  1.47\n",
      "Epoch [2/2], Step[300/1549], Loss: 0.3750, Perplexity:  1.45\n",
      "Epoch [2/2], Step[400/1549], Loss: 0.3575, Perplexity:  1.43\n",
      "Epoch [2/2], Step[500/1549], Loss: 0.2902, Perplexity:  1.34\n",
      "Epoch [2/2], Step[600/1549], Loss: 0.3393, Perplexity:  1.40\n",
      "Epoch [2/2], Step[700/1549], Loss: 0.3863, Perplexity:  1.47\n",
      "Epoch [2/2], Step[800/1549], Loss: 0.2779, Perplexity:  1.32\n",
      "Epoch [2/2], Step[900/1549], Loss: 0.2478, Perplexity:  1.28\n",
      "Epoch [2/2], Step[1000/1549], Loss: 0.2757, Perplexity:  1.32\n",
      "Epoch [2/2], Step[1100/1549], Loss: 0.3195, Perplexity:  1.38\n",
      "Epoch [2/2], Step[1200/1549], Loss: 0.2623, Perplexity:  1.30\n",
      "Epoch [2/2], Step[1300/1549], Loss: 0.2599, Perplexity:  1.30\n",
      "Epoch [2/2], Step[1400/1549], Loss: 0.2494, Perplexity:  1.28\n",
      "Epoch [2/2], Step[1500/1549], Loss: 0.2820, Perplexity:  1.33\n",
      "Epoch: 0002 train_loss = 0.3012 valid_loss = 0.4621 time = 122.9708\n"
     ]
    }
   ],
   "source": [
    "# excise 3.4 bidirectional recurrent neural network (RNN)\n",
    "learning_rate = 0.002\n",
    "num_epochs = 2\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "\n",
    "# Train model\n",
    "model_5 = BIDI_RECTIONAL_LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "train_losses_5, valid_losses_5, train_perplexities_5, valid_perplexities_5 = train(model=model_5,\n",
    "                                                                           model_type='LSTM',\n",
    "                                                                            train_data=train_data,\n",
    "                                                                            valid_data=valid_data,\n",
    "                                                                            vocab_size=vocab_size, \n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            seq_len=seq_len,\n",
    "                                                                            num_epochs=num_epochs, \n",
    "                                                                            learning_rate=learning_rate, \n",
    "                                                                            device=device,\n",
    "                                                                            clip=0.5\n",
    "                                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 0.2594 test_perplexity = 1.2962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'but stocks the stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks stocks'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_loss_5, test_perplexity_5 = evaluate(model_5, 'LSTM', test_data, vocab_size, batch_size, seq_len, device)\n",
    "print('test_loss =', '{:.4f}'.format(test_loss_5), 'test_perplexity =', '{:.4f}'.format(test_perplexity_5))\n",
    "\n",
    "# Generate text\n",
    "sample(model_5, 'LSTM', int_to_word, 50, vocab, device, start='but stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excise 3.4 transformer neural network (optional)\n",
    "class transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, number_decoder_layers):\n",
    "        super(transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=8, num_encoder_layers=6, num_decoder_layers=number_decoder_layers, dim_feedforward=hidden_size)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.embedding(inputs)\n",
    "        output = self.transformer(embeds, embeds)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step[0/1549], Loss: 9.3651, Perplexity: 11673.88\n",
      "Epoch [1/30], Step[100/1549], Loss: 6.6819, Perplexity: 797.87\n",
      "Epoch [1/30], Step[200/1549], Loss: 6.1918, Perplexity: 488.74\n",
      "Epoch [1/30], Step[300/1549], Loss: 6.0639, Perplexity: 430.05\n",
      "Epoch [1/30], Step[400/1549], Loss: 6.0327, Perplexity: 416.86\n",
      "Epoch [1/30], Step[500/1549], Loss: 5.6584, Perplexity: 286.70\n",
      "Epoch [1/30], Step[600/1549], Loss: 5.6100, Perplexity: 273.15\n",
      "Epoch [1/30], Step[700/1549], Loss: 5.8557, Perplexity: 349.22\n",
      "Epoch [1/30], Step[800/1549], Loss: 5.5923, Perplexity: 268.34\n",
      "Epoch [1/30], Step[900/1549], Loss: 5.6175, Perplexity: 275.19\n",
      "Epoch [1/30], Step[1000/1549], Loss: 5.6203, Perplexity: 275.96\n",
      "Epoch [1/30], Step[1100/1549], Loss: 5.8664, Perplexity: 352.98\n",
      "Epoch [1/30], Step[1200/1549], Loss: 5.7368, Perplexity: 310.06\n",
      "Epoch [1/30], Step[1300/1549], Loss: 5.6801, Perplexity: 292.97\n",
      "Epoch [1/30], Step[1400/1549], Loss: 5.4816, Perplexity: 240.22\n",
      "Epoch [1/30], Step[1500/1549], Loss: 5.7184, Perplexity: 304.42\n",
      "Epoch: 0001 train_loss = 5.9220 valid_loss = 5.5510 time = 21.2010\n",
      "Epoch [2/30], Step[0/1549], Loss: 5.7407, Perplexity: 311.27\n",
      "Epoch [2/30], Step[100/1549], Loss: 5.3101, Perplexity: 202.36\n",
      "Epoch [2/30], Step[200/1549], Loss: 5.2678, Perplexity: 193.98\n",
      "Epoch [2/30], Step[300/1549], Loss: 5.3629, Perplexity: 213.35\n",
      "Epoch [2/30], Step[400/1549], Loss: 5.3074, Perplexity: 201.82\n",
      "Epoch [2/30], Step[500/1549], Loss: 5.1169, Perplexity: 166.82\n",
      "Epoch [2/30], Step[600/1549], Loss: 5.2441, Perplexity: 189.44\n",
      "Epoch [2/30], Step[700/1549], Loss: 5.3609, Perplexity: 212.93\n",
      "Epoch [2/30], Step[800/1549], Loss: 5.2020, Perplexity: 181.64\n",
      "Epoch [2/30], Step[900/1549], Loss: 5.1837, Perplexity: 178.34\n",
      "Epoch [2/30], Step[1000/1549], Loss: 5.2480, Perplexity: 190.19\n",
      "Epoch [2/30], Step[1100/1549], Loss: 5.4870, Perplexity: 241.54\n",
      "Epoch [2/30], Step[1200/1549], Loss: 5.4183, Perplexity: 225.50\n",
      "Epoch [2/30], Step[1300/1549], Loss: 5.2517, Perplexity: 190.88\n",
      "Epoch [2/30], Step[1400/1549], Loss: 5.1044, Perplexity: 164.75\n",
      "Epoch [2/30], Step[1500/1549], Loss: 5.3843, Perplexity: 217.96\n",
      "Epoch: 0002 train_loss = 5.2788 valid_loss = 5.4437 time = 21.6764\n",
      "Epoch [3/30], Step[0/1549], Loss: 5.3582, Perplexity: 212.35\n",
      "Epoch [3/30], Step[100/1549], Loss: 5.0990, Perplexity: 163.86\n",
      "Epoch [3/30], Step[200/1549], Loss: 5.0526, Perplexity: 156.42\n",
      "Epoch [3/30], Step[300/1549], Loss: 5.1561, Perplexity: 173.49\n",
      "Epoch [3/30], Step[400/1549], Loss: 5.0824, Perplexity: 161.15\n",
      "Epoch [3/30], Step[500/1549], Loss: 4.9285, Perplexity: 138.17\n",
      "Epoch [3/30], Step[600/1549], Loss: 5.0969, Perplexity: 163.52\n",
      "Epoch [3/30], Step[700/1549], Loss: 5.1650, Perplexity: 175.03\n",
      "Epoch [3/30], Step[800/1549], Loss: 5.0512, Perplexity: 156.20\n",
      "Epoch [3/30], Step[900/1549], Loss: 5.0014, Perplexity: 148.63\n",
      "Epoch [3/30], Step[1000/1549], Loss: 5.0782, Perplexity: 160.49\n",
      "Epoch [3/30], Step[1100/1549], Loss: 5.2993, Perplexity: 200.19\n",
      "Epoch [3/30], Step[1200/1549], Loss: 5.2432, Perplexity: 189.27\n",
      "Epoch [3/30], Step[1300/1549], Loss: 5.0467, Perplexity: 155.52\n",
      "Epoch [3/30], Step[1400/1549], Loss: 4.9140, Perplexity: 136.19\n",
      "Epoch [3/30], Step[1500/1549], Loss: 5.2032, Perplexity: 181.86\n",
      "Epoch: 0003 train_loss = 5.0879 valid_loss = 5.4135 time = 20.7015\n",
      "Epoch [4/30], Step[0/1549], Loss: 5.0518, Perplexity: 156.30\n",
      "Epoch [4/30], Step[100/1549], Loss: 4.9896, Perplexity: 146.88\n",
      "Epoch [4/30], Step[200/1549], Loss: 4.9407, Perplexity: 139.86\n",
      "Epoch [4/30], Step[300/1549], Loss: 5.0397, Perplexity: 154.42\n",
      "Epoch [4/30], Step[400/1549], Loss: 4.9610, Perplexity: 142.74\n",
      "Epoch [4/30], Step[500/1549], Loss: 4.8237, Perplexity: 124.42\n",
      "Epoch [4/30], Step[600/1549], Loss: 5.0067, Perplexity: 149.41\n",
      "Epoch [4/30], Step[700/1549], Loss: 5.0452, Perplexity: 155.27\n",
      "Epoch [4/30], Step[800/1549], Loss: 4.9570, Perplexity: 142.17\n",
      "Epoch [4/30], Step[900/1549], Loss: 4.8891, Perplexity: 132.83\n",
      "Epoch [4/30], Step[1000/1549], Loss: 4.9826, Perplexity: 145.85\n",
      "Epoch [4/30], Step[1100/1549], Loss: 5.1790, Perplexity: 177.50\n",
      "Epoch [4/30], Step[1200/1549], Loss: 5.1248, Perplexity: 168.14\n",
      "Epoch [4/30], Step[1300/1549], Loss: 4.9261, Perplexity: 137.84\n",
      "Epoch [4/30], Step[1400/1549], Loss: 4.7952, Perplexity: 120.93\n",
      "Epoch [4/30], Step[1500/1549], Loss: 5.0895, Perplexity: 162.30\n",
      "Epoch: 0004 train_loss = 4.9735 valid_loss = 5.4100 time = 19.8195\n",
      "Epoch [5/30], Step[0/1549], Loss: 4.8199, Perplexity: 123.95\n",
      "Epoch [5/30], Step[100/1549], Loss: 4.9183, Perplexity: 136.77\n",
      "Epoch [5/30], Step[200/1549], Loss: 4.8708, Perplexity: 130.42\n",
      "Epoch [5/30], Step[300/1549], Loss: 4.9627, Perplexity: 142.98\n",
      "Epoch [5/30], Step[400/1549], Loss: 4.8828, Perplexity: 132.00\n",
      "Epoch [5/30], Step[500/1549], Loss: 4.7508, Perplexity: 115.68\n",
      "Epoch [5/30], Step[600/1549], Loss: 4.9439, Perplexity: 140.32\n",
      "Epoch [5/30], Step[700/1549], Loss: 4.9613, Perplexity: 142.78\n",
      "Epoch [5/30], Step[800/1549], Loss: 4.8881, Perplexity: 132.70\n",
      "Epoch [5/30], Step[900/1549], Loss: 4.8099, Perplexity: 122.71\n",
      "Epoch [5/30], Step[1000/1549], Loss: 4.9169, Perplexity: 136.58\n",
      "Epoch [5/30], Step[1100/1549], Loss: 5.0942, Perplexity: 163.07\n",
      "Epoch [5/30], Step[1200/1549], Loss: 5.0366, Perplexity: 153.95\n",
      "Epoch [5/30], Step[1300/1549], Loss: 4.8471, Perplexity: 127.37\n",
      "Epoch [5/30], Step[1400/1549], Loss: 4.7150, Perplexity: 111.61\n",
      "Epoch [5/30], Step[1500/1549], Loss: 5.0132, Perplexity: 150.38\n",
      "Epoch: 0005 train_loss = 4.8947 valid_loss = 5.4197 time = 20.3429\n",
      "Epoch [6/30], Step[0/1549], Loss: 4.7196, Perplexity: 112.12\n",
      "Epoch [6/30], Step[100/1549], Loss: 4.8654, Perplexity: 129.72\n",
      "Epoch [6/30], Step[200/1549], Loss: 4.8188, Perplexity: 123.82\n",
      "Epoch [6/30], Step[300/1549], Loss: 4.9078, Perplexity: 135.34\n",
      "Epoch [6/30], Step[400/1549], Loss: 4.8264, Perplexity: 124.76\n",
      "Epoch [6/30], Step[500/1549], Loss: 4.6937, Perplexity: 109.26\n",
      "Epoch [6/30], Step[600/1549], Loss: 4.8956, Perplexity: 133.71\n",
      "Epoch [6/30], Step[700/1549], Loss: 4.8994, Perplexity: 134.21\n",
      "Epoch [6/30], Step[800/1549], Loss: 4.8337, Perplexity: 125.67\n",
      "Epoch [6/30], Step[900/1549], Loss: 4.7499, Perplexity: 115.57\n",
      "Epoch [6/30], Step[1000/1549], Loss: 4.8668, Perplexity: 129.91\n",
      "Epoch [6/30], Step[1100/1549], Loss: 5.0307, Perplexity: 153.04\n",
      "Epoch [6/30], Step[1200/1549], Loss: 4.9712, Perplexity: 144.21\n",
      "Epoch [6/30], Step[1300/1549], Loss: 4.7915, Perplexity: 120.48\n",
      "Epoch [6/30], Step[1400/1549], Loss: 4.6552, Perplexity: 105.13\n",
      "Epoch [6/30], Step[1500/1549], Loss: 4.9568, Perplexity: 142.14\n",
      "Epoch: 0006 train_loss = 4.8359 valid_loss = 5.4276 time = 22.9399\n",
      "Epoch [7/30], Step[0/1549], Loss: 4.6615, Perplexity: 105.80\n",
      "Epoch [7/30], Step[100/1549], Loss: 4.8229, Perplexity: 124.32\n",
      "Epoch [7/30], Step[200/1549], Loss: 4.7763, Perplexity: 118.66\n",
      "Epoch [7/30], Step[300/1549], Loss: 4.8644, Perplexity: 129.60\n",
      "Epoch [7/30], Step[400/1549], Loss: 4.7823, Perplexity: 119.37\n",
      "Epoch [7/30], Step[500/1549], Loss: 4.6472, Perplexity: 104.29\n",
      "Epoch [7/30], Step[600/1549], Loss: 4.8565, Perplexity: 128.57\n",
      "Epoch [7/30], Step[700/1549], Loss: 4.8522, Perplexity: 128.03\n",
      "Epoch [7/30], Step[800/1549], Loss: 4.7887, Perplexity: 120.15\n",
      "Epoch [7/30], Step[900/1549], Loss: 4.7020, Perplexity: 110.16\n",
      "Epoch [7/30], Step[1000/1549], Loss: 4.8263, Perplexity: 124.75\n",
      "Epoch [7/30], Step[1100/1549], Loss: 4.9808, Perplexity: 145.59\n",
      "Epoch [7/30], Step[1200/1549], Loss: 4.9219, Perplexity: 137.26\n",
      "Epoch [7/30], Step[1300/1549], Loss: 4.7501, Perplexity: 115.60\n",
      "Epoch [7/30], Step[1400/1549], Loss: 4.6072, Perplexity: 100.21\n",
      "Epoch [7/30], Step[1500/1549], Loss: 4.9117, Perplexity: 135.87\n",
      "Epoch: 0007 train_loss = 4.7895 valid_loss = 5.4402 time = 20.9752\n",
      "Epoch [8/30], Step[0/1549], Loss: 4.6125, Perplexity: 100.74\n",
      "Epoch [8/30], Step[100/1549], Loss: 4.7871, Perplexity: 119.95\n",
      "Epoch [8/30], Step[200/1549], Loss: 4.7398, Perplexity: 114.41\n",
      "Epoch [8/30], Step[300/1549], Loss: 4.8283, Perplexity: 125.00\n",
      "Epoch [8/30], Step[400/1549], Loss: 4.7467, Perplexity: 115.20\n",
      "Epoch [8/30], Step[500/1549], Loss: 4.6088, Perplexity: 100.36\n",
      "Epoch [8/30], Step[600/1549], Loss: 4.8233, Perplexity: 124.37\n",
      "Epoch [8/30], Step[700/1549], Loss: 4.8147, Perplexity: 123.30\n",
      "Epoch [8/30], Step[800/1549], Loss: 4.7506, Perplexity: 115.66\n",
      "Epoch [8/30], Step[900/1549], Loss: 4.6622, Perplexity: 105.87\n",
      "Epoch [8/30], Step[1000/1549], Loss: 4.7925, Perplexity: 120.61\n",
      "Epoch [8/30], Step[1100/1549], Loss: 4.9395, Perplexity: 139.70\n",
      "Epoch [8/30], Step[1200/1549], Loss: 4.8839, Perplexity: 132.14\n",
      "Epoch [8/30], Step[1300/1549], Loss: 4.7170, Perplexity: 111.83\n",
      "Epoch [8/30], Step[1400/1549], Loss: 4.5671, Perplexity: 96.27\n",
      "Epoch [8/30], Step[1500/1549], Loss: 4.8738, Perplexity: 130.82\n",
      "Epoch: 0008 train_loss = 4.7514 valid_loss = 5.4534 time = 21.3192\n",
      "Epoch [9/30], Step[0/1549], Loss: 4.5715, Perplexity: 96.69\n",
      "Epoch [9/30], Step[100/1549], Loss: 4.7564, Perplexity: 116.33\n",
      "Epoch [9/30], Step[200/1549], Loss: 4.7075, Perplexity: 110.78\n",
      "Epoch [9/30], Step[300/1549], Loss: 4.7973, Perplexity: 121.18\n",
      "Epoch [9/30], Step[400/1549], Loss: 4.7173, Perplexity: 111.86\n",
      "Epoch [9/30], Step[500/1549], Loss: 4.5769, Perplexity: 97.21\n",
      "Epoch [9/30], Step[600/1549], Loss: 4.7941, Perplexity: 120.79\n",
      "Epoch [9/30], Step[700/1549], Loss: 4.7835, Perplexity: 119.52\n",
      "Epoch [9/30], Step[800/1549], Loss: 4.7175, Perplexity: 111.89\n",
      "Epoch [9/30], Step[900/1549], Loss: 4.6281, Perplexity: 102.31\n",
      "Epoch [9/30], Step[1000/1549], Loss: 4.7637, Perplexity: 117.18\n",
      "Epoch [9/30], Step[1100/1549], Loss: 4.9045, Perplexity: 134.90\n",
      "Epoch [9/30], Step[1200/1549], Loss: 4.8535, Perplexity: 128.19\n",
      "Epoch [9/30], Step[1300/1549], Loss: 4.6894, Perplexity: 108.79\n",
      "Epoch [9/30], Step[1400/1549], Loss: 4.5335, Perplexity: 93.08\n",
      "Epoch [9/30], Step[1500/1549], Loss: 4.8415, Perplexity: 126.66\n",
      "Epoch: 0009 train_loss = 4.7193 valid_loss = 5.4679 time = 20.3355\n",
      "Epoch [10/30], Step[0/1549], Loss: 4.5362, Perplexity: 93.33\n",
      "Epoch [10/30], Step[100/1549], Loss: 4.7298, Perplexity: 113.27\n",
      "Epoch [10/30], Step[200/1549], Loss: 4.6787, Perplexity: 107.63\n",
      "Epoch [10/30], Step[300/1549], Loss: 4.7702, Perplexity: 117.94\n",
      "Epoch [10/30], Step[400/1549], Loss: 4.6922, Perplexity: 109.09\n",
      "Epoch [10/30], Step[500/1549], Loss: 4.5496, Perplexity: 94.59\n",
      "Epoch [10/30], Step[600/1549], Loss: 4.7681, Perplexity: 117.70\n",
      "Epoch [10/30], Step[700/1549], Loss: 4.7570, Perplexity: 116.39\n",
      "Epoch [10/30], Step[800/1549], Loss: 4.6882, Perplexity: 108.66\n",
      "Epoch [10/30], Step[900/1549], Loss: 4.5981, Perplexity: 99.29\n",
      "Epoch [10/30], Step[1000/1549], Loss: 4.7387, Perplexity: 114.28\n",
      "Epoch [10/30], Step[1100/1549], Loss: 4.8741, Perplexity: 130.86\n",
      "Epoch [10/30], Step[1200/1549], Loss: 4.8284, Perplexity: 125.02\n",
      "Epoch [10/30], Step[1300/1549], Loss: 4.6657, Perplexity: 106.24\n",
      "Epoch [10/30], Step[1400/1549], Loss: 4.5048, Perplexity: 90.45\n",
      "Epoch [10/30], Step[1500/1549], Loss: 4.8137, Perplexity: 123.19\n",
      "Epoch: 0010 train_loss = 4.6916 valid_loss = 5.4823 time = 21.4214\n",
      "Epoch [11/30], Step[0/1549], Loss: 4.5055, Perplexity: 90.51\n",
      "Epoch [11/30], Step[100/1549], Loss: 4.7063, Perplexity: 110.64\n",
      "Epoch [11/30], Step[200/1549], Loss: 4.6525, Perplexity: 104.85\n",
      "Epoch [11/30], Step[300/1549], Loss: 4.7461, Perplexity: 115.14\n",
      "Epoch [11/30], Step[400/1549], Loss: 4.6703, Perplexity: 106.73\n",
      "Epoch [11/30], Step[500/1549], Loss: 4.5257, Perplexity: 92.36\n",
      "Epoch [11/30], Step[600/1549], Loss: 4.7451, Perplexity: 115.01\n",
      "Epoch [11/30], Step[700/1549], Loss: 4.7340, Perplexity: 113.75\n",
      "Epoch [11/30], Step[800/1549], Loss: 4.6622, Perplexity: 105.86\n",
      "Epoch [11/30], Step[900/1549], Loss: 4.5714, Perplexity: 96.68\n",
      "Epoch [11/30], Step[1000/1549], Loss: 4.7167, Perplexity: 111.80\n",
      "Epoch [11/30], Step[1100/1549], Loss: 4.8471, Perplexity: 127.38\n",
      "Epoch [11/30], Step[1200/1549], Loss: 4.8072, Perplexity: 122.39\n",
      "Epoch [11/30], Step[1300/1549], Loss: 4.6450, Perplexity: 104.06\n",
      "Epoch [11/30], Step[1400/1549], Loss: 4.4805, Perplexity: 88.28\n",
      "Epoch [11/30], Step[1500/1549], Loss: 4.7894, Perplexity: 120.23\n",
      "Epoch: 0011 train_loss = 4.6674 valid_loss = 5.4970 time = 20.2943\n",
      "Epoch [12/30], Step[0/1549], Loss: 4.4788, Perplexity: 88.13\n",
      "Epoch [12/30], Step[100/1549], Loss: 4.6854, Perplexity: 108.36\n",
      "Epoch [12/30], Step[200/1549], Loss: 4.6289, Perplexity: 102.41\n",
      "Epoch [12/30], Step[300/1549], Loss: 4.7243, Perplexity: 112.66\n",
      "Epoch [12/30], Step[400/1549], Loss: 4.6507, Perplexity: 104.66\n",
      "Epoch [12/30], Step[500/1549], Loss: 4.5046, Perplexity: 90.43\n",
      "Epoch [12/30], Step[600/1549], Loss: 4.7248, Perplexity: 112.71\n",
      "Epoch [12/30], Step[700/1549], Loss: 4.7139, Perplexity: 111.48\n",
      "Epoch [12/30], Step[800/1549], Loss: 4.6388, Perplexity: 103.42\n",
      "Epoch [12/30], Step[900/1549], Loss: 4.5475, Perplexity: 94.39\n",
      "Epoch [12/30], Step[1000/1549], Loss: 4.6973, Perplexity: 109.66\n",
      "Epoch [12/30], Step[1100/1549], Loss: 4.8228, Perplexity: 124.31\n",
      "Epoch [12/30], Step[1200/1549], Loss: 4.7891, Perplexity: 120.19\n",
      "Epoch [12/30], Step[1300/1549], Loss: 4.6265, Perplexity: 102.15\n",
      "Epoch [12/30], Step[1400/1549], Loss: 4.4593, Perplexity: 86.43\n",
      "Epoch [12/30], Step[1500/1549], Loss: 4.7680, Perplexity: 117.69\n",
      "Epoch: 0012 train_loss = 4.6460 valid_loss = 5.5111 time = 20.0611\n",
      "Epoch [13/30], Step[0/1549], Loss: 4.4560, Perplexity: 86.14\n",
      "Epoch [13/30], Step[100/1549], Loss: 4.6668, Perplexity: 106.35\n",
      "Epoch [13/30], Step[200/1549], Loss: 4.6076, Perplexity: 100.25\n",
      "Epoch [13/30], Step[300/1549], Loss: 4.7046, Perplexity: 110.45\n",
      "Epoch [13/30], Step[400/1549], Loss: 4.6332, Perplexity: 102.84\n",
      "Epoch [13/30], Step[500/1549], Loss: 4.4858, Perplexity: 88.74\n",
      "Epoch [13/30], Step[600/1549], Loss: 4.7069, Perplexity: 110.71\n",
      "Epoch [13/30], Step[700/1549], Loss: 4.6961, Perplexity: 109.52\n",
      "Epoch [13/30], Step[800/1549], Loss: 4.6178, Perplexity: 101.28\n",
      "Epoch [13/30], Step[900/1549], Loss: 4.5260, Perplexity: 92.38\n",
      "Epoch [13/30], Step[1000/1549], Loss: 4.6801, Perplexity: 107.78\n",
      "Epoch [13/30], Step[1100/1549], Loss: 4.8008, Perplexity: 121.60\n",
      "Epoch [13/30], Step[1200/1549], Loss: 4.7733, Perplexity: 118.30\n",
      "Epoch [13/30], Step[1300/1549], Loss: 4.6097, Perplexity: 100.46\n",
      "Epoch [13/30], Step[1400/1549], Loss: 4.4411, Perplexity: 84.87\n",
      "Epoch [13/30], Step[1500/1549], Loss: 4.7490, Perplexity: 115.47\n",
      "Epoch: 0013 train_loss = 4.6270 valid_loss = 5.5248 time = 20.2146\n",
      "Epoch [14/30], Step[0/1549], Loss: 4.4367, Perplexity: 84.49\n",
      "Epoch [14/30], Step[100/1549], Loss: 4.6500, Perplexity: 104.59\n",
      "Epoch [14/30], Step[200/1549], Loss: 4.5885, Perplexity: 98.35\n",
      "Epoch [14/30], Step[300/1549], Loss: 4.6863, Perplexity: 108.45\n",
      "Epoch [14/30], Step[400/1549], Loss: 4.6171, Perplexity: 101.20\n",
      "Epoch [14/30], Step[500/1549], Loss: 4.4688, Perplexity: 87.25\n",
      "Epoch [14/30], Step[600/1549], Loss: 4.6914, Perplexity: 109.01\n",
      "Epoch [14/30], Step[700/1549], Loss: 4.6803, Perplexity: 107.81\n",
      "Epoch [14/30], Step[800/1549], Loss: 4.5991, Perplexity: 99.39\n",
      "Epoch [14/30], Step[900/1549], Loss: 4.5066, Perplexity: 90.62\n",
      "Epoch [14/30], Step[1000/1549], Loss: 4.6646, Perplexity: 106.13\n",
      "Epoch [14/30], Step[1100/1549], Loss: 4.7807, Perplexity: 119.19\n",
      "Epoch [14/30], Step[1200/1549], Loss: 4.7593, Perplexity: 116.67\n",
      "Epoch [14/30], Step[1300/1549], Loss: 4.5944, Perplexity: 98.93\n",
      "Epoch [14/30], Step[1400/1549], Loss: 4.4249, Perplexity: 83.51\n",
      "Epoch [14/30], Step[1500/1549], Loss: 4.7320, Perplexity: 113.52\n",
      "Epoch: 0014 train_loss = 4.6100 valid_loss = 5.5377 time = 19.8681\n",
      "Epoch [15/30], Step[0/1549], Loss: 4.4202, Perplexity: 83.11\n",
      "Epoch [15/30], Step[100/1549], Loss: 4.6351, Perplexity: 103.03\n",
      "Epoch [15/30], Step[200/1549], Loss: 4.5715, Perplexity: 96.69\n",
      "Epoch [15/30], Step[300/1549], Loss: 4.6697, Perplexity: 106.66\n",
      "Epoch [15/30], Step[400/1549], Loss: 4.6024, Perplexity: 99.72\n",
      "Epoch [15/30], Step[500/1549], Loss: 4.4535, Perplexity: 85.93\n",
      "Epoch [15/30], Step[600/1549], Loss: 4.6778, Perplexity: 107.53\n",
      "Epoch [15/30], Step[700/1549], Loss: 4.6664, Perplexity: 106.31\n",
      "Epoch [15/30], Step[800/1549], Loss: 4.5823, Perplexity: 97.74\n",
      "Epoch [15/30], Step[900/1549], Loss: 4.4893, Perplexity: 89.06\n",
      "Epoch [15/30], Step[1000/1549], Loss: 4.6506, Perplexity: 104.64\n",
      "Epoch [15/30], Step[1100/1549], Loss: 4.7626, Perplexity: 117.05\n",
      "Epoch [15/30], Step[1200/1549], Loss: 4.7468, Perplexity: 115.21\n",
      "Epoch [15/30], Step[1300/1549], Loss: 4.5802, Perplexity: 97.53\n",
      "Epoch [15/30], Step[1400/1549], Loss: 4.4109, Perplexity: 82.34\n",
      "Epoch [15/30], Step[1500/1549], Loss: 4.7166, Perplexity: 111.79\n",
      "Epoch: 0015 train_loss = 4.5946 valid_loss = 5.5499 time = 19.3237\n",
      "Epoch [16/30], Step[0/1549], Loss: 4.4060, Perplexity: 81.94\n",
      "Epoch [16/30], Step[100/1549], Loss: 4.6216, Perplexity: 101.66\n",
      "Epoch [16/30], Step[200/1549], Loss: 4.5564, Perplexity: 95.24\n",
      "Epoch [16/30], Step[300/1549], Loss: 4.6542, Perplexity: 105.03\n",
      "Epoch [16/30], Step[400/1549], Loss: 4.5887, Perplexity: 98.37\n",
      "Epoch [16/30], Step[500/1549], Loss: 4.4396, Perplexity: 84.74\n",
      "Epoch [16/30], Step[600/1549], Loss: 4.6659, Perplexity: 106.26\n",
      "Epoch [16/30], Step[700/1549], Loss: 4.6539, Perplexity: 104.99\n",
      "Epoch [16/30], Step[800/1549], Loss: 4.5672, Perplexity: 96.27\n",
      "Epoch [16/30], Step[900/1549], Loss: 4.4737, Perplexity: 87.68\n",
      "Epoch [16/30], Step[1000/1549], Loss: 4.6377, Perplexity: 103.31\n",
      "Epoch [16/30], Step[1100/1549], Loss: 4.7462, Perplexity: 115.15\n",
      "Epoch [16/30], Step[1200/1549], Loss: 4.7353, Perplexity: 113.90\n",
      "Epoch [16/30], Step[1300/1549], Loss: 4.5669, Perplexity: 96.25\n",
      "Epoch [16/30], Step[1400/1549], Loss: 4.3981, Perplexity: 81.29\n",
      "Epoch [16/30], Step[1500/1549], Loss: 4.7025, Perplexity: 110.22\n",
      "Epoch: 0016 train_loss = 4.5808 valid_loss = 5.5614 time = 20.1485\n",
      "Epoch [17/30], Step[0/1549], Loss: 4.3936, Perplexity: 80.93\n",
      "Epoch [17/30], Step[100/1549], Loss: 4.6096, Perplexity: 100.44\n",
      "Epoch [17/30], Step[200/1549], Loss: 4.5430, Perplexity: 93.98\n",
      "Epoch [17/30], Step[300/1549], Loss: 4.6401, Perplexity: 103.56\n",
      "Epoch [17/30], Step[400/1549], Loss: 4.5760, Perplexity: 97.13\n",
      "Epoch [17/30], Step[500/1549], Loss: 4.4269, Perplexity: 83.67\n",
      "Epoch [17/30], Step[600/1549], Loss: 4.6551, Perplexity: 105.12\n",
      "Epoch [17/30], Step[700/1549], Loss: 4.6427, Perplexity: 103.82\n",
      "Epoch [17/30], Step[800/1549], Loss: 4.5537, Perplexity: 94.99\n",
      "Epoch [17/30], Step[900/1549], Loss: 4.4597, Perplexity: 86.46\n",
      "Epoch [17/30], Step[1000/1549], Loss: 4.6259, Perplexity: 102.10\n",
      "Epoch [17/30], Step[1100/1549], Loss: 4.7314, Perplexity: 113.45\n",
      "Epoch [17/30], Step[1200/1549], Loss: 4.7246, Perplexity: 112.69\n",
      "Epoch [17/30], Step[1300/1549], Loss: 4.5547, Perplexity: 95.08\n",
      "Epoch [17/30], Step[1400/1549], Loss: 4.3867, Perplexity: 80.37\n",
      "Epoch [17/30], Step[1500/1549], Loss: 4.6894, Perplexity: 108.78\n",
      "Epoch: 0017 train_loss = 4.5681 valid_loss = 5.5723 time = 19.9407\n",
      "Epoch [18/30], Step[0/1549], Loss: 4.3825, Perplexity: 80.04\n",
      "Epoch [18/30], Step[100/1549], Loss: 4.5987, Perplexity: 99.35\n",
      "Epoch [18/30], Step[200/1549], Loss: 4.5313, Perplexity: 92.88\n",
      "Epoch [18/30], Step[300/1549], Loss: 4.6270, Perplexity: 102.21\n",
      "Epoch [18/30], Step[400/1549], Loss: 4.5641, Perplexity: 95.98\n",
      "Epoch [18/30], Step[500/1549], Loss: 4.4152, Perplexity: 82.70\n",
      "Epoch [18/30], Step[600/1549], Loss: 4.6453, Perplexity: 104.09\n",
      "Epoch [18/30], Step[700/1549], Loss: 4.6324, Perplexity: 102.76\n",
      "Epoch [18/30], Step[800/1549], Loss: 4.5418, Perplexity: 93.86\n",
      "Epoch [18/30], Step[900/1549], Loss: 4.4470, Perplexity: 85.37\n",
      "Epoch [18/30], Step[1000/1549], Loss: 4.6151, Perplexity: 100.99\n",
      "Epoch [18/30], Step[1100/1549], Loss: 4.7180, Perplexity: 111.94\n",
      "Epoch [18/30], Step[1200/1549], Loss: 4.7146, Perplexity: 111.57\n",
      "Epoch [18/30], Step[1300/1549], Loss: 4.5432, Perplexity: 94.00\n",
      "Epoch [18/30], Step[1400/1549], Loss: 4.3761, Perplexity: 79.53\n",
      "Epoch [18/30], Step[1500/1549], Loss: 4.6771, Perplexity: 107.45\n",
      "Epoch: 0018 train_loss = 4.5566 valid_loss = 5.5826 time = 20.3292\n",
      "Epoch [19/30], Step[0/1549], Loss: 4.3724, Perplexity: 79.23\n",
      "Epoch [19/30], Step[100/1549], Loss: 4.5888, Perplexity: 98.37\n",
      "Epoch [19/30], Step[200/1549], Loss: 4.5209, Perplexity: 91.92\n",
      "Epoch [19/30], Step[300/1549], Loss: 4.6149, Perplexity: 100.98\n",
      "Epoch [19/30], Step[400/1549], Loss: 4.5530, Perplexity: 94.92\n",
      "Epoch [19/30], Step[500/1549], Loss: 4.4044, Perplexity: 81.81\n",
      "Epoch [19/30], Step[600/1549], Loss: 4.6360, Perplexity: 103.13\n",
      "Epoch [19/30], Step[700/1549], Loss: 4.6229, Perplexity: 101.79\n",
      "Epoch [19/30], Step[800/1549], Loss: 4.5311, Perplexity: 92.87\n",
      "Epoch [19/30], Step[900/1549], Loss: 4.4353, Perplexity: 84.38\n",
      "Epoch [19/30], Step[1000/1549], Loss: 4.6050, Perplexity: 99.98\n",
      "Epoch [19/30], Step[1100/1549], Loss: 4.7058, Perplexity: 110.58\n",
      "Epoch [19/30], Step[1200/1549], Loss: 4.7053, Perplexity: 110.53\n",
      "Epoch [19/30], Step[1300/1549], Loss: 4.5327, Perplexity: 93.00\n",
      "Epoch [19/30], Step[1400/1549], Loss: 4.3666, Perplexity: 78.78\n",
      "Epoch [19/30], Step[1500/1549], Loss: 4.6655, Perplexity: 106.21\n",
      "Epoch: 0019 train_loss = 4.5460 valid_loss = 5.5925 time = 20.3363\n",
      "Epoch [20/30], Step[0/1549], Loss: 4.3632, Perplexity: 78.51\n",
      "Epoch [20/30], Step[100/1549], Loss: 4.5797, Perplexity: 97.48\n",
      "Epoch [20/30], Step[200/1549], Loss: 4.5118, Perplexity: 91.08\n",
      "Epoch [20/30], Step[300/1549], Loss: 4.6037, Perplexity: 99.85\n",
      "Epoch [20/30], Step[400/1549], Loss: 4.5426, Perplexity: 93.94\n",
      "Epoch [20/30], Step[500/1549], Loss: 4.3944, Perplexity: 81.00\n",
      "Epoch [20/30], Step[600/1549], Loss: 4.6273, Perplexity: 102.23\n",
      "Epoch [20/30], Step[700/1549], Loss: 4.6140, Perplexity: 100.89\n",
      "Epoch [20/30], Step[800/1549], Loss: 4.5216, Perplexity: 91.98\n",
      "Epoch [20/30], Step[900/1549], Loss: 4.4246, Perplexity: 83.48\n",
      "Epoch [20/30], Step[1000/1549], Loss: 4.5957, Perplexity: 99.05\n",
      "Epoch [20/30], Step[1100/1549], Loss: 4.6946, Perplexity: 109.36\n",
      "Epoch [20/30], Step[1200/1549], Loss: 4.6964, Perplexity: 109.56\n",
      "Epoch [20/30], Step[1300/1549], Loss: 4.5228, Perplexity: 92.09\n",
      "Epoch [20/30], Step[1400/1549], Loss: 4.3577, Perplexity: 78.08\n",
      "Epoch [20/30], Step[1500/1549], Loss: 4.6544, Perplexity: 105.05\n",
      "Epoch: 0020 train_loss = 4.5363 valid_loss = 5.6019 time = 20.0899\n",
      "Epoch [21/30], Step[0/1549], Loss: 4.3546, Perplexity: 77.83\n",
      "Epoch [21/30], Step[100/1549], Loss: 4.5713, Perplexity: 96.67\n",
      "Epoch [21/30], Step[200/1549], Loss: 4.5037, Perplexity: 90.35\n",
      "Epoch [21/30], Step[300/1549], Loss: 4.5933, Perplexity: 98.82\n",
      "Epoch [21/30], Step[400/1549], Loss: 4.5331, Perplexity: 93.04\n",
      "Epoch [21/30], Step[500/1549], Loss: 4.3852, Perplexity: 80.25\n",
      "Epoch [21/30], Step[600/1549], Loss: 4.6187, Perplexity: 101.36\n",
      "Epoch [21/30], Step[700/1549], Loss: 4.6057, Perplexity: 100.06\n",
      "Epoch [21/30], Step[800/1549], Loss: 4.5130, Perplexity: 91.20\n",
      "Epoch [21/30], Step[900/1549], Loss: 4.4147, Perplexity: 82.66\n",
      "Epoch [21/30], Step[1000/1549], Loss: 4.5869, Perplexity: 98.19\n",
      "Epoch [21/30], Step[1100/1549], Loss: 4.6843, Perplexity: 108.24\n",
      "Epoch [21/30], Step[1200/1549], Loss: 4.6881, Perplexity: 108.64\n",
      "Epoch [21/30], Step[1300/1549], Loss: 4.5136, Perplexity: 91.25\n",
      "Epoch [21/30], Step[1400/1549], Loss: 4.3497, Perplexity: 77.45\n",
      "Epoch [21/30], Step[1500/1549], Loss: 4.6440, Perplexity: 103.96\n",
      "Epoch: 0021 train_loss = 4.5273 valid_loss = 5.6109 time = 20.0195\n",
      "Epoch [22/30], Step[0/1549], Loss: 4.3466, Perplexity: 77.21\n",
      "Epoch [22/30], Step[100/1549], Loss: 4.5634, Perplexity: 95.91\n",
      "Epoch [22/30], Step[200/1549], Loss: 4.4965, Perplexity: 89.71\n",
      "Epoch [22/30], Step[300/1549], Loss: 4.5836, Perplexity: 97.87\n",
      "Epoch [22/30], Step[400/1549], Loss: 4.5241, Perplexity: 92.22\n",
      "Epoch [22/30], Step[500/1549], Loss: 4.3766, Perplexity: 79.56\n",
      "Epoch [22/30], Step[600/1549], Loss: 4.6105, Perplexity: 100.54\n",
      "Epoch [22/30], Step[700/1549], Loss: 4.5979, Perplexity: 99.28\n",
      "Epoch [22/30], Step[800/1549], Loss: 4.5051, Perplexity: 90.48\n",
      "Epoch [22/30], Step[900/1549], Loss: 4.4057, Perplexity: 81.91\n",
      "Epoch [22/30], Step[1000/1549], Loss: 4.5788, Perplexity: 97.40\n",
      "Epoch [22/30], Step[1100/1549], Loss: 4.6748, Perplexity: 107.21\n",
      "Epoch [22/30], Step[1200/1549], Loss: 4.6802, Perplexity: 107.79\n",
      "Epoch [22/30], Step[1300/1549], Loss: 4.5050, Perplexity: 90.47\n",
      "Epoch [22/30], Step[1400/1549], Loss: 4.3421, Perplexity: 76.87\n",
      "Epoch [22/30], Step[1500/1549], Loss: 4.6341, Perplexity: 102.94\n",
      "Epoch: 0022 train_loss = 4.5189 valid_loss = 5.6196 time = 20.2560\n",
      "Epoch [23/30], Step[0/1549], Loss: 4.3391, Perplexity: 76.64\n",
      "Epoch [23/30], Step[100/1549], Loss: 4.5561, Perplexity: 95.21\n",
      "Epoch [23/30], Step[200/1549], Loss: 4.4901, Perplexity: 89.13\n",
      "Epoch [23/30], Step[300/1549], Loss: 4.5747, Perplexity: 97.00\n",
      "Epoch [23/30], Step[400/1549], Loss: 4.5160, Perplexity: 91.47\n",
      "Epoch [23/30], Step[500/1549], Loss: 4.3686, Perplexity: 78.93\n",
      "Epoch [23/30], Step[600/1549], Loss: 4.6024, Perplexity: 99.73\n",
      "Epoch [23/30], Step[700/1549], Loss: 4.5905, Perplexity: 98.54\n",
      "Epoch [23/30], Step[800/1549], Loss: 4.4979, Perplexity: 89.83\n",
      "Epoch [23/30], Step[900/1549], Loss: 4.3973, Perplexity: 81.23\n",
      "Epoch [23/30], Step[1000/1549], Loss: 4.5712, Perplexity: 96.66\n",
      "Epoch [23/30], Step[1100/1549], Loss: 4.6660, Perplexity: 106.27\n",
      "Epoch [23/30], Step[1200/1549], Loss: 4.6727, Perplexity: 106.98\n",
      "Epoch [23/30], Step[1300/1549], Loss: 4.4970, Perplexity: 89.75\n",
      "Epoch [23/30], Step[1400/1549], Loss: 4.3354, Perplexity: 76.35\n",
      "Epoch [23/30], Step[1500/1549], Loss: 4.6248, Perplexity: 101.98\n",
      "Epoch: 0023 train_loss = 4.5112 valid_loss = 5.6278 time = 19.9811\n",
      "Epoch [24/30], Step[0/1549], Loss: 4.3322, Perplexity: 76.11\n",
      "Epoch [24/30], Step[100/1549], Loss: 4.5491, Perplexity: 94.55\n",
      "Epoch [24/30], Step[200/1549], Loss: 4.4843, Perplexity: 88.61\n",
      "Epoch [24/30], Step[300/1549], Loss: 4.5664, Perplexity: 96.20\n",
      "Epoch [24/30], Step[400/1549], Loss: 4.5085, Perplexity: 90.79\n",
      "Epoch [24/30], Step[500/1549], Loss: 4.3611, Perplexity: 78.34\n",
      "Epoch [24/30], Step[600/1549], Loss: 4.5946, Perplexity: 98.95\n",
      "Epoch [24/30], Step[700/1549], Loss: 4.5835, Perplexity: 97.85\n",
      "Epoch [24/30], Step[800/1549], Loss: 4.4912, Perplexity: 89.23\n",
      "Epoch [24/30], Step[900/1549], Loss: 4.3896, Perplexity: 80.61\n",
      "Epoch [24/30], Step[1000/1549], Loss: 4.5640, Perplexity: 95.96\n",
      "Epoch [24/30], Step[1100/1549], Loss: 4.6577, Perplexity: 105.39\n",
      "Epoch [24/30], Step[1200/1549], Loss: 4.6656, Perplexity: 106.23\n",
      "Epoch [24/30], Step[1300/1549], Loss: 4.4895, Perplexity: 89.08\n",
      "Epoch [24/30], Step[1400/1549], Loss: 4.3290, Perplexity: 75.87\n",
      "Epoch [24/30], Step[1500/1549], Loss: 4.6159, Perplexity: 101.08\n",
      "Epoch: 0024 train_loss = 4.5040 valid_loss = 5.6357 time = 20.1501\n",
      "Epoch [25/30], Step[0/1549], Loss: 4.3256, Perplexity: 75.61\n",
      "Epoch [25/30], Step[100/1549], Loss: 4.5426, Perplexity: 93.93\n",
      "Epoch [25/30], Step[200/1549], Loss: 4.4789, Perplexity: 88.14\n",
      "Epoch [25/30], Step[300/1549], Loss: 4.5588, Perplexity: 95.47\n",
      "Epoch [25/30], Step[400/1549], Loss: 4.5017, Perplexity: 90.17\n",
      "Epoch [25/30], Step[500/1549], Loss: 4.3542, Perplexity: 77.80\n",
      "Epoch [25/30], Step[600/1549], Loss: 4.5869, Perplexity: 98.19\n",
      "Epoch [25/30], Step[700/1549], Loss: 4.5767, Perplexity: 97.20\n",
      "Epoch [25/30], Step[800/1549], Loss: 4.4850, Perplexity: 88.68\n",
      "Epoch [25/30], Step[900/1549], Loss: 4.3824, Perplexity: 80.03\n",
      "Epoch [25/30], Step[1000/1549], Loss: 4.5570, Perplexity: 95.30\n",
      "Epoch [25/30], Step[1100/1549], Loss: 4.6501, Perplexity: 104.59\n",
      "Epoch [25/30], Step[1200/1549], Loss: 4.6588, Perplexity: 105.51\n",
      "Epoch [25/30], Step[1300/1549], Loss: 4.4824, Perplexity: 88.45\n",
      "Epoch [25/30], Step[1400/1549], Loss: 4.3234, Perplexity: 75.44\n",
      "Epoch [25/30], Step[1500/1549], Loss: 4.6076, Perplexity: 100.24\n",
      "Epoch: 0025 train_loss = 4.4972 valid_loss = 5.6432 time = 21.1151\n",
      "Epoch [26/30], Step[0/1549], Loss: 4.3195, Perplexity: 75.15\n",
      "Epoch [26/30], Step[100/1549], Loss: 4.5363, Perplexity: 93.35\n",
      "Epoch [26/30], Step[200/1549], Loss: 4.4740, Perplexity: 87.71\n",
      "Epoch [26/30], Step[300/1549], Loss: 4.5517, Perplexity: 94.79\n",
      "Epoch [26/30], Step[400/1549], Loss: 4.4955, Perplexity: 89.62\n",
      "Epoch [26/30], Step[500/1549], Loss: 4.3477, Perplexity: 77.30\n",
      "Epoch [26/30], Step[600/1549], Loss: 4.5795, Perplexity: 97.46\n",
      "Epoch [26/30], Step[700/1549], Loss: 4.5703, Perplexity: 96.57\n",
      "Epoch [26/30], Step[800/1549], Loss: 4.4792, Perplexity: 88.17\n",
      "Epoch [26/30], Step[900/1549], Loss: 4.3759, Perplexity: 79.51\n",
      "Epoch [26/30], Step[1000/1549], Loss: 4.5505, Perplexity: 94.68\n",
      "Epoch [26/30], Step[1100/1549], Loss: 4.6429, Perplexity: 103.85\n",
      "Epoch [26/30], Step[1200/1549], Loss: 4.6524, Perplexity: 104.84\n",
      "Epoch [26/30], Step[1300/1549], Loss: 4.4757, Perplexity: 87.86\n",
      "Epoch [26/30], Step[1400/1549], Loss: 4.3181, Perplexity: 75.04\n",
      "Epoch [26/30], Step[1500/1549], Loss: 4.5997, Perplexity: 99.45\n",
      "Epoch: 0026 train_loss = 4.4909 valid_loss = 5.6504 time = 21.0112\n",
      "Epoch [27/30], Step[0/1549], Loss: 4.3138, Perplexity: 74.72\n",
      "Epoch [27/30], Step[100/1549], Loss: 4.5304, Perplexity: 92.80\n",
      "Epoch [27/30], Step[200/1549], Loss: 4.4694, Perplexity: 87.30\n",
      "Epoch [27/30], Step[300/1549], Loss: 4.5451, Perplexity: 94.17\n",
      "Epoch [27/30], Step[400/1549], Loss: 4.4900, Perplexity: 89.12\n",
      "Epoch [27/30], Step[500/1549], Loss: 4.3418, Perplexity: 76.84\n",
      "Epoch [27/30], Step[600/1549], Loss: 4.5722, Perplexity: 96.75\n",
      "Epoch [27/30], Step[700/1549], Loss: 4.5640, Perplexity: 95.97\n",
      "Epoch [27/30], Step[800/1549], Loss: 4.4738, Perplexity: 87.69\n",
      "Epoch [27/30], Step[900/1549], Loss: 4.3698, Perplexity: 79.03\n",
      "Epoch [27/30], Step[1000/1549], Loss: 4.5441, Perplexity: 94.08\n",
      "Epoch [27/30], Step[1100/1549], Loss: 4.6363, Perplexity: 103.17\n",
      "Epoch [27/30], Step[1200/1549], Loss: 4.6463, Perplexity: 104.20\n",
      "Epoch [27/30], Step[1300/1549], Loss: 4.4694, Perplexity: 87.31\n",
      "Epoch [27/30], Step[1400/1549], Loss: 4.3133, Perplexity: 74.69\n",
      "Epoch [27/30], Step[1500/1549], Loss: 4.5922, Perplexity: 98.71\n",
      "Epoch: 0027 train_loss = 4.4850 valid_loss = 5.6573 time = 20.1085\n",
      "Epoch [28/30], Step[0/1549], Loss: 4.3083, Perplexity: 74.32\n",
      "Epoch [28/30], Step[100/1549], Loss: 4.5248, Perplexity: 92.28\n",
      "Epoch [28/30], Step[200/1549], Loss: 4.4651, Perplexity: 86.93\n",
      "Epoch [28/30], Step[300/1549], Loss: 4.5390, Perplexity: 93.59\n",
      "Epoch [28/30], Step[400/1549], Loss: 4.4849, Perplexity: 88.67\n",
      "Epoch [28/30], Step[500/1549], Loss: 4.3362, Perplexity: 76.41\n",
      "Epoch [28/30], Step[600/1549], Loss: 4.5651, Perplexity: 96.07\n",
      "Epoch [28/30], Step[700/1549], Loss: 4.5580, Perplexity: 95.39\n",
      "Epoch [28/30], Step[800/1549], Loss: 4.4687, Perplexity: 87.24\n",
      "Epoch [28/30], Step[900/1549], Loss: 4.3641, Perplexity: 78.58\n",
      "Epoch [28/30], Step[1000/1549], Loss: 4.5381, Perplexity: 93.51\n",
      "Epoch [28/30], Step[1100/1549], Loss: 4.6302, Perplexity: 102.53\n",
      "Epoch [28/30], Step[1200/1549], Loss: 4.6405, Perplexity: 103.59\n",
      "Epoch [28/30], Step[1300/1549], Loss: 4.4634, Perplexity: 86.78\n",
      "Epoch [28/30], Step[1400/1549], Loss: 4.3087, Perplexity: 74.35\n",
      "Epoch [28/30], Step[1500/1549], Loss: 4.5850, Perplexity: 98.01\n",
      "Epoch: 0028 train_loss = 4.4793 valid_loss = 5.6640 time = 20.4946\n",
      "Epoch [29/30], Step[0/1549], Loss: 4.3031, Perplexity: 73.93\n",
      "Epoch [29/30], Step[100/1549], Loss: 4.5195, Perplexity: 91.79\n",
      "Epoch [29/30], Step[200/1549], Loss: 4.4610, Perplexity: 86.57\n",
      "Epoch [29/30], Step[300/1549], Loss: 4.5332, Perplexity: 93.05\n",
      "Epoch [29/30], Step[400/1549], Loss: 4.4804, Perplexity: 88.27\n",
      "Epoch [29/30], Step[500/1549], Loss: 4.3309, Perplexity: 76.01\n",
      "Epoch [29/30], Step[600/1549], Loss: 4.5583, Perplexity: 95.42\n",
      "Epoch [29/30], Step[700/1549], Loss: 4.5522, Perplexity: 94.84\n",
      "Epoch [29/30], Step[800/1549], Loss: 4.4639, Perplexity: 86.83\n",
      "Epoch [29/30], Step[900/1549], Loss: 4.3589, Perplexity: 78.17\n",
      "Epoch [29/30], Step[1000/1549], Loss: 4.5322, Perplexity: 92.97\n",
      "Epoch [29/30], Step[1100/1549], Loss: 4.6244, Perplexity: 101.94\n",
      "Epoch [29/30], Step[1200/1549], Loss: 4.6349, Perplexity: 103.02\n",
      "Epoch [29/30], Step[1300/1549], Loss: 4.4576, Perplexity: 86.28\n",
      "Epoch [29/30], Step[1400/1549], Loss: 4.3046, Perplexity: 74.04\n",
      "Epoch [29/30], Step[1500/1549], Loss: 4.5783, Perplexity: 97.35\n",
      "Epoch: 0029 train_loss = 4.4740 valid_loss = 5.6704 time = 20.4500\n",
      "Epoch [30/30], Step[0/1549], Loss: 4.2981, Perplexity: 73.56\n",
      "Epoch [30/30], Step[100/1549], Loss: 4.5144, Perplexity: 91.32\n",
      "Epoch [30/30], Step[200/1549], Loss: 4.4571, Perplexity: 86.24\n",
      "Epoch [30/30], Step[300/1549], Loss: 4.5277, Perplexity: 92.55\n",
      "Epoch [30/30], Step[400/1549], Loss: 4.4763, Perplexity: 87.91\n",
      "Epoch [30/30], Step[500/1549], Loss: 4.3260, Perplexity: 75.64\n",
      "Epoch [30/30], Step[600/1549], Loss: 4.5517, Perplexity: 94.79\n",
      "Epoch [30/30], Step[700/1549], Loss: 4.5466, Perplexity: 94.31\n",
      "Epoch [30/30], Step[800/1549], Loss: 4.4594, Perplexity: 86.44\n",
      "Epoch [30/30], Step[900/1549], Loss: 4.3541, Perplexity: 77.80\n",
      "Epoch [30/30], Step[1000/1549], Loss: 4.5266, Perplexity: 92.45\n",
      "Epoch [30/30], Step[1100/1549], Loss: 4.6189, Perplexity: 101.39\n",
      "Epoch [30/30], Step[1200/1549], Loss: 4.6295, Perplexity: 102.47\n",
      "Epoch [30/30], Step[1300/1549], Loss: 4.4521, Perplexity: 85.81\n",
      "Epoch [30/30], Step[1400/1549], Loss: 4.3007, Perplexity: 73.75\n",
      "Epoch [30/30], Step[1500/1549], Loss: 4.5719, Perplexity: 96.73\n",
      "Epoch: 0030 train_loss = 4.4690 valid_loss = 5.6766 time = 20.8732\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAKnCAYAAABqJ7ddAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACloUlEQVR4nOzdeXxU1f3/8fdkJpnJvu+EhC1h32SRTauALIprq0Uq0lr9WbVSKdZiXUBaY7VSbHGptn7pptSiohVcAAVRVlkUkCVAQgIkLAnZk5lkZn5/TBiIbIEk3Enyej4e88jMuWfu/VzHgbw5555rcrvdbgEAAAAAGsXP6AIAAAAAoDUgXAEAAABAEyBcAQAAAEATIFwBAAAAQBMgXAEAAABAEyBcAQAAAEATIFwBAAAAQBMgXAEAAABAE7AYXYAvcrlcOnTokEJDQ2UymYwuBwAAAIBB3G63ysrKlJSUJD+/c49NEa7O4NChQ0pJSTG6DAAAAAA+Ii8vT+3atTtnH8LVGYSGhkry/AcMCwszuBoAAAAARiktLVVKSoo3I5wL4eoMTkwFDAsLI1wBAAAAaNDlQixoAQAAAABNgHAFAAAAAE2AcAUAAAAATYBrrgAAAIAzcLvdqq2tldPpNLoUNCOz2SyLxdIkt2AiXAEAAADf4XA4lJ+fr8rKSqNLwSUQFBSkxMREBQQENGo/hCsAAADgFC6XS9nZ2TKbzUpKSlJAQECTjGrA97jdbjkcDh09elTZ2dnq0qXLeW8UfC6EKwAAAOAUDodDLpdLKSkpCgoKMrocNLPAwED5+/tr//79cjgcstlsF70vFrQAAAAAzqAxIxhoWZrqs+b/GB/3ty+y9bN/bVR+SZXRpQAAAAA4B8KVj3tn0wF9uK1Am/YXG10KAAAAgHMgXPm4vikRkqQteceNLQQAAABtSlpamubOndsk+1qxYoVMJpOKi4ubZH++inDl406Gq2JD6wAAAIDv+973vqdf/OIXTbKvDRs26J577mmSfV1q//3vf9W1a1fZbDb16tVLS5YsuSTHJVz5uH7tIyRJ3xwoUY3TZWwxAAAAaNFO3Bi5IWJjY1vkaomrV6/WxIkTddddd2nz5s268cYbdeONN2rbtm3NfmzClY/rGBOiUJtF9lqXdhWUGV0OAABAm+R2u1XpqDXk4Xa7G1TjlClTtHLlSr3wwgsymUwymUyaP3++TCaTPvzwQ1122WWyWq364osvtHfvXt1www2Kj49XSEiIBg4cqGXLltXb33enBZpMJv31r3/VTTfdpKCgIHXp0kXvv//+Rf83ffvtt9WjRw9ZrValpaXp+eefr7f9pZdeUpcuXWSz2RQfH6/vf//73m0LFy5Ur169FBgYqOjoaI0aNUoVFRWSpBdeeEFjx47Vww8/rG7dumn27Nnq37+/5s2bd9G1NhT3ufJxfn4m9U2J0KqsY9qcV6yeyeFGlwQAANDmVNU41f2Jjw059rdPjVFQwPl/bX/hhRe0e/du9ezZU0899ZQkafv27ZKkX//61/rDH/6gjh07KjIyUnl5eRo/frx+97vfyWq16h//+IcmTJigXbt2qX379mc9xqxZs/Tss8/queee05///GdNmjRJ+/fvV1RU1AWd08aNG3Xrrbdq5syZuu2227R69Wrdd999io6O1pQpU/TVV1/pwQcf1D//+U8NHTpURUVFWrVqlSQpPz9fEydO1LPPPqubbrpJZWVlWrVqlTeErlmzRtOmTat3vDFjxmjRokUXVOPFIFy1ACfC1ZbcYt1xearR5QAAAMAHhYeHKyAgQEFBQUpISJAk7dy5U5L01FNPafTo0d6+UVFR6tOnj/f17Nmz9e677+r999/XAw88cNZjTJkyRRMnTpQkPf300/rTn/6k9evXa+zYsRdU65w5czRy5Eg9/vjjkqT09HR9++23eu655zRlyhTl5uYqODhY1113nUJDQ5Wamqp+/fpJ8oSr2tpa3XzzzUpN9fxu3KtXL+++CwoKFB8fX+948fHxKigouKAaLwbhqgVgxUAAAABjBfqb9e1TYww7dmMNGDCg3uvy8nLNnDlTixcv9oaVqqoq5ebmnnM/vXv39j4PDg5WWFiYjhw5csH17NixQzfccEO9tmHDhmnu3LlyOp0aPXq0UlNT1bFjR40dO1Zjx471Tkfs06ePRo4cqV69emnMmDG65ppr9P3vf1+RkZEXXEdT45qrFuBEuNp7tEIllTXGFgMAANAGmUwmBQVYDHmYTKZG1x8cHFzv9fTp0/Xuu+/q6aef1qpVq7Rlyxb16tVLDofjnPvx9/c/7b+Ly9X0i66FhoZq06ZNevPNN5WYmKgnnnhCffr0UXFxscxms5YuXaoPP/xQ3bt315///GdlZGQoOztbkpSQkKDDhw/X29/hw4e9o3nNiXDVAkSHWNU+yrNSy9cHio0tBgAAAD4rICBATqfzvP2+/PJLTZkyRTfddJN69eqlhIQE5eTkNH+Bdbp166Yvv/zytJrS09NlNntG6iwWi0aNGqVnn31W33zzjXJycvTpp59K8oS6YcOGadasWdq8ebMCAgL07rvvSpKGDBmi5cuX19v30qVLNWTIkGY/L6YFthB9UyKUW1SpLXnFuiI91uhyAAAA4IPS0tK0bt065eTkKCQk5KyjSl26dNE777yjCRMmyGQy6fHHH2+WEaiz+eUvf6mBAwdq9uzZuu2227RmzRrNmzdPL730kiTpgw8+0L59+3TFFVcoMjJSS5YskcvlUkZGhtatW6fly5frmmuuUVxcnNatW6ejR4+qW7dukqSpU6fqyiuv1PPPP69rr71WCxYs0FdffaVXX3212c+LkasWgpsJAwAA4HymT58us9ms7t27KzY29qzXUM2ZM0eRkZEaOnSoJkyYoDFjxqh///6XrM7+/fvrrbfe0oIFC9SzZ0898cQTeuqppzRlyhRJUkREhN555x1dffXV6tatm1555RW9+eab6tGjh8LCwvT5559r/PjxSk9P12OPPabnn39e48aNkyQNHTpUb7zxhl599VX16dNHCxcu1KJFi9SzZ89mPy+Tu6EL57chpaWlCg8PV0lJicLCwowuR5K0Kfe4bn5ptaKCA7TxsVFNMvcWAAAAp6uurlZ2drY6dOggm81mdDm4BM71mV9INmDkqoXonhgmf7NJRRUO5RZVGl0OAAAAgO8gXLUQNn+zuid5biDM1EAAAAD4knvvvVchISFnfNx7771Gl3fJsKBFC9IvJUJf5xVrc26xbuibbHQ5AAAAgCTPTYqnT59+xm2+cpnNpUC4akFY1AIAAAC+KC4uTnFxcUaXYTimBbYgJ8LVt4dKZa89//0LAAAAcPFY963taKrP2qfD1cyZM2Uymeo9unbtes73zJ07VxkZGQoMDFRKSooeeughVVdXX6KKm1dqdJAig/zlcLr07aFSo8sBAABolfz9/SVJlZUsItZWnPisT3z2F8vnpwX26NFDy5Yt8762WM5e8htvvKFf//rXev311zV06FDt3r1bU6ZMkclk0pw5cy5Fuc3KZDKpb0qEPtt1VFvyitWvfaTRJQEAALQ6ZrNZEREROnLkiCQpKCiI2+C0Um63W5WVlTpy5IgiIiJkNpsbtT+fD1cWi0UJCQkN6rt69WoNGzZMt99+uyTPHaonTpyodevWNWeJl1TflEhvuAIAAEDzOPH754mAhdYtIiKiwZnjXHw+XGVlZSkpKUk2m01DhgxRZmam2rdvf8a+Q4cO1b/+9S+tX79egwYN0r59+7RkyRLdcccd5zyG3W6X3W73vi4t9d0pd33bR0hiUQsAAIDmZDKZlJiYqLi4ONXU1BhdDpqRv79/o0esTvDpcDV48GDNnz9fGRkZys/P16xZszRixAht27ZNoaGhp/W//fbbdezYMQ0fPlxut1u1tbW699579eijj57zOJmZmZo1a1ZznUaT6tsuQpK0v7BSRRUORQUHGFsQAABAK2Y2m5vsF2+0fj69oMW4ceP0gx/8QL1799aYMWO0ZMkSFRcX66233jpj/xUrVujpp5/WSy+9pE2bNumdd97R4sWLNXv27HMeZ8aMGSopKfE+8vLymuN0mkR4kL86xgRLkr5m9AoAAADwGT49cvVdERERSk9P1549e864/fHHH9cdd9yhn/70p5KkXr16qaKiQvfcc49+85vfyM/vzFnSarXKarU2W91NrW/7CO07VqHNucd1VVfuJwAAAAD4Ap8eufqu8vJy7d27V4mJiWfcXllZeVqAOjGM25ruU9Cv7n5Xmxm5AgAAAHyGT4er6dOna+XKlcrJydHq1at10003yWw2a+LEiZKkyZMna8aMGd7+EyZM0Msvv6wFCxYoOztbS5cu1eOPP64JEya0qrmyfVM8S7B/nVcsl6v1hEYAAACgJfPpaYEHDhzQxIkTVVhYqNjYWA0fPlxr165VbGysJCk3N7feSNVjjz0mk8mkxx57TAcPHlRsbKwmTJig3/3ud0adQrPomhgqq8VPpdW1yi6sUKfYEKNLAgAAANo8k7s1zZdrIqWlpQoPD1dJSYnCwsKMLueMbnl5tTbuP67nf9BHt1zWzuhyAAAAgFbpQrKBT08LxNmdvO7quLGFAAAAAJBEuGqxuJkwAAAA4FsIVy1U37qRq535ZaqucRpbDAAAAADCVUuVHBGomBCral1ubTtYYnQ5AAAAQJtHuGqhTCaTd/SKqYEAAACA8QhXLVi/uuuuNucWG1oHAAAAAMJVi9aPkSsAAADAZxCuWrBe7cJlMkkHi6t0pKza6HIAAACANo1w1YKF2vzVJS5EkrSFqYEAAACAoQhXLRyLWgAAAAC+gXDVwvVrHymJRS0AAAAAoxGuWrgTI1ffHCiW0+U2thgAAACgDSNctXDp8aEKCjCrwuHUniPlRpcDAAAAtFmEqxbO7GdSr+RwSdKWvOMGVwMAAAC0XYSrVqBv3c2EWdQCAAAAMA7hqhXol8KiFgAAAIDRCFetQL+6kavdh8tUYa81thgAAACgjSJctQLxYTYlhtvkckvfHCgxuhwAAACgTSJctRLcTBgAAAAwFuGqlTgZrlgxEAAAADAC4aqV6Nf+5KIWbjc3EwYAAAAuNcJVK9ErOVxmP5OOlNmVX1JtdDkAAABAm0O4aiUCA8zKiA+VxHVXAAAAgBEIV60INxMGAAAAjOPT4WrmzJkymUz1Hl27dj3ne4qLi3X//fcrMTFRVqtV6enpWrJkySWq2FjeRS24mTAAAABwyVmMLuB8evTooWXLlnlfWyxnL9nhcGj06NGKi4vTwoULlZycrP379ysiIuISVGq8fnXh6puDxap1umQx+3R2BgAAAFoVnw9XFotFCQkJDer7+uuvq6ioSKtXr5a/v78kKS0trRmr8y2dYkMUarWozF6rnQVl6pkcbnRJAAAAQJvh80MbWVlZSkpKUseOHTVp0iTl5uaete/777+vIUOG6P7771d8fLx69uypp59+Wk6n85zHsNvtKi0trfdoifz8TOrDzYQBAAAAQ/h0uBo8eLDmz5+vjz76SC+//LKys7M1YsQIlZWVnbH/vn37tHDhQjmdTi1ZskSPP/64nn/+ef32t78953EyMzMVHh7ufaSkpDTH6VwSfQlXAAAAgCFM7hZ0x9ni4mKlpqZqzpw5uuuuu07bnp6erurqamVnZ8tsNkuS5syZo+eee075+fln3a/dbpfdbve+Li0tVUpKikpKShQWFtb0J9KMln17WD/9x1fqHBeiZdOuNLocAAAAoEUrLS1VeHh4g7KBz19zdaqIiAilp6drz549Z9yemJgof39/b7CSpG7duqmgoEAOh0MBAQFnfJ/VapXVam2Wmi+1E8ux7zlSrpKqGoUH+htbEAAAANBG+PS0wO8qLy/X3r17lZiYeMbtw4YN0549e+Ryubxtu3fvVmJi4lmDVWsTE2JVSlSgJOmbA8XGFgMAAAC0IT4drqZPn66VK1cqJydHq1ev1k033SSz2ayJEydKkiZPnqwZM2Z4+//sZz9TUVGRpk6dqt27d2vx4sV6+umndf/99xt1CobomxIpiftdAQAAAJeST08LPHDggCZOnKjCwkLFxsZq+PDhWrt2rWJjYyVJubm58vM7mQ9TUlL08ccf66GHHlLv3r2VnJysqVOn6pFHHjHqFAzRNyVC//v6EItaAAAAAJdQi1rQ4lK5kIvWfNHG/cd1y8urFR0coK8eGyWTyWR0SQAAAECLdCHZwKenBeLi9EgKk7/ZpMIKhw4crzK6HAAAAKBNIFy1QjZ/s7onelL1ptzjBlcDAAAAtA2Eq1aKmwkDAAAAlxbhqpU6cb8rwhUAAABwaRCuWqkTy7FvP1QqR63rPL0BAAAANBbhqpVKiw5SRJC/HLUu7cgvNbocAAAAoNUjXLVSJpPJe93VZha1AAAAAJod4aoVY1ELAAAA4NIhXLVihCsAAADg0iFctWInwlVOYaWOVziMLQYAAABo5QhXrVhEUIA6xARLkrYcKDa2GAAAAKCVI1y1cv28i1oUG1oHAAAA0NoRrlo5biYMAAAAXBqEq1buxHVXX+cVy+12G1sMAAAA0IoRrlq5rglhCrD4qaSqRtnHKowuBwAAAGi1CFetXIDFTz2TwiQxNRAAAABoToSrNqBf+0hJLGoBAAAANCfCVRvAzYQBAACA5ke4agNOhKsd+aWqrnEaWwwAAADQShGu2oB2kYGKCQlQrcut7YdKjC4HAAAAaJUIV22AyWTyjl5x3RUAAADQPAhXbcSJRS247goAAABoHoSrNoKRKwAAAKB5Ea7aiN7twmUySQeLq3S0zG50OQAAAECr49PhaubMmTKZTPUeXbt2bdB7FyxYIJPJpBtvvLF5i2whQm3+6hwbIompgQAAAEBz8OlwJUk9evRQfn6+9/HFF1+c9z05OTmaPn26RowYcQkqbDlO3u/quLGFAAAAAK2Qz4cri8WihIQE7yMmJuac/Z1OpyZNmqRZs2apY8eOl6jKloFFLQAAAIDm4/PhKisrS0lJSerYsaMmTZqk3Nzcc/Z/6qmnFBcXp7vuuqvBx7Db7SotLa33aI1OjFx9nVcip8ttbDEAAABAK+PT4Wrw4MGaP3++PvroI7388svKzs7WiBEjVFZWdsb+X3zxhf72t7/ptddeu6DjZGZmKjw83PtISUlpivJ9Tnp8iAL9zSq312rv0XKjywEAAABaFZ8OV+PGjdMPfvAD9e7dW2PGjNGSJUtUXFyst95667S+ZWVluuOOO/Taa6+dd+rgd82YMUMlJSXeR15eXlOdgk+xmP3Uq124JGkLS7IDAAAATcpidAEXIiIiQunp6dqzZ89p2/bu3aucnBxNmDDB2+ZyuSR5rtvatWuXOnXqdMb9Wq1WWa3W5inax/RLidD67CJtzivWrQNb5wgdAAAAYIQWFa7Ky8u1d+9e3XHHHadt69q1q7Zu3Vqv7bHHHlNZWZleeOGFVjvV70L1ax8hiUUtAAAAgKbm0+Fq+vTpmjBhglJTU3Xo0CE9+eSTMpvNmjhxoiRp8uTJSk5OVmZmpmw2m3r27Fnv/REREZJ0Wntb1jfFs2LgroJSVdhrFWz16f8FAAAAgBbDp3+zPnDggCZOnKjCwkLFxsZq+PDhWrt2rWJjYyVJubm58vPz6cvGfE5CuE0JYTYVlFZr68ESXd4x2uiSAAAAgFbB5Ha7WZP7O0pLSxUeHq6SkhKFhYUZXU6Tu/efG/XR9gL9elxX3Xvlma9DAwAAAHBh2YBhnzao74nrrlgxEAAAAGgyhKs2qF/dzYRZ1AIAAABoOoSrNqhXu3CZ/UwqKK1WfkmV0eUAAAAArQLhqg0KCrAoPT5UElMDAQAAgKZCuGqj+jI1EAAAAGhShKs26sR1V5sJVwAAAECTIFy1Uf3qVgzceqBEtU6XscUAAAAArQDhqo3qFBuiUKtFVTVO7TpcZnQ5AAAAQItHuGqj/PxM6p0SLonrrgAAAICmQLhqw7yLWrBiIAAAANBohKs2rG9KpCRGrgAAAICmQLhqw06MXO05Wq6y6hpjiwEAAABaOMJVGxYbalW7yEC53dI3B0qMLgcAAABo0QhXbdyJ0at12UXGFgIAAAC0cISrNu6KLrGSpL+u2qe9R8sNrgYAAABouQhXbdwtl7XTkI7RqnQ49eCbm2WvdRpdEgAAANAiEa7aOLOfSX+8ra8ig/y1/VCpnv1ol9ElAQAAAC0S4QpKCLfpue/3kST97YtsfbbziMEVAQAAAC0P4QqSpFHd4zVlaJokafp/v9aR0mpjCwIAAABamGYLV3//+9+1ePFi7+tf/epXioiI0NChQ7V///7mOiwa4dfjuqprQqgKKxya9tbXcrncRpcEAAAAtBjNFq6efvppBQYGSpLWrFmjF198Uc8++6xiYmL00EMPNddh0Qg2f7Pm3d5PNn8/fbHnmF5dtc/okgAAAIAWo9nCVV5enjp37ixJWrRokW655Rbdc889yszM1KpVq5rrsGikznGhmjmhhyTpDx/v0pa8YmMLAgAAAFqIZgtXISEhKiwslCR98sknGj16tCTJZrOpqqqquQ6LJnDbwBRd2ytRtS63Hnxzs8qqa4wuCQAAAPB5zRauRo8erZ/+9Kf66U9/qt27d2v8+PGSpO3btystLa25DosmYDKZ9PTNvZQcEajcoko9tmib3G6uvwIAAADOpdnC1YsvvqghQ4bo6NGjevvttxUdHS1J2rhxoyZOnNigfcycOVMmk6neo2vXrmft/9prr2nEiBGKjIxUZGSkRo0apfXr1zfJ+bQ14YH++tPEvjL7mfTelkN6Z9NBo0sCAAAAfJqluXYcERGhefPmndY+a9asC9pPjx49tGzZMu9ri+XsJa9YsUITJ07U0KFDZbPZ9Pvf/17XXHONtm/fruTk5As6LqTLUqP0i5Fd9PzS3Xr8vW3q1z5CHWNDjC4LAAAA8EnNNnL10Ucf6YsvvvC+fvHFF9W3b1/dfvvtOn78eIP3Y7FYlJCQ4H3ExMScte+///1v3Xffferbt6+6du2qv/71r3K5XFq+fHmjzqUtu++qzhrcIUqVDqceXLBZjlqX0SUBAAAAPqnZwtXDDz+s0tJSSdLWrVv1y1/+UuPHj1d2dramTZvW4P1kZWUpKSlJHTt21KRJk5Sbm9vg91ZWVqqmpkZRUVHn7Ge321VaWlrvAQ+zn0lzf9hXEUH+2nawVM99vNPokgAAAACf1GzhKjs7W927d5ckvf3227ruuuv09NNP68UXX9SHH37YoH0MHjxY8+fP10cffaSXX35Z2dnZGjFihMrKyhr0/kceeURJSUkaNWrUOftlZmYqPDzc+0hJSWnQ/tuKxPBAPXtLb0nSa6uytWLXEYMrAgAAAHxPs4WrgIAAVVZWSpKWLVuma665RpIUFRXV4JGhcePG6Qc/+IF69+6tMWPGaMmSJSouLtZbb7113vc+88wzWrBggd59913ZbLZz9p0xY4ZKSkq8j7y8vAbV15Zc0yNBk4ekSpKm//drHSmrNrgiAAAAwLc024IWw4cP17Rp0zRs2DCtX79e//nPfyRJu3fvVrt27S5qnxEREUpPT9eePXvO2e8Pf/iDnnnmGS1btky9e/c+736tVqusVutF1dSWPDq+m9ZnF2lnQZl++dbX+vuPB8nPz2R0WQAAAIBPaLaRq3nz5slisWjhwoV6+eWXvav1ffjhhxo7duxF7bO8vFx79+5VYmLiWfs8++yzmj17tj766CMNGDDgoo6DM7P5m/Xnif1k8/fTqqxj+usX+4wuCQAAAPAZJrcP3x12+vTpmjBhglJTU3Xo0CE9+eST2rJli7799lvFxsZq8uTJSk5OVmZmpiTp97//vZ544gm98cYbGjZsmHc/ISEhCglp+BLipaWlCg8PV0lJicLCwpr8vFq6N9bl6tF3t8riZ9LbPxuqPikRRpcEAAAANIsLyQbNNi1QkpxOpxYtWqQdO3ZI8tyz6vrrr5fZbG7Q+w8cOKCJEyeqsLBQsbGxGj58uNauXavY2FhJUm5urvz8Tg6+vfzyy3I4HPr+979fbz9PPvmkZs6c2TQnBU0clKJVWUf14bYCPbhgsxY/OEIh1mb9XwkAAADwec02crVnzx6NHz9eBw8eVEZGhiRp165dSklJ0eLFi9WpU6fmOGyTYOTq/EoqazTuhc91qKRaN/dL1pzb+hpdEgAAANDkLiQbNNs1Vw8++KA6deqkvLw8bdq0SZs2bVJubq46dOigBx98sLkOi0skPMhfL0zsJz+T9M7mg3p38wGjSwIAAAAM1WwjV8HBwVq7dq169epVr/3rr7/WsGHDVF5e3hyHbRKMXDXcC8uy9MdluxUcYNaSqSOUGh1sdEkAAABAk/GJkSur1XrGm/2Wl5crICCguQ6LS+yBqztrUIcoVTicevDNzXLUuowuCQAAADBEs4Wr6667Tvfcc4/WrVsnt9stt9uttWvX6t5779X111/fXIfFJWb2M2nubX0VHuivrw+U6Pmlu4wuCQAAADBEs4WrP/3pT+rUqZOGDBkim80mm82moUOHqnPnzpo7d25zHRYGSIoI1O9v8dys+S8r9+nz3UcNrggAAAC49Jr9Pld79uzxLsXerVs3de7cuTkP1yS45uriPLZoq/61NlcxIVZ99IsRigmxGl0SAAAA0CgXkg2aNFxNmzatwX3nzJnTVIdtcoSri1Nd49T1877Q7sPlujI9Vv83ZaD8/ExGlwUAAABcNMNuIrx58+YG9TOZ+IW7NbL5m/Xnif11/bwvtHL3Ub3+ZbZ+OqKj0WUBAAAAl0SzTwtsiRi5apx/rd2vxxZtk7/ZpHfvG6aeyeFGlwQAAABcFJ9Yih1t16TB7TWmR7xqnG79/M3NqrDXGl0SAAAA0OwIV2hyJpNJv7+ltxLDbco+VqEn399udEkAAABAsyNcoVlEBAVo7m195WeSFm48oEWbDxpdEgAAANCsCFdoNoM7RuvnV3eRJE3/79f659r9BlcEAAAANB/CFZrVz6/urJv7JavW5dbji7bpsUVbVeN0GV0WAAAA0OQIV2hWFrOfnr+1j349rqtMJulfa3M1+W/rdbzCYXRpAAAAQJMiXKHZmUwm3XtlJ/118gAFB5i1Zl+hbnjxS+0+XGZ0aQAAAECTIVzhkhnZLV7v3j9M7aOClFtUqZtfWq3lOw4bXRYAAADQJAhXuKTS40P13v3DdHnHKJXba/XTf3yll1fsFfeyBgAAQEtHuMIlFxkcoH/eNVg/ury93G7p9x/t1LS3vlZ1jdPo0gAAAICLRriCIfzNfvrtjb00+8aeMvuZ9O7mg7rt1bU6UlptdGkAAADARSFcwVB3XJ6qf941SBFB/vo6r1jXz/tS3xwoNrosAAAA4IIRrmC4oZ1i9N79w9Q5LkQFpdX6wStr9P7Xh4wuCwAAALgghCv4hNToYL1731Bd3TVO9lqXHnxzs/7w8S65XCx0AQAAgJaBcAWfEWrz12uTB+j/XdlRkjTvsz26918bVWGvNbgyAAAA4Px8OlzNnDlTJpOp3qNr167nfM9///tfde3aVTabTb169dKSJUsuUbVoCmY/k2aM66Y5t/ZRgNlPn3x7WLe8vFp5RZVGlwYAAACck0+HK0nq0aOH8vPzvY8vvvjirH1Xr16tiRMn6q677tLmzZt144036sYbb9S2bdsuYcVoCjf3b6cF/+9yxYZatbOgTDe8+KXW7Ss0uiwAAADgrHw+XFksFiUkJHgfMTExZ+37wgsvaOzYsXr44YfVrVs3zZ49W/3799e8efMuYcVoKv3bR+r9B4apZ3KYiiocmvTXdXpzfa7RZQEAAABn5PPhKisrS0lJSerYsaMmTZqk3Nyz/3K9Zs0ajRo1ql7bmDFjtGbNmnMew263q7S0tN4DviExPFD//X9DdW3vRNW63JrxzlbNfH+7ap0uo0sDAAAA6vHpcDV48GDNnz9fH330kV5++WVlZ2drxIgRKisrO2P/goICxcfH12uLj49XQUHBOY+TmZmp8PBw7yMlJaXJzgGNFxhg1ryJ/fTL0emSpPmrczTl/zaopLLG4MoAAACAk3w6XI0bN04/+MEP1Lt3b40ZM0ZLlixRcXGx3nrrrSY9zowZM1RSUuJ95OXlNen+0Xgmk0k/H9lFr/zoMgUFmPXFnmO68aUvtedIudGlAQAAAJJ8PFx9V0REhNLT07Vnz54zbk9ISNDhw4frtR0+fFgJCQnn3K/ValVYWFi9B3zT2J4JWnjvUCVHBCr7WIVueulLrdh1xOiyAAAAgJYVrsrLy7V3714lJiaecfuQIUO0fPnyem1Lly7VkCFDLkV5uES6J4XpvQeGaWBapMqqa/WT+Rv06ud75eSGwwAAADCQT4er6dOna+XKlcrJydHq1at10003yWw2a+LEiZKkyZMna8aMGd7+U6dO1UcffaTnn39eO3fu1MyZM/XVV1/pgQceMOoU0ExiQqz6908v120DUuRyS08v2akJf/5Ca/ayXDsAAACM4dPh6sCBA5o4caIyMjJ06623Kjo6WmvXrlVsbKwkKTc3V/n5+d7+Q4cO1RtvvKFXX31Vffr00cKFC7Vo0SL17NnTqFNAMwqw+OmZW3rptzf2VKjNom/zSzXxtbX6f//8SvsLK4wuDwAAAG2Mye12M5fqO0pLSxUeHq6SkhKuv2ohiioc+uPS3Xpjfa6cLrf8zSb9eFgHPXB1Z4XZ/I0uDwAAAC3UhWQDwtUZEK5art2HyzT7g2+1KuuYJCk6OEAPjU7XDwemyGL26YFaAAAA+CDCVSMRrlo2t9utFbuO6reLv9Xeo57pgRnxoXrsum4a0SXW4OoAAADQkhCuGolw1TrUOF3699r9mrs8S8V1Nxwe2TVOj17bTZ1iQwyuDgAAAC0B4aqRCFetS3GlQy8sz9I/1+xXrcsti59JdwxJ1dSRXRQRFGB0eQAAAPBhhKtGIly1TnuPluvpxTu0fKfnpsMRQf76xcgumnR5qvy5HgsAAABnQLhqJMJV67Yq66h++8EO7TpcJknqFBusx67rrqsy4gyuDAAAAL6GcNVIhKvWr9bp0oINeZqzdLeKKhySpCvSY/XYtd2UHh9qcHUAAADwFYSrRiJctR2l1TV68dM9ev3LbNU43TL7mXT7oPZ6aHS6ooK5HgsAAKCtI1w1EuGq7ck5VqHMD3fo4+2HJUmhNoumjuyiyUPSFGDheiwAAIC2inDVSISrtmvN3kLN/uBbfZtfKklKiw7So+O7aXT3eJlMJoOrAwAAwKVGuGokwlXb5nS5tXBjnp77eLeOldslSUM7ReuBqzprSKdoQhYAAEAbQrhqJMIVJKncXquXV+zRa6uy5ah1SZK6xIVo8tA03dwvWcFWi8EVAgAAoLkRrhqJcIVT5RVV6i+f79U7mw6q0uGUJIVaLfr+gHaaPCRNHWKCDa4QAAAAzYVw1UiEK5xJaXWN3t54QP9Ys1/Zxyq87Vemx2rK0DRdmR4rPz+mDAIAALQmhKtGIlzhXFwut1btOaa/r87RZ7uO6MQ3KDU6SHdcnqofDEhReKC/sUUCAACgSRCuGolwhYbaX1ihf67Zr7e+ylNpda0kKdDfrJv6J2vykFR1TeD/HwAAgJaMcNVIhCtcqEpHrRZtPqS/r87RrsNl3vbLO0bpziFpGt09XhYz98sCAABoaQhXjUS4wsVyu91al12kf6zJ0cfbD8vp8ny9EsNt+tHlqfrhwBRFh1gNrhIAAAANRbhqJMIVmsKh4iq9sS5Xb67PVWGFQ5IUYPbTdX0SNWVomnq3izC2QAAAAJwX4aqRCFdoStU1Ti3Zmq+/r87R1wdKvO19UyI0ZWiaxvVKkNViNrBCAAAAnA3hqpEIV2guW/KK9ffVOfrgm0OqcXq+ejEhAZo4qL1uHZCilKgggysEAADAqQhXjUS4QnM7WmbXgvW5+ve6XBWUVnvb+7QL17W9EzW+V6LaRRK0AAAAjEa4aiTCFS6VGqdLn2w/rH+v26+1+wrlOuXb2CclQtf1StS4XgkELQAAAIMQrhqJcAUjHC2z66PtBVr8zSGtyy7Sqd/MvikRuq53osb1SlRyRKBxRQIAALQxF5INWtSNd5555hmZTCb94he/OGe/uXPnKiMjQ4GBgUpJSdFDDz2k6urqc74HMFpsqFV3XJ6qBfcM0bpHR2r2DT00uEOUTCbPtVq/XbxDw575VDe99KX+umqfDhZXGV0yAAAATmExuoCG2rBhg/7yl7+od+/e5+z3xhtv6Ne//rVef/11DR06VLt379aUKVNkMpk0Z86cS1Qt0DhxoTbdMSRNdwxJ05Gyan20rUCLv8nX+pwibc4t1uZcT9jq3z5C43t5rtFKYkQLAADAUC0iXJWXl2vSpEl67bXX9Nvf/vacfVevXq1hw4bp9ttvlySlpaVp4sSJWrdu3aUoFWhycaE2TR6SpslD0nSktFofbS/QB9/ka0NOkTblFmvTKUHr2t5JGt8rQYnhBC0AAIBLrUVMC7z//vt17bXXatSoUeftO3ToUG3cuFHr16+XJO3bt09LlizR+PHjz/oeu92u0tLSeg/AF8WFeYLWW/9viNbOGKlZ1/fQoDTP1MFNucWa/cG3GpL5qW55ebVe/yJbBSVMhwUAALhUfH7kasGCBdq0aZM2bNjQoP633367jh07puHDh8vtdqu2tlb33nuvHn300bO+JzMzU7NmzWqqkoFLIj7MpjuHpunOoWk6XFqtD7fma/HWfH21/7g21j2e+uBbDUiN1PheiRrbM4GpgwAAAM3Ip1cLzMvL04ABA7R06VLvtVbf+9731LdvX82dO/eM71mxYoV++MMf6re//a0GDx6sPXv2aOrUqbr77rv1+OOPn/E9drtddrvd+7q0tFQpKSmsFogWqaCkWh9uy9eSrfnakHO83rYucSEa0SVWI9JjNLhDlIICfP7fVwAAAAzVapZiX7RokW666SaZzWZvm9PplMlkkp+fn+x2e71tkjRixAhdfvnleu6557xt//rXv3TPPfeovLxcfn7nnwnJUuxoLfJLqvTh1gIt2ZqvTbnH691HK8Dsp4EdIj1hq0uMuiWEyc/PZFyxAAAAPuhCsoFP/7P1yJEjtXXr1nptP/7xj9W1a1c98sgjpwUrSaqsrDwtQJ3o58M5EmgWieGB+snwDvrJ8A4qrnRo9d5Crco6qs93H9PB4ip9uadQX+4p1DMfSjEhVo3oEqMRXWI0vEuM4kJtRpcPAADQovh0uAoNDVXPnj3rtQUHBys6OtrbPnnyZCUnJyszM1OSNGHCBM2ZM0f9+vXzTgt8/PHHNWHChDOGMaCtiAgK8C7b7na7te9YhVbtPqpVWce0Zl+hjpXb9e7mg3p380FJUteEUF2ZHqsRXWI1IC1SNn++PwAAAOfi0+GqIXJzc+uNVD322GMymUx67LHHdPDgQcXGxmrChAn63e9+Z2CVgG8xmUzqFBuiTrEhmjKsg+y1Tm3aX+wZ1co6qm0HS7WzoEw7C8r0l8/3yWrx0+CO0bqiS4yuSI9Vl7gQmUxMIQQAADiVT19zZRSuuUJbV1hu1xd7jmlV1jGtyjqqw6X2etvjw6wa0SVWV6THanjnGEUFBxhUKQAAQPNqNQtaGIVwBZzkdru1+3C5VmUd1crdR7U+u0j2Wpd3u8kk9UwK1/AuMRqUFqX+qZEKD/Q3sGIAAICmQ7hqJMIVcHbVNU5tyCnSqqxj+nz3Ue0sKKu33WSS0uNCNSAt0vNIjVK7yECmEQIAgBaJcNVIhCug4Q6XVuuLrGNavbdQG/cXKaew8rQ+8WFWDUiN8oatbomhspjPf1sEAAAAoxGuGolwBVy8o2V2bdxfpK9yjmvD/uPafrBEta76f8wEBZjVr32ELkuN0sC0SPVrH6kQa4tfXwcAALRChKtGIlwBTafK4dTXB4r1VU6Rvtp/XBv3H1dZdW29Pn4mqWtCmAamReqyNE/gSgwPNKhiAACAkwhXjUS4ApqPy+XW7iNl+irHE7Q25BTpwPGq0/olRwTqstRIT+BKjVJGQqjMfly3BQAALi3CVSMRroBL63BptWcaYU6RNu4/rm/zS+X8zlTCUKtFPZPD1atduHomh6t3crhSo4NYKAMAADQrwlUjEa4AY1XYa7Ulr1hf5RzXV/uLtDm3WOX22tP6hdos6pl0MnD1Sg5XalSQ/BjhAgAATYRw1UiEK8C3OF1u7Soo07aDJfrmYLG2HizVjvxSOU6539YJoVaLeiSHqVdy3QhXuwgCFwAAuGiEq0YiXAG+r8bpUtbhcm07WKKtdY8d+aX1bnB8wncDV6/kcKVFBxO4AADAeRGuGolwBbRMNU6X9hwp19YDDQtc3ZM8gatXOwIXAAA4M8JVIxGugNbDG7gOlnhHub49dObAFRRgVpf4UHWND1V6Qqi6JoQqIyFUMSFWAyoHAAC+gHDVSIQroHWrdbqU1cDAJUnRwQHKSAhVevzJwJUeH6pgbnwMAECrR7hqJMIV0PbUOl3KKazUroIy7Tpcpl0FpdpVUKb9RZU625+SKVGByogPU0ZCiDISwtQ1IVQdYoLlb/a7tMUDAIBmQ7hqJMIVgBMqHbXac6RcOwvKtKugTLsPl2lnQZmOltnP2N/fbFKn2JDTRrqSIwK5JxcAAC0Q4aqRCFcAzqeowuEZ5Soo1a66wLW7oEwVDucZ+4dYLeoSH6JOsSHqGBusTrEh6hQbrPZRwQqwMNIFAICvIlw1EuEKwMVwu906cLzqlKmFnsfeo+WqdZ35j1qzn0nto4LUMSZYneJC6v2MCg5gtAsAAIMRrhqJcAWgKTlqXco+VqHdh8u072iF9h0r196j5dp3tEKVZxnpkqTwQH91ig1Wx9j6I17to4IY7QIA4BIhXDUS4QrApeB2u3W41F4XtMq192iFN3QdLK466/tOjHadDF6en4x2AQDQ9AhXjUS4AmC0KodT2cfqRrmONHy0K9RqUfvoIKVGB6l9VLDSooPqXgcrIcwmMzdJBgDgghCuGolwBcBXud1uFZRWe6YXXsBolyQFmP3ULipQqVGesNU+yhPCUqOD1C4ySDZ/8yU6CwAAWg7CVSMRrgC0RNU1TuUVVWp/YaVyCiuUW/c8t6hSB45XqsZ59j/uTSYpIczmCVtRwd7RrxPPwwP9L+GZAADgOwhXjUS4AtDaOF1uHSqu8gau/UUVyi2sVE5hpXILK866hPwJEUH+So0KUruoILWLDFS7iEC1iwxScmSgkiMCFWy1XKIzAQDg0iJcNRLhCkBb4na7VVjhqBvlqvD8LKzU/rogdqz8zDdMPlVkkL+SIwPVLsITuNrVhS7Pc0a+AAAt14Vkgxb1T43PPPOMZsyYoalTp2ru3Lln7VdcXKzf/OY3euedd1RUVKTU1FTNnTtX48ePv3TFAkALYTKZFBNiVUyIVZelRp62vcJe6x3xOnC8UgeOV+lgcZXn5/FKlVbX6nhljY5X1mjbwdIzHiPUavGGrnaRQacEL08IY5VDAEBr0GLC1YYNG/SXv/xFvXv3Pmc/h8Oh0aNHKy4uTgsXLlRycrL279+viIiIS1MoALQywVaLuiWGqVvimf+1rrS6RgePV+ng8SodOF55MnjV/SyqcKjMXqudBWXaWVB2xn0E+puVHBmopIhAJYXblBBuU1J4oBLCbUoMtykxIlAhTD0EAPi4FvE3VXl5uSZNmqTXXntNv/3tb8/Z9/XXX1dRUZFWr14tf3/PNJS0tLRLUCUAtE1hNn+FJfqfNXxVOmp1qLhKed4AdiJ4Verg8SodKbOrqsapPUfKtedI+VmPE2q1eMJWRKASw+oCWIRNCeGBngAWblOojemHAADjtIhrru68805FRUXpj3/8o773ve+pb9++Z50WOH78eEVFRSkoKEjvvfeeYmNjdfvtt+uRRx6R2XzmZYbtdrvs9pPXFJSWliolJYVrrgDgEqiucSq/pFoHjlcqv7ha+SXVyi+pUn5JtQpKqnWopEpl1bUN2lfIiQBW90gIPzkSllg3EhZmszAFEQDQYK3qmqsFCxZo06ZN2rBhQ4P679u3T59++qkmTZqkJUuWaM+ePbrvvvtUU1OjJ5988ozvyczM1KxZs5qybABAA9n8zeoQE6wOMcFn7VNur1XBd0JXvQBWXKXS6lqV22vPOwJm8/dTfJhN8aE2xYZZFR9qU3yYVXF1z+PCbIoLsyrUSggDAFwYnx65ysvL04ABA7R06VLvtVbnG7lKT09XdXW1srOzvSNVc+bM0XPPPaf8/PwzvoeRKwBo+SrstSoora4b/aqqGwGrVsEpz0uqahq8v0B/8ymBy6q4uhAWH2ZTXKhVcWGe1yGEMABo1VrNyNXGjRt15MgR9e/f39vmdDr1+eefa968ebLb7adN9UtMTJS/v3+99m7duqmgoEAOh0MBAQGnHcdqtcpqtTbfiQAAml2w1aJOsSHqFBty1j5VDqeOlFXrcKldh0urdaTMriN1Pw+XVnvbyqprVVXj9NwTrLDynMcN9DfXjXx5QldsqGflxdhQq2JDTr6ODgmQv9mvqU8bAOBDfDpcjRw5Ulu3bq3X9uMf/1hdu3Y96zVUw4YN0xtvvCGXyyU/P89fYrt371ZiYuIZgxUAoO0IDDArNTpYqdFnn4IoeRbhOFJqrxe6jpadDGSHS6t1pNSuMrsnhOXU3ZD5fCKD/L3L3p8IXTGhAYoNsSrmlDAWFUwQA4CWyKfDVWhoqHr27FmvLTg4WNHR0d72yZMnKzk5WZmZmZKkn/3sZ5o3b56mTp2qn//858rKytLTTz+tBx988JLXDwBomYICLEqLsSjtHNeBSSdD2Kmh61i5Q0fL7DpW7nkcLbOrsMIhp8vtvR9Y1jmuCTshMsj/ZAA7NYyFBCg6JEBRwVZFB3ueBwX49F/nANBmtPg/jXNzc70jVJKUkpKijz/+WA899JB69+6t5ORkTZ06VY888oiBVQIAWqOGhjCXy63iqprTQtfRcruOlTnqfnravxvEdh8+fxCz+fspOthaF7o8D0/wstZ7Hl23LSjAzHViANAMfHpBC6NcyEVrAAA0JZfLreOVjjOOgB0tt6uw3KGiCocK64KYvdZ1wcewWvwUUxe8oupGvzzByxPAIoMDFBnkX/czQOGB/jL7EcYAtE2tZkELAADaGj8/k2eUKcSqjITQc/Z1u92qdDhVWO5QYYW9LnQ5VFjhUFGF/ZTn9cOYvdalg8Wemzk3hMkkRQT6KzLolODlfX4yiEWdsi080F8WrhsD0MYQrgAAaKFMJpOCrRYFWy1qHx103v4nwlhRhUPHyuvCWF0gK6qwe58XVzo80xIrHCqz18rtlneaoo5VNLi+8ED/eiNgpwax8EB/RQT5KyIwQBFB/t7XLG0PoCUjXAEA0EacGsZSos4fxiTJUetScZVDxZU1KqrwBK+iihodr3ToeIVDRZXf3eZQaXWtJKmkqkYlVTUNWknxBLOfSRGB/goP8vf8DPRXRNCpYazu9SnPIwL9FcbURQA+gHAFAADOKsDip7hQm+JCbQ1+T63TpeKqmtOCmGf0yxPCiitrVFxVo5LKGhVXOVRSVaPqGpecLrdnBK3CccG1htks3iAWFmhReF04C7N5wleY9/Up2+p+svQ9gKZAuAIAAE3KYvbzLiF/IaprnCqpqvEEr0pHvfD13TDm6eMZGSu3e0bKSqtrvaNmFyrQ33xK4LLUC2XhZwhmoTZ/hdosCgv0TGVk1AyARLgCAAA+wuZvls3frPiwho+SSVKN0+Wdglhc6RkFK62q9baVnvhZXVPXVqvSuvayumBWVeNUVY1TBaXVF1V7iNWiMJulXugKtVkUVvc61OYJbd7tNn9v/7BAiwL9WR4faA0IVwAAoEXzv8iRMskzhbHcfmoQq/1OEDsZzjztnmBWVu157qhbCr/cXusZQSu5uHBm9jN9J4xZFGI99blFISfCm9XzOtRW11bXL8RmYXojYDDCFQAAaLMsZj/PohhBARf1/uoap8qqa1VWXaOy6lqV1v0sq/YEtRMh7LR2+8ntLrfkdLm9Ux0bw2rx846OeQPYiWBm9YSzkBNtdYubBFvNCrX6K9hq9vZlJA24OIQrAACAi3RiKmNs6IWPmkknl8c/PXzVqry6VuX2GpXXXUtWXtdWVtd2ok9Zda2qapyS5LmPWd2NpxvDzyQFB3iC1okVJkPqwlewtX44O73tZL/gAM9r7nmGtoJwBQAAYJBTl8dPDL/4/dQ6Xaqwe0LaiSmK5XUjZuXeoFbrHV2rsNeqwu709q045afLLbncUpm91ntNWmMFWPwUYrUoKMDs/XkyfHkCmOd1/fYgq9kb0E7ty8gafBXhCgAAoIWzmP0UHuSn8CD/Ru3H7XarqsZZF7Sc3lB2InydeF5RF7xOhLSyU9ura1Xp8LQ7nJ5r0hy1LhXVOlTU8HtQn5PJJAX5mxVUF8gCAzw/g6yWunZPGAsKMCuoLpwFBtRvC7KaPSHvlDabvx+hDY1CuAIAAIAkz0haUIBFQQEWKbTx+3PUujxBy+H0hq8Ku1MVDk8AK7c7VXmi/USfU/s6alVZN8JW6fC8z+2W3G55+jmcOtr4Mr1MddMhAwM8wSvQ3+wNXifaPO2eQFavLaAu2Hnb6/exWczyY8n+Vo9wBQAAgGYRYPFTgCVAEUFNsz+Xy63qWk/YqnI4VWF3qqrGE9gqHScCmCewVTqc3mB38rUnoFXV/ay0e9pOXLPmdp+y8mMzCKwXvjzhLbDeT4sCA/zqRtFOhrtAf7NsAWYFneh34j112211r1kt0niEKwAAALQIfn6njKw1IafLMx2ysi5wnQhgJwJZVY0nnNVrqwtzlTUn2r/7Hk9bdY3Le5wT91NTE02P/C5/s6leKLPVhTGb5WQg87T5eZ+f6H/qdpu/32ltpz7nptlnR7gCAABAm2b2M3lXO2yK6ZCncnmDW10Iqwtq1aeEsKq6gPbdn5UOp6prTga1qhqX5301tapyuFTl8KwU6XJ7jlXjdKvG6bnurTkFmP1k8/erF85s/n6ynvL8ZLtZVv9TwpzFzxvWrJaTQe5s+wqwtKzROMIVAAAA0Ez8/E6uCNkc3G63HE7XaaGsqsYTzKprXJ7n3wly1TUng1tVjcvbVnVK+6nvOXUEzuF0yeF0qbSZQ1xqdJBWPnxVsx6jqRGuAAAAgBbKZDLJavGMAkU043HcbrfstaeEuJqTAa26xnVKMHOd0n5KuPvuttoTIc+l6lqn7PX6eUbjbBZzM55R8yBcAQAAADgnk8nknboX2czHOjEaV+N0N/ORmh7hCgAAAIDPODkaZ3QlF65lXSEGAAAAAD6KcAUAAAAATYBwBQAAAABNgHAFAAAAAE2AcAUAAAAATaBFhatnnnlGJpNJv/jFLxrUf8GCBTKZTLrxxhubtS4AAAAAaDHhasOGDfrLX/6i3r17N6h/Tk6Opk+frhEjRjRzZQAAAADQQsJVeXm5Jk2apNdee02Rkee/bZnT6dSkSZM0a9YsdezY8RJUCAAAAKCtaxHh6v7779e1116rUaNGNaj/U089pbi4ON11110N6m+321VaWlrvAQAAAAAXwufve7xgwQJt2rRJGzZsaFD/L774Qn/729+0ZcuWBh8jMzNTs2bNOq2dkAUAAAC0bScygdvtPm9fnw5XeXl5mjp1qpYuXSqbzXbe/mVlZbrjjjv02muvKSYmpsHHmTFjhqZNm+Z9ffDgQXXv3l0pKSkXVTcAAACA1qWsrEzh4eHn7GNyNySCGWTRokW66aabZDabvW1Op1Mmk0l+fn6y2+31tm3ZskX9+vWr1+ZyuSRJfn5+2rVrlzp16nTe47pcLh06dEihoaEymUxNeEYXrrS0VCkpKcrLy1NYWJihtaD58Dm3fnzGbQOfc+vHZ9w28Dm3fhfyGbvdbpWVlSkpKUl+fue+qsqnR65GjhyprVu31mv78Y9/rK5du+qRRx6pF6IkqWvXrqf1f+yxx1RWVqYXXnihwSNRfn5+ateuXeOKb2JhYWF8udsAPufWj8+4beBzbv34jNsGPufWr6Gf8flGrE7w6XAVGhqqnj171msLDg5WdHS0t33y5MlKTk5WZmambDbbaf0jIiIk6bR2AAAAAGhKPh2uGiI3N/e8w3MAAAAA0NxaXLhasWLFOV9/1/z585utlkvBarXqySeflNVqNboUNCM+59aPz7ht4HNu/fiM2wY+59avuT5jn17QAgAAAABaCubTAQAAAEATIFwBAAAAQBMgXAEAAABAEyBcAQAAAEATIFz5uBdffFFpaWmy2WwaPHiw1q9fb3RJaCIzZ86UyWSq9+jatavRZaGRPv/8c02YMEFJSUkymUxatGhRve1ut1tPPPGEEhMTFRgYqFGjRikrK8uYYnFRzvcZT5ky5bTv9tixY40pFhclMzNTAwcOVGhoqOLi4nTjjTdq165d9fpUV1fr/vvvV3R0tEJCQnTLLbfo8OHDBlWMi9GQz/l73/vead/ne++916CKcaFefvll9e7d23uj4CFDhujDDz/0bm+O7zHhyof95z//0bRp0/Tkk09q06ZN6tOnj8aMGaMjR44YXRqaSI8ePZSfn+99fPHFF0aXhEaqqKhQnz599OKLL55x+7PPPqs//elPeuWVV7Ru3ToFBwdrzJgxqq6uvsSV4mKd7zOWpLFjx9b7br/55puXsEI01sqVK3X//fdr7dq1Wrp0qWpqanTNNdeooqLC2+ehhx7S//73P/33v//VypUrdejQId18880GVo0L1ZDPWZLuvvvuet/nZ5991qCKcaHatWunZ555Rhs3btRXX32lq6++WjfccIO2b98uqZm+x274rEGDBrnvv/9+72un0+lOSkpyZ2ZmGlgVmsqTTz7p7tOnj9FloBlJcr/77rve1y6Xy52QkOB+7rnnvG3FxcVuq9XqfvPNNw2oEI313c/Y7Xa777zzTvcNN9xgSD1oHkeOHHFLcq9cudLtdnu+t/7+/u7//ve/3j47duxwS3KvWbPGqDLRSN/9nN1ut/vKK690T5061bii0OQiIyPdf/3rX5vte8zIlY9yOBzauHGjRo0a5W3z8/PTqFGjtGbNGgMrQ1PKyspSUlKSOnbsqEmTJik3N9foktCMsrOzVVBQUO97HR4ersGDB/O9bmVWrFihuLg4ZWRk6Gc/+5kKCwuNLgmNUFJSIkmKioqSJG3cuFE1NTX1vstdu3ZV+/bt+S63YN/9nE/497//rZiYGPXs2VMzZsxQZWWlEeWhkZxOpxYsWKCKigoNGTKk2b7HlqYoFk3v2LFjcjqdio+Pr9ceHx+vnTt3GlQVmtLgwYM1f/58ZWRkKD8/X7NmzdKIESO0bds2hYaGGl0emkFBQYEknfF7fWIbWr6xY8fq5ptvVocOHbR37149+uijGjdunNasWSOz2Wx0ebhALpdLv/jFLzRs2DD17NlTkue7HBAQoIiIiHp9+S63XGf6nCXp9ttvV2pqqpKSkvTNN9/okUce0a5du/TOO+8YWC0uxNatWzVkyBBVV1crJCRE7777rrp3764tW7Y0y/eYcAUYZNy4cd7nvXv31uDBg5Wamqq33npLd911l4GVAWiMH/7wh97nvXr1Uu/evdWpUyetWLFCI0eONLAyXIz7779f27Zt45rYVu5sn/M999zjfd6rVy8lJiZq5MiR2rt3rzp16nSpy8RFyMjI0JYtW1RSUqKFCxfqzjvv1MqVK5vteEwL9FExMTEym82nrVhy+PBhJSQkGFQVmlNERITS09O1Z88eo0tBMznx3eV73bZ07NhRMTExfLdboAceeEAffPCBPvvsM7Vr187bnpCQIIfDoeLi4nr9+S63TGf7nM9k8ODBksT3uQUJCAhQ586dddlllykzM1N9+vTRCy+80GzfY8KVjwoICNBll12m5cuXe9tcLpeWL1+uIUOGGFgZmkt5ebn27t2rxMREo0tBM+nQoYMSEhLqfa9LS0u1bt06vtet2IEDB1RYWMh3uwVxu9164IEH9O677+rTTz9Vhw4d6m2/7LLL5O/vX++7vGvXLuXm5vJdbkHO9zmfyZYtWySJ73ML5nK5ZLfbm+17zLRAHzZt2jTdeeedGjBggAYNGqS5c+eqoqJCP/7xj40uDU1g+vTpmjBhglJTU3Xo0CE9+eSTMpvNmjhxotGloRHKy8vr/Ytmdna2tmzZoqioKLVv316/+MUv9Nvf/lZdunRRhw4d9PjjjyspKUk33nijcUXjgpzrM46KitKsWbN0yy23KCEhQXv37tWvfvUrde7cWWPGjDGwalyI+++/X2+88Ybee+89hYaGeq+/CA8PV2BgoMLDw3XXXXdp2rRpioqKUlhYmH7+859ryJAhuvzyyw2uHg11vs957969euONNzR+/HhFR0frm2++0UMPPaQrrrhCvXv3Nrh6NMSMGTM0btw4tW/fXmVlZXrjjTe0YsUKffzxx833PW78goZoTn/+85/d7du3dwcEBLgHDRrkXrt2rdEloYncdttt7sTERHdAQIA7OTnZfdttt7n37NljdFlopM8++8wt6bTHnXfe6Xa7PcuxP/744+74+Hi31Wp1jxw50r1r1y5ji8YFOddnXFlZ6b7mmmvcsbGxbn9/f3dqaqr77rvvdhcUFBhdNi7AmT5fSe7/+7//8/apqqpy33fffe7IyEh3UFCQ+6abbnLn5+cbVzQu2Pk+59zcXPcVV1zhjoqKclutVnfnzp3dDz/8sLukpMTYwtFgP/nJT9ypqanugIAAd2xsrHvkyJHuTz75xLu9Ob7HJrfb7b74aAYAAAAAkLjmCgAAAACaBOEKAAAAAJoA4QoAAAAAmgDhCgAAAACaAOEKAAAAAJoA4QoAAAAAmgDhCgAAAACaAOEKAIAmtGLFCplMJhUXFxtdCgDgEiNcAQAAAEATIFwBAAAAQBMgXAEAWhWXy6XMzEx16NBBgYGB6tOnjxYuXCjp5JS9xYsXq3fv3rLZbLr88su1bdu2evt4++231aNHD1mtVqWlpen555+vt91ut+uRRx5RSkqKrFarOnfurL/97W/1+mzcuFEDBgxQUFCQhg4dql27djXviQMADEe4AgC0KpmZmfrHP/6hV155Rdu3b9dDDz2kH/3oR1q5cqW3z8MPP6znn39eGzZsUGxsrCZMmKCamhpJnlB066236oc//KG2bt2qmTNn6vHHH9f8+fO97588ebLefPNN/elPf9KOHTv0l7/8RSEhIfXq+M1vfqPnn39eX331lSwWi37yk59ckvMHABjH5Ha73UYXAQBAU7Db7YqKitKyZcs0ZMgQb/tPf/pTVVZW6p577tFVV12lBQsW6LbbbpMkFRUVqV27dpo/f75uvfVWTZo0SUePHtUnn3ziff+vfvUrLV68WNu3b9fu3buVkZGhpUuXatSoUafVsGLFCl111VVatmyZRo4cKUlasmSJrr32WlVVVclmszXzfwUAgFEYuQIAtBp79uxRZWWlRo8erZCQEO/jH//4h/bu3evtd2rwioqKUkZGhnbs2CFJ2rFjh4YNG1Zvv8OGDVNWVpacTqe2bNkis9msK6+88py19O7d2/s8MTFRknTkyJFGnyMAwHdZjC4AAICmUl5eLklavHixkpOT622zWq31AtbFCgwMbFA/f39/73OTySTJcz0YAKD1YuQKANBqdO/eXVarVbm5uercuXO9R0pKirff2rVrvc+PHz+u3bt3q1u3bpKkbt266csvv6y33y+//FLp6ekym83q1auXXC5XvWu4AACQGLkCALQioaGhmj59uh566CG5XC4NHz5cJSUl+vLLLxUWFqbU1FRJ0lNPPaXo6GjFx8frN7/5jWJiYnTjjTdKkn75y19q4MCBmj17tm677TatWbNG8+bN00svvSRJSktL05133qmf/OQn+tOf/qQ+ffpo//79OnLkiG699VajTh0A4AMIVwCAVmX27NmKjY1VZmam9u3bp4iICPXv31+PPvqod1reM888o6lTpyorK0t9+/bV//73PwUEBEiS+vfvr7feektPPPGEZs+ercTERD311FOaMmWK9xgvv/yyHn30Ud13330qLCxU+/bt9eijjxpxugAAH8JqgQCANuPESn7Hjx9XRESE0eUAAFoZrrkCAAAAgCZAuAIAAACAJsC0QAAAAABoAoxcAQAAAEATIFwBAAAAQBMgXAEAAABAEyBcAQAAAEATIFwBAAAAQBMgXAEAAABAEyBcAQAAAEATIFwBAAAAQBMgXAEAAABAEyBcAQAAAEATIFwBAAAAQBOwGF2AL3K5XDp06JBCQ0NlMpmMLgcAAACAQdxut8rKypSUlCQ/v3OPTRGuzuDQoUNKSUkxugwAAAAAPiIvL0/t2rU7Zx/C1RmEhoZK8vwHDAsLM7gaAAAAAEYpLS1VSkqKNyOcC+HqDE5MBQwLCyNcAQAAAGjQ5UIsaAEAAAAATYBwBQAAAABNgHAFAAAAAE2Aa64AAACA73C73aqtrZXT6TS6FDQzs9ksi8XSJLdgIlwBAAAAp3A4HMrPz1dlZaXRpeASCQoKUmJiogICAhq1H8IVAAAAUMflcik7O1tms1lJSUkKCAhokhEN+Ca32y2Hw6GjR48qOztbXbp0Oe+Ngs+FcAUAAADUcTgccrlcSklJUVBQkNHl4BIIDAyUv7+/9u/fL4fDIZvNdtH7YkELAAAA4DsaM3qBlqepPm9GrgAAAAD4hFqnSxUOpyrstTL7mRQfdvGjSEYgXAEAAAAwRI3TpUp7rcrrAlV1zcnVGf3NfooLtbaoa94Y7wQAAACgtLQ0zZ071/vaZDJp0aJFZ+2fk5Mjk8mkLVu2nHffK1askMlk0tHCIhVXOnTgeKV2FZRpR36p9hdVqrDc7g1WVotZUcEBSgxvWaNWEuEKAAAAwBnk5+dr3LhxjdqHo9ap4xUOHS2tliTtKihTblGliiocstd6wpTN36zoEKvaRwWpW2KYMhJC1S4ySBFBF79S43//+1917dpVNptNvXr10pIlSxp1Hg3FtEAAAAAAp0lISLig/m63W45alyoctaqwe6b5OZwuSVKpvVaSZJIU6G9WsNXieQSYZTE37XjP6tWrNXHiRGVmZuq6667TG2+8oRtvvFGbNm1Sz549m/RY38XIFQAAAHAObrdblY5aQx5ut7tBNb766qtKSkqSy+Wq137DDTfoJz/5ifbu3asbbrhB8fHxCgkJ0cCBA7Vs2bJz7vO70wLXr1+vfv36yWazacCAAdq0aZMkqbjSodzCSu0sKNOuw2U6cLxKxysdcjhdMsmkoACLIgL9JUldE0PVJT5USRGBWrbkffXp3UtWq1VpaWl6/vnn6x3/pZdeUpcuXWSz2RQfH6/vf//73m0LFy5Ur169FBgYqOjoaI0aNUoVFRWSpBdeeEFjx47Vww8/rG7dumn27Nnq37+/5s2b16D/lo3ByBUAAABwDlU1TnV/4mNDjv3tU2MUFHD+X9l/8IMf6Oc//7k+++wzjRw5UpJUVFSkjz76SEuWLFF5ebnGjx+v3/3ud7JarfrHP/6hCRMmaNeuXWrfvv15919eXq7rrrtOV48cpT+/+rp2Z+3VL6b9UpJ0pMyuqCqHJE8gC/KOTJkVFGCR2c+kAyFWSZK5bsnzjRs36tZbb9XMmTN12223afXq1brvvvsUHR2tKVOm6KuvvtKDDz6of/7znxo6dKiKioq0atUqSZ7pihMnTtSzzz6rm266SWVlZVq1apU3iK5Zs0bTpk2rV/+YMWPOef1YUyFcAQAAAC1cZGSkxo0bpzfeeMMbrhYuXKiYmBhdddVV8vPzU58+fbz9Z8+erXfffVfvv/++HnjggbPu11Hr0rFyu1579f9UU+vUL2fPkdVm04DEDpr8/36u3z36SwX5mxUfZlNwgEVBAWb5+Z3/Oqk5c+Zo5MiRevzxxyVJ6enp+vbbb/Xcc89pypQpys3NVXBwsK677jqFhoYqNTVV/fr1k+QJV7W1tbr55puVmpoqSerVq5d33wUFBYqPj693vPj4eBUUFDTwv+bFI1wBAAAA5xDob9a3T40x7NgNNWnSJN1999166aWXZLVa9e9//1s//OEP5efnp/Lycs2cOVOLFy/2hpOqqirl5ubW20eN06XCcrvK666ROlRcpUPFVfr22x3q0q2HAgMDFRRgVojVoutGXanfPSq1iwq64PtR7dixQzfccEO9tmHDhmnu3LlyOp0aPXq0UlNT1bFjR40dO1Zjx47VTTfdpKCgIPXp00cjR45Ur169NGbMGF1zzTX6/ve/r8jIyAuqoTlwzRUAAABwDiaT57ohIx4XslrehAkT5Ha7tXjxYuXl5WnVqlWaNGmSJGn69Ol699139fTTT2vVqlXasmWLevXqpcpquwrL7dpfWKEap1tHy+w6WFylkqqaunOXQqwWBVnNCvQ3q3tSmDrGhiguzKbABkxXvFihoaHatGmT3nzzTSUmJuqJJ55Qnz59VFxcLLPZrKVLl+rDDz9U9+7d9ec//1kZGRnKzs6W5FmI4/Dhw/X2d/jw4QteoONiEK4AAACAVsBms+nmm2/Wv//9b7355pvKyMhQ//79JUlffvml7rzzTo277noldUhXjX+Y9mXnqLjScUqYcnvD1ImRqJSoIHWMDdFlfXpp+7atctjt3uOtXbv2omvt1q2bvvzyy3ptX375pdLT02U2e0brLBaLRo0apWeffVbffPONcnJy9Omnn0ryBN5hw4Zp1qxZ2rx5swICAvTuu+9KkoYMGaLly5fX2/fSpUs1ZMiQi663oZgWCAAAALQSkyZN0nXXXaft27dr0qRJstc4VW6vVXJqR7351kJ1HXyVTCbpxeee9q4sGFK3LLrFz08JYTZ1jA3x7s+vbuTs9ttv129+8xvdfffdmjFjhnJycvSHP/zhouv85S9/qYEDB2r27Nm67bbbtGbNGs2bN08vvfSSJOmDDz7Qvn37dMUVVygyMlJLliyRy+VSRkaG1q1bp+XLl+uaa65RXFyc1q1bp6NHj6pbt26SpKlTp+rKK6/U888/r2uvvVYLFizQV199pVdfffWi620oRq4AAACAVmL4FVcqIjJKu3bt0uDR12vX4TIdLK7S1N/MVmh4hO68cYym/uR2jRk7Rv369VN0cIA6xoYoPswmk0lnnYYYEhKi//3vf9q6dav69eun3/zmN/r9739/0XX2799fb731lhYsWKCePXvqiSee0FNPPaUpU6ZIkiIiIvTOO+/o6quvVrdu3fTKK6/ozTffVI8ePRQWFqbPP/9c48ePV3p6uh577DE9//zz3hseDx06VG+88YZeffVV9enTRwsXLtSiRYua/R5XkmRyN3Tx/DaktLRU4eHhKikpUVhYmNHlAAAA4BKprq5Wdna2OnToIJvtwhZpMEKN06UKe63K6x6O2vr3ufJcL2b2jk4F+TdsNb+25lyf+4VkA6YFAgAAAC2E0+WuF6aqa5z1tntu2mtWsM2ikAtYGh1Ng3AFAAAA+CiX261Ku9MbpqocTrlVf+KZzd8zMnVidMpsQJi699579a9//euM2370ox/plVdeucQVGYNwBQAAAPgIt9utqrpFKMqra1XpcMr1nat4Aix+3jAVYrXIYjZ+GYWnnnpK06dPP+O2tnSZDeEKAAAAMIjb7Za9tv51U05X/TBl8fNTiO1EmDIrwNLwGwtfKnFxcYqLizO6DMMRrgAAAIDvaM4132pqXd4gVW6vVY2z/iIUZpNJwVaLN1BZLX4XdDNhXLim+rwJVwAAAEAdf39/SVJlZaUCAwObZJ/1FqGorlV17XcWoTCZFBxw8rqpwAAzYeoSq6yslHTy879YhCsAAACgjtlsVkREhI4cOSJJCgoKuuCg43a7VV3jVKXDqQpHraprXPVGRkySrP5mBQaYFRxgVqC/5eSKfu5a2e21TXU6OA+3263KykodOXJEERERMpsbN+WScAUAAACcIiEhQZK8Aashal0u2Wtcste6ZK9xyvmdWWYWP5Os/n6yWcyyWvzk8jOpQlJFE9aNixcREeH93BuDcAUAAACcwmQyKTExUXFxcaqpqTljnwp7jTbnFmtT7nF9tb9YB49X1tseHGBR35QIXZYWqQGpkUqKCGSqn4/y9/dv9IjVCYaGq5kzZ2rWrFn12jIyMrRz586zvqe4uFi/+c1v9M4776ioqEipqamaO3euxo8ff9H7BAAAAL7LbDZ7f+mudbr09YESrco6qi+yjmlzXnG9Vf3Mfib1S4nQ8C4xGtElRn3aRfjEEum4tAwfuerRo4eWLVvmfW2xnL0kh8Oh0aNHKy4uTgsXLlRycrL279+viIiIi94nAAAAcCb7Cyv0edYxfZF1VKv3Fqqsuv61UB1igjW8sydMXd4pWmG2xi2GgJbP8NRhsVgaPL/x9ddfV1FRkVavXu1dySMtLa1R+wQAAAAkqay6Rl/uKdTnWUe1Kuuo8oqq6m0PD/TX8M4xGt4lRsM7xyglKsigSuGrDA9XWVlZSkpKks1m05AhQ5SZman27dufse/777+vIUOG6P7779d7772n2NhY3X777XrkkUfqzZO8kH1Kkt1ul91u974uLS1tuhMEAACAT3K73dpZUKYVu45q5e4j+irnuGpPmernbzapf/tIjegSoxFdYtUzOVxmP66bwtkZGq4GDx6s+fPnKyMjQ/n5+Zo1a5ZGjBihbdu2KTQ09LT++/bt06effqpJkyZpyZIl2rNnj+677z7V1NToySefvKh9SlJmZuZp12kBAACg9SmpqtGXe45pxa4jWrn7qA6X2utt7xATrCvTY3VFeowGd4hWsNXwsQi0ICZ3c95++gIVFxcrNTVVc+bM0V133XXa9vT0dFVXVys7O9s7UjVnzhw999xzys/Pv6h9SmceuUpJSVFJSYnCwsKa4MwAAABgBJfLrW/zS7Vy91Gt2HVEm3LrL0Rh8/fT0E4xujI9Vt/LiFVqdLCB1cIXlZaWKjw8vEHZwKeieEREhNLT07Vnz54zbk9MTDxtqcRu3bqpoKBADodDAQEBF7xPSbJarbJarY0/AQAAABiuuNKhVVnH6qb7HdWx8vqjU51ig/W9jDhdmR6rQR2iZPNvmmW4AZ8KV+Xl5dq7d6/uuOOOM24fNmyY3njjDblcLvn5eZa23L17txITE88YrBqyTwAAALRsLpdbWw+WeEentuQV65TBKQUFmDW0U4y+lxGrK9NjWYgCzcbQcDV9+nRNmDBBqampOnTokJ588kmZzWZNnDhRkjR58mQlJycrMzNTkvSzn/1M8+bN09SpU/Xzn/9cWVlZevrpp/Xggw82eJ8AAABo+YoqHPp8t2dk6vPdR1VY4ai3PSM+VFdmxOp76bEakBalAAv3nELzMzRcHThwQBMnTlRhYaFiY2M1fPhwrV27VrGxsZKk3Nxc7wiVJKWkpOjjjz/WQw89pN69eys5OVlTp07VI4880uB9AgAAoOVxudzafqhUy3ce1me7juqbA8U6deWAEKtFwzvH6Mq60amkiEDjikWb5VMLWviKC7loDQAAAM2j3F6rL7KO6bOdR/TpriM6Wlb/2qluiWHeqX6XpUbK38zoFJpei13QAgAAAG3b/sIKfbrziD7deUTr9hXJ4XR5twUHmDWiS6yu7hqnKzNiFR9mM7BS4HSEKwAAABimxunSVznH9dmuI1q+47D2Hq2otz01OkhXd43TyK7xGtghUlYLK/vBdxGuAAAAcEkVVTi0YtcRLd95RJ/vPqqy6lrvNoufSQPTojSyW5yu6hqnjjHBMplMBlYLNBzhCgAAAM3K7XZrR36Zd3Rqc179xSiiggP0vYxYjewarxHpMQqz+RtXLNAIhCsAAAA0uSqHU6v3HtPynUf02c4jyi+prre9e2KYd3SqT7sImf0YnULLR7gCAABAkygoqdbSHYf16Y7DWr23UPbak4tR2Pz9NLyzZzGKq7rGKjGcpdLR+hCuAAAAcFHcbrd2Hy7X0m8LtPTbw/r6QEm97ckRgd7RqSEdo2XzZzEKtG6EKwAAADRYrdOljfuP65NvD2vpt4eVW1Tp3WYySf1SIjSqe7xGdYtXl7gQFqNAm0K4AgAAwDlVOmr1+e5jWvrtYX2687COV9Z4twVY/DSic4xGd4/XyG7xig21GlgpYCzCFQAAAE5zrNyu5TsO65Pth/XFnmP1rp+KCPLX1V3jdE33eI3oEqtgK79SAhLhCgAAAHX2Hi3X0rrpfptyj9dbLj0lKlDXdE/Q6O7xGpAaKYvZz7hCAR9FuAIAAGijXC63NucV1wWqAu09WlFve+924RrdLV6je8QrIz6U66eA8yBcAQAAtCHVNZ77T32y/bCW7TiiY+V27zZ/s0mXd4zWNd3jNap7PMulAxeIcAUAANDKVdhr9enOI/poe4E+23lElQ6nd1uo1aKrusZpdPd4XZkRqzCbv4GVAi0b4QoAAKAVKqmq0fIdh/XhtgKt3H1UjlMWpEgMt2l093iN7h6vwR2iFWDh+imgKRCuAAAAWonCcruWfusJVKv3HlON8+SKFGnRQRrbM1Hjeiaod7twrp8CmgHhCgAAoAU7XFqtj7cX6MOtBVqXXSjXKSv8pceHeANV1wQWpACaG+EKAACghTlwvFIfbSvQh9sKTlsyvWdymMb1TNTYngnqFBtiXJFAG0S4AgAAaAGyj1Xow235+mhbgb45UFJvW//2Ed5AlRIVZFCFAAhXAAAAPsjtdmv34XJvoNpZUObd5meSBnWI0rieiRrTI0EJ4TYDKwVwAuEKAADAR7jdbm07WOoNVPuOnbypr8XPpCGdojWuZ6Ku6RGvmBCrgZUCOBPCFQAAgIHcbre+zS/VB9/ka/E3+cotqvRuC7D46YouMRrbM1Gju8UrPIh7UAG+jHAFAABggF0FZfrgm0Na/E1+vRGqQH+zruoaq7E9E3V11ziFWPl1DWgp+LYCAABcInuOlGvxN/n64JtDyjpS7m23Wvx0VUacruvjCVRBAfyKBrREfHMBAACaUc6xCi3emq//fX2o3qIUAWY/XZEeqwl9EjWyWzwjVEArwLcYAACgieUVVWrxVs8I1baDpd52i59JI7rE6LreSRrdI15hNq6hAloTwhUAAEATOFRcpSVb8/W/b/L1dV6xt93sZ9LQTtGa0DtJ1/SIV0RQgHFFAmhWhCsAAICLdLi0Wku25uuDb/K1cf9xb7ufSbq8Y7Su7Z2osT0SFM2y6UCbQLgCAAC4AEfL7Ppom2eEakNOkdxuT7vJJA1MjdJ1fRI1tmeC4kK5sS/Q1hCuAAAAzqO0ukYfbyvQe1sOafXeY3K5T27r3z5C1/VO0vheiUoIJ1ABbRnhCgAA4AzstU6t2HVU7205qGU7jshR6/Ju69Mu3BOoeicqOSLQwCoB+BLCFQAAQB2Xy6212YV6f8shLdmar9LqWu+2znEhurFvkq7vk6z20UEGVgnAVxGuAABAm+Z2u7X9UKne//qQ3t9ySAWl1d5tCWE2Xd83Sdf3SVKPpDCZTCYDKwXg6whXAACgTcotrNT7Xx/Uoi2HtOdIubc9zGbR+F6Jur5vkgZ3iJbZj0AFoGEIVwAAoM04Vm7X4m/y9d6Wg9qUW+xtD7D4aVS3OF3fJ1lXdY2V1WI2rkgALRbhCgAAtGoV9lp98q1npb9VWcfkrFvqz88kDe0Uoxv6JmlMzwSF2fwNrhRAS+dn5MFnzpwpk8lU79G1a9dzvqe4uFj333+/EhMTZbValZ6eriVLltTr8+KLLyotLU02m02DBw/W+vXrm/M0AACAj6lxurR8x2H9/M3Nuuy3S/XQf77Wil1H5XS51btduB6/rrvWzhipf/10sH4wIIVgBaBJGD5y1aNHDy1btsz72mI5e0kOh0OjR49WXFycFi5cqOTkZO3fv18RERHePv/5z380bdo0vfLKKxo8eLDmzp2rMWPGaNeuXYqLi2vOUwEAAAZyu93auP+43t18UEu25ut4ZY13W1p0kG7om6zr+yapU2yIgVUCaM0MD1cWi0UJCQkN6vv666+rqKhIq1evlr+/51+Y0tLS6vWZM2eO7r77bv34xz+WJL3yyitavHixXn/9df36179u0toBAIDx8ooq9famA3pn00HlFlV622NCrJrQJ1E39E1Wn3bhrPQHoNkZHq6ysrKUlJQkm82mIUOGKDMzU+3btz9j3/fff19DhgzR/fffr/fee0+xsbG6/fbb9cgjj8hsNsvhcGjjxo2aMWOG9z1+fn4aNWqU1qxZc6lOCQAANLOy6hp9uLVACzcd0PrsIm97cIBZY3om6Ma+yRraKVoWs6FXQABoYwwNV4MHD9b8+fOVkZGh/Px8zZo1SyNGjNC2bdsUGhp6Wv99+/bp008/1aRJk7RkyRLt2bNH9913n2pqavTkk0/q2LFjcjqdio+Pr/e++Ph47dy586x12O122e127+vS0tKmO0kAANAknC63Vu89prc3HtBH2wtUXeOSJJlM0rBOMbq5f7LG9kxQUIDh/3YMoI0y9E+fcePGeZ/37t1bgwcPVmpqqt566y3dddddp/V3uVyKi4vTq6++KrPZrMsuu0wHDx7Uc889pyeffPKi68jMzNSsWbMu+v0AAKD57DlSpoUbD2rR5oP1bvDbMTZYt/Rvp5v6JSspItDACgHAw6f+aSciIkLp6enas2fPGbcnJibK399fZvPJe09069ZNBQUFcjgciomJkdls1uHDh+u97/Dhw+e8rmvGjBmaNm2a93VpaalSUlIaeTYAAOBiHa9w6H/fHNLbGw/o6wMl3vbwQH9N6JOoW/q3U9+UCK6jAuBTfCpclZeXa+/evbrjjjvOuH3YsGF644035HK55OfnmUO9e/duJSYmKiAgQJJ02WWXafny5brxxhsleUa7li9frgceeOCsx7VarbJarU17MgAA4II4al1aseuI3t50QJ/uPKIap+d+VGY/k67KiNUt/dvp6m5x3OAXgM8yNFxNnz5dEyZMUGpqqg4dOqQnn3xSZrNZEydOlCRNnjxZycnJyszMlCT97Gc/07x58zR16lT9/Oc/V1ZWlp5++mk9+OCD3n1OmzZNd955pwYMGKBBgwZp7ty5qqio8K4eCAAAfIfb7da2g6V6e9MBvf/1IRVVOLzbeiSF6eb+7XRD3yTFhPCPoAB8n6Hh6sCBA5o4caIKCwsVGxur4cOHa+3atYqNjZUk5ebmekeoJCklJUUff/yxHnroIfXu3VvJycmaOnWqHnnkEW+f2267TUePHtUTTzyhgoIC9e3bVx999NFpi1wAAADjHCmt1rubD+rtTQe0+3C5tz0mxKqb+iXplsvaqWtCmIEVAsCFM7ndbrfRRfia0tJShYeHq6SkRGFh/MEOAEBTqK5x6pNvD+vtjQe0KuuoXHW/gQRY/HRN93jd0r+dRnSJYfl0AD7lQrKBT11zBQAAWp/th0r01oY8LdpySCVVNd72y1IjdUv/drq2d6LCA/0NrBAAmgbhCgAANLmSqhq9//UhvbUhT1sPnlztLyncplsua6eb+7dTh5hgAysEgKZHuAIAAE3C7XZrXXaR/rMhT0u25ste67nJr7/ZpGu6J+i2gSka1jlGZj+WTwfQOhGuAABAoxwurdbCjQf036/ylFNY6W1Pjw/RbQPb66Z+yYoKDjCwQgC4NAhXAADggtU4Xfps5xG99VWePtt1VM661SmCA8y6vm+Sbh2Qwk1+AbQ5hCsAANBg+46W662vDujtTQd0tMzubR+QGqlbB6bo2l6JCrby6wWAtok//QAAwDlVOZxasjVf/9mQp/U5Rd72mJAA3dy/nW4dkKLOcSEGVggAvoFwBQAATuN2u7X1YIkWbMjT/7YcUpm9VpLkZ5KuTI/VbQPba2S3OPlzTyoA8CJcAQAAr+JKh97dfFD/2ZCnnQVl3vb2UUG6dUA73XJZOyWGBxpYIQD4LsIVAABtnNvt1vrsIr2xPlcfbi2Qw+lZQj3A4qdxPRN024AUXd4xWn4soQ4A50S4AgCgjSqprNE7mw/o3+tytedIube9e2KYbhuYohv7Jis8yN/ACgGgZSFcAQDQhrjdbm3JK9a/1+Xqg28OqbrGM0oV6G/WDX2TdPvg9urdLsLYIgGghSJcAQDQBpTba/XeloP699pcfZtf6m3PiA/Vjy5vrxv6JSvMxigVADQG4QoAgFZs+6ESvbEuV4s2H1SFwynJcy3Vdb0SNeny9urfPpIb/QJAEyFcAQDQylQ5nPrgm0P697pcbckr9rZ3jAnW7YPb65b+7RQZHGBcgQDQShGuAABoJbIOl+nf63L1zqYDKq323JfK32zSNT0SNGlwew3pGM0oFQA0I8IVAAAtmL3WqY+2Fejf63K1PrvI294uMlC3D26vH1yWothQq4EVAkDbQbgCAKAFyjlWoTfX5+q/Gw+oqMIhSfIzSSO7xWvS4Pa6okss96UCgEuMcAUAQAtR43Rp+Y7D+ve6XK3KOuZtTwiz6YeDUnTbwBQlhgcaWCEAtG2EKwAAfNyxcrveXJerf63br8OldkmSySRd0SVWkwa319Vd42Qx+xlcJQCAcAUAgI/adrBE//dljv739SE5nJ6b/caEBOjWASmaOKi9UqKCDK4QAHAqwhUAAD6kxunSx9sLNP/LHH21/7i3vU+7cP14WAeN75WoAAujVADgiwhXAAD4gMJyuxZsyNM/1+xXQWm1JMniZ9K1vRM1ZWia+rWPNLhCAMD5EK4AADDQ9kMlmv9ljt77+pActSen/t0+OFWTBrdXfJjN4AoBAA1FuAIA4BKrdbq09NvD+r/VOfXuTdUrOVw/Hpama3snymoxG1ghAOBiEK4AALhEjlc46qb+5ehQycmpf+N6JWrK0FT1bx8pk4l7UwFAS0W4AgCgme3IL9XfV+fo3c0HZa+b+hcdHKDbB7fXpMGpSghn6h8AtAaEKwAAmoHT5dbSbw9r/upsrd13cupfj6Qw/XhYB13XO1E2f6b+AUBrQrgCAKAJFVc69J8NefrHmv06WFwlSTL7mTS2R4J+PCxNl6Uy9Q8AWivCFQAATWDPkXL97Ytsvbv5gKprPFP/IoP8NXFQe/3o8lQlRQQaXCEAoLkRrgAAuEhut1tr9xXptVX79OnOI972bolh+vGwNF3fJ4mpfwDQhhCuAAC4QDVOl5ZszddfV2Vr68ESSZLJJI3uFq+7hnfQoA5RTP0DgDaIcAUAQAOVVdfoPxvy9H9f5nivp7Ja/PSDAe101/CO6hATbHCFAAAjEa4AADiPQ8VVmr86R2+uy1WZvVaSFBMSoMlD0vSjy1MVFRxgcIUAAF9AuAIA4Cy2HSzRX1ft0wff5KvW5ZYkdYoN1t0jOurGfslcTwUAqIdwBQDAKdxut1bsPqrXPt+n1XsLve1DOkbr7is66HvpcfLz43oqAMDpCFcAAEiqrnHqvS0H9ddV2co6Ui7Jc3+q63on6u4RHdUzOdzgCgEAvs7PyIPPnDlTJpOp3qNr165n7T9//vzT+ttstnp9pkyZclqfsWPHNvepAABaqOMVDv15eZaG//4zPfL2VmUdKVeI1aK7R3TQ57+6Si/8sB/BCgDQIBc1cvX3v/9dMTExuvbaayVJv/rVr/Tqq6+qe/fuevPNN5WamtrgffXo0UPLli07WZDl3CWFhYVp165d3tdnWup27Nix+r//+z/va6vV2uB6AABtw/7CCv3ti2y99VWe96a/ieE2/WRYB902KEVhNn+DKwQAtDQXFa6efvppvfzyy5KkNWvW6MUXX9Qf//hHffDBB3rooYf0zjvvNLwAi0UJCQkN7m8ymc7b32q1XtA+fdnzn+zS6r2FeuGHfdUuMsjocgCgxdu4v0ivfZ6tj78tkNuzRoV6JIXpnis6anyvRPmbDZ3UAQBowS4qXOXl5alz586SpEWLFumWW27RPffco2HDhul73/veBe0rKytLSUlJstlsGjJkiDIzM9W+ffuz9i8vL1dqaqpcLpf69++vp59+Wj169KjXZ8WKFYqLi1NkZKSuvvpq/fa3v1V0dPRZ92m322W3272vS0tLL+gcmtPnu4/q6wMl+irnOOEKAC6Sy+XW0h2H9ZeVe7Upt9jbflVGrO6+oqOGdIzmpr8AgEa7qH+eCwkJUWGhZwWlTz75RKNHj5Yk2Ww2VVVVNXg/gwcP1vz58/XRRx/p5ZdfVnZ2tkaMGKGysrIz9s/IyNDrr7+u9957T//617/kcrk0dOhQHThwwNtn7Nix+sc//qHly5fr97//vVauXKlx48bJ6XSetY7MzEyFh4d7HykpKQ0+h+Y2MC1KkrQ+p8jgSgCg5al1urRo80GNfeFz/b9/btSm3GIFmP30w4EpWvrQFfq/Hw/S0E4xBCsAQJMwud0nJkU03KRJk7Rz507169dPb775pnJzcxUdHa33339fjz76qLZt23ZRxRQXFys1NVVz5szRXXfddd7+NTU16tatmyZOnKjZs2efsc++ffvUqVMnLVu2TCNHjjxjnzONXKWkpKikpERhYWEXdS5N5ePtBfp//9yoLnEhWjrtSkNrAYCWwl7r1DubDuqVlXu1v7BSkhRqteiOIamaMixNcaG28+wBAACP0tJShYeHNygbXNS0wBdffFGPPfaY8vLy9Pbbb3un3G3cuFETJ068mF1KkiIiIpSenq49e/Y0qL+/v7/69et3zv4dO3ZUTEyM9uzZc9ZwZbVafXbRiwGpkZKkrCPlOl7hUGRwgMEVAYDvqnI49eb6XL36+T4VlFZLkqKCA/STYWm6Y0iawgNZpAIA0HwuKlxFRERo3rx5p7XPmjWrUcWUl5dr7969uuOOOxrU3+l0auvWrRo/fvxZ+xw4cECFhYVKTExsVG1GiQ6xqlNssPYerdBX+49rdPd4o0sCAJ9TWl2jf67Zr799ka2iCockKT7Mqnuu6KSJg1IUFMBtHQEAze+irrn66KOP9MUXX3hfv/jii+rbt69uv/12HT9+vMH7mT59ulauXKmcnBytXr1aN910k8xms3f0a/LkyZoxY4a3/1NPPaVPPvlE+/bt06ZNm/SjH/1I+/fv109/+lNJnnD28MMPa+3atcrJydHy5ct1ww03qHPnzhozZszFnKpPGNTBc93VBq67AoB6iioc+sPHuzTsmU/13Me7VFThUPuoIGXe3Euf/+oq3TW8A8EKAHDJXFS4evjhh70r6m3dulW//OUvNX78eGVnZ2vatGkN3s+BAwc0ceJEZWRk6NZbb1V0dLTWrl2r2NhYSVJubq7y8/O9/Y8fP667775b3bp10/jx41VaWqrVq1ere/fukiSz2axvvvlG119/vdLT03XXXXfpsssu06pVq3x22l9DeBe1yCZcAYAkFZRU66n/fathz3yqeZ/tUVl1rbrEhWjubX316S+v1MRB7WW1mI0uEwDQxlzUghYhISHatm2b0tLSNHPmTG3btk0LFy7Upk2bNH78eBUUFDRHrZfMhVy0dinkFVVqxLOfyeJn0taZYxQYwC8MANqm/YUVemXlPr298YAcTs+Nf3slh+v+qzrrmu7x8vNj1T8AQNNq9gUtAgICVFnpWX1p2bJlmjx5siQpKirKp+4R1Vq0iwxUQphNBaXV2px3XEM7xRhdEgBcUrsPl+mlz/bo/a8PyVX3T4KDOkTpgas6a0QXllIHAPiGiwpXw4cP17Rp0zRs2DCtX79e//nPfyRJu3fvVrt27Zq0QEgmk0kDO0Tpf18f0oZswhWAtuObA8V68bM9+nj7YW/blemxeuDqzt4p0wAA+IqLClfz5s3Tfffdp4ULF+rll19WcnKyJOnDDz/U2LFjm7RAeAxKi/SEKxa1ANAGrNtXqHmf7dGqrGOSJJNJGtsjQfdf1Vk9k8MNrg4AgDO7qHDVvn17ffDBB6e1//GPf2x0QTizAXX/Qrsp97hqnS5ZzBe1FgkA+Cy3260Vu4/qpc/2aEOOZ+VZs59JN/RN0n3f66TOcaEGVwgAwLld9Pq0TqdTixYt0o4dOyRJPXr00PXXXy+zmcUWmkNGfKjCbBaVVtfq2/xS9W4XYXRJANAk3G63Ptt1RH9cmqWtB0skSQFmP/1gQDvde2UnpUQFGVwhAAANc1Hhas+ePRo/frwOHjyojIwMSVJmZqZSUlK0ePFiderUqUmLhOTnZ9KAtCh9uvOI1mcXEa4AtHhut1srdx/VH5dl6eu8YklSoL9Zkwa3191XdFR8mM3YAgEAuEAXFa4efPBBderUSWvXrlVUlGe6WmFhoX70ox/pwQcf1OLFi5u0SHgMrAtXG3KK9NMRHY0uBwAuitvt1hd7jumPS3drU26xJE+omjw0VfeM6KjokJZ7X0IAQNt2UeFq5cqV9YKVJEVHR+uZZ57RsGHDmqw41DcwLVKS9FXOcbndbpYeBtDirN7rCVUnrqmyWvw0eUiq/t+VnRRDqAIAtHAXFa6sVqvKyspOay8vL1dAQECji8KZ9WoXrgCLnworHNp3rEKdYkOMLgkAGmTdvkLNWbpb67I9K54GWPw0aXB7/ezKTopj+h8AoJW4qHB13XXX6Z577tHf/vY3DRo0SJK0bt063Xvvvbr++uubtECcZLWY1TclQuuzi7Qhu4hwBcDnfZVTpD8u260v9xRK8ixUMXFQin72vc5KCCdUAQBal4sKV3/605905513asiQIfL395ck1dTU6IYbbtDcuXObsj58x6C0KK3PLtL6nCL9cFB7o8sBgDPalHtcf1y623ufKn+zSbcNTNF93+uspIhAg6sDAKB5XFS4ioiI0Hvvvac9e/Z4l2Lv1q2bOnfu3KTF4XQDTrnuCgB8zdd5xfrjst1aseuoJMniZ9IPBrTT/Vd1VrtIllQHALRuDQ5X06ZNO+f2zz77zPt8zpw5F18Rzumy1Ej5maTcokodLq1mqWIAPmHbwRL9celuLd95RJLn5r+39E/Wz6/uwn2qAABtRoPD1ebNmxvUjxXsmleozV/dEsO0/VCp1mcXaUKfJKNLAtCGbT9UornLsrT028OSJD+TdFO/dvr51Z2VFhNscHUAAFxaDQ5Xp45MwVgD06K0/VCpNuQQrgAYY2dBqV5YlqUPtxVIkkwm6YY+SXpwZBd1ZLEdAEAbdVHXXMFYA9OiNH91jvc+MQBwqWQdLtPc5Vla/E2+JE+ouq53kqaO7KzOcaEGVwcAgLEIVy3QwA6eRS12FpSqpKpG4YH+BlcEoLXbX1ihOUt36/2vD8nt9rRd2ytRU0d1UXo8oQoAAIlw1SLFhdqUFh2knMJKbdp/XFd1jTO6JACt1NEyu/78aZbeWJerWpcnVY3tkaCpo7qoW2KYwdUBAOBbCFct1IC0KOUUVmpDThHhCkCTK7fX6rXP9+m1VftU6XBKkq5Mj9XDYzLUMznc4OoAAPBNhKsWalBalBZuPKANOUVGlwKgFXHUuvTm+lz9aXmWCisckqTe7cL167FdNbRzjMHVAQDg2whXLdTADlGSpK/zSlRd45TN32xwRQBaMpfLrQ+25usPH+9SblGlJKlDTLCmX5Oh8b0SuM0GAAANQLhqodKigxQTYtWxcru+OVCiQXVhCwAu1Kqso3rmw53afqhUkhQTYtUvRnXRbQNT5G/2M7g6AABaDsJVC2UymTQwLVIfbivQhpwiwhWAC7b1QIl+/9FOfbHnmCQpxGrR/7uio34yvIOCrfz1AADAheJvzxZsYFqUN1wBQEPtL6zQHz7Zrf99fUiS5G826Y7L03T/VZ0UHWI1uDoAAFouwlULdmK0amPOcTldbpn9uCYCwNl9d1l1k0m6sW+ypo1OV0pUkNHlAQDQ4hGuWrCuCaEKDjCrzF6rnQWl6pHE8sgATne2ZdV/NTaDPzcAAGhChKsWzGL2U//USK3KOqavco7zSxKAes60rHqfduF6ZFxXDe3EsuoAADQ1wlULNygtSquyjml9TpHuHJpmdDkAfIDL5db/vjmk5z/ZXW9Z9YfHZGhcT5ZVBwCguRCuWrgT97vakF0kt9vNL01AG8ey6gAAGIdw1cL1TYmQv9mkI2V25RZVKjU62OiSABhg28ESPfNh/WXV773Ss6x6UAB/1AMAcCnwN24LZ/M3q1dyuDblFmtDznHCFdDGHCqu0h8+2aV3Nx+U231yWfUHru6sqOAAo8sDAKBNIVy1AgM7RHnCVXaRvn9ZO6PLAXAJlFXX6JWVe/XXVdmy17okSdf3SdLDYzJYVh0AAIMQrlqBQWlR+svKfdxMGGgDap0uLdiQp7nLdutYuWcFwEFpUfrNtd3UJyXC2OIAAGjjCFetwGWpkZKkfccqdLTMrthQq8EVAWhqbrdbn+48oqeX7NDeoxWSPCsA/npcV13TPZ7FbAAA8AGEq1YgIihAGfGh2nW4TBv3F2lsz0SjSwLQhLYdLNHTS3Zo9d5CSVJkkL9+MSpdtw9uzwqAAAD4EMJVKzGwQ6R2HS7T+uzjhCuglfjuYhUBFj/9ZFgH3XdVJ4XZ/I0uDwAAfAfhqpUYmBalf63N5boroBU402IVN/T1LFbRLpLFKgAA8FWGzieZOXOmTCZTvUfXrl3P2n/+/Pmn9bfZbPX6uN1uPfHEE0pMTFRgYKBGjRqlrKys5j4Vww1M89xMePuhEpXbaw2uBsDFqHW69K+1+3XVH1boxc/2yl7r0qC0KL13/zC98MN+BCsAAHyc4SNXPXr00LJly7yvLZZzlxQWFqZdu3Z5X3/3Iu5nn31Wf/rTn/T3v/9dHTp00OOPP64xY8bo22+/PS2ItSZJEYFKjgjUweIqbc49rhFdYo0uCUADsVgFAACtg+HhymKxKCEhocH9TSbTWfu73W7NnTtXjz32mG644QZJ0j/+8Q/Fx8dr0aJF+uEPf9gkNfuqQR2i9O7mg9qQXUS4AlqIbQdL9LvFO7RmH4tVAADQ0hn+N3dWVpaSkpLUsWNHTZo0Sbm5uefsX15ertTUVKWkpOiGG27Q9u3bvduys7NVUFCgUaNGedvCw8M1ePBgrVmz5qz7tNvtKi0trfdoiQakeZZkX891V4DPO1RcpWlvbdGEeV9ozb5CBVj8dO+VnbTyV1fpzqFpBCsAAFogQ0euBg8erPnz5ysjI0P5+fmaNWuWRowYoW3btik0NPS0/hkZGXr99dfVu3dvlZSU6A9/+IOGDh2q7du3q127diooKJAkxcfH13tffHy8d9uZZGZmatasWU17cgYYVHfd1Za8YjlqXQqw8MsZ4GtYrAIAgNbL0HA1btw47/PevXtr8ODBSk1N1VtvvaW77rrrtP5DhgzRkCFDvK+HDh2qbt266S9/+Ytmz5590XXMmDFD06ZN874uLS1VSkrKRe/PKJ3jQhQZ5K/jlTXadqhE/dtHGl0SgDq1TpcWbMjT3GW7dazcIckzlfc347upT0qEscUBAIAmYfg1V6eKiIjQ/2/vzqOrqg+1j39PTpITQmYyk5GEBAIEZAoIODFXqVRakFLAgloL9hUp4hVFxrfx6vVWsE5tfaXeVi1qtddZQEFBZkXmkARCEjJB5oEM5Oz3j8CxKaAMCTvJeT5rnWWy9z4nz2GvnytP9tm/X0JCAhkZGZd0vJubG9ddd53j+HP3YhUWFhIW9t1aT4WFhfTr1++ir2Oz2bDZbFcevI2wWCwMjAlg3cFCdh4rUbkSaSO+OHKSFe8fJL2oCoBuZyerGK3JKkRERDqUNvW5saqqKjIzM5sVo+/T2NjIvn37HMfHxsYSGhrKhg0bHMdUVFSwffv2Zle8OrJBZ++70npXIuY7erKK2Wt2MuP/7SC9qAp/TzeW/bgXnzx4A2N6hapYiYiIdDCmXrlasGABEyZMIDo6mry8PJYsWYLVamXq1KkAzJgxg65du5KamgrA8uXLGTJkCPHx8ZSVlfHUU09x/Phx7r77bqDpys28efNYuXIl3bt3d0zFHh4ezsSJE816m9fUufWudh0vxW43cHHRL28i11pFbQPPbkhnzVdZNDQauLpYmDE0hgdGdsfX083seCIiItJKTC1Xubm5TJ06leLiYoKCghg+fDjbtm0jKKhpGvHs7GxcXL67uFZaWso999xDQUEB/v7+DBgwgK+++oqkpCTHMQsXLqS6upp7772XsrIyhg8fzscff9yh17j6V727+tLJzUpZTQMZJ6tICDl/YhARaR2NdoO1u3L4r0/SKK5uuq/q5sQgHr01ifhgL5PTiYiISGuzGIZhmB2iramoqMDX15fy8nJ8fHzMjnPZfv6nbXyVWczKib35xZBos+OIOIVtR4tZ/t5BDuY3LeXQLagzi29L4ubEYJOTiYiIyNW4nG7Qpia0kJYxMCaArzKL2ZlVonIl0spySmpI/egQH+5rWu7Bx8OVeaMSmD40WmtViYiIOBmVqw7o3HpXu7JKTU4i0nFV153hhY2Z/PHLo9SfseNigZ+nRDF/dCIBnd3NjiciIiImULnqgK6L8sPqYuFE2WlOlJ2mq18nsyOJdBh2u8G7e07wnx8fprCiDoDr47qw+LYkeoa1v48Ri4iISMtRueqAOttc6RXuw97ccnYeK6HrdV3NjiTSIXydXcqy9w7ybU4ZAFEBnjx6a0/GaL0qERERQeWqwxoUE8De3HJ2ZJUwUeVK5KoUlNfynx8f5p1vTgDQ2d3K/bd0Z9bwGGyuVpPTiYiISFuhctVBDYoJ4OXNx9ilxYRFrlhtQyN//OIoL2zM5HRDIxYL/LR/BA+NTSTYxzmWdxAREZFLp3LVQQ2K8QfgSGEVpdX1+OsGe5FLZhgGH+zLJ/XDw5woOw3AgGh/lkxIIjnCz9xwIiIi0mapXHVQXbxsdAvqzNGT1ew6XsropBCzI4m0C/tPlLP8vYPsOHvVN8zXg0d+1JMJyWG6r0pERES+l8pVBzY4JqCpXGWVqFyJ/ICTlXX81ydprN2dg2GAh5sL990Yx69uiKOTu+6rEhERkR+mctWBDYoJ4I2dOY6/wIvI+erONPLKliz+8FkGVXVnAPhx33D+Y3wPwrWMgYiIiFwGlasObHBs02LC+3LLOV3fqL++i/wLwzD4aH8BqR8dIqek6b6qPl19WTIhiYFnF+IWERERuRwqVx1YhH8nQnxsFFbU8U1OKdfHBZodSaRN2Jdbzor3v7uvKsTHxsKxPfjJdV1xcdF9VSIiInJlVK46MIvFwqCYAN7fm8+uLJUrkcKKWp76JI23v8513Fd17w1x3HdjNzzd9b9DERERuTr6baKDGxzbVK526r4rcWKn6xv505dHeXFTJjX1jQBM7BfOwnG6r0pERERajspVBzcwuuneka+Pl3Km0Y6r1cXkRCLXjmEY/O+3efznR4fJK68F4LooPx6/LYnrovxNTiciIiIdjcpVB5cY6o23hyuVtWc4mF+hBVDFaXydXcqK9w/yTXYZAOG+HvyH1qsSERGRVqRy1cFZXSwMjPbn87ST7MwqVbmSDu9E2Wme/Pgw/9yTB4Cnu5U5N8Vx94hueLhpxkwRERFpPSpXTmBQbEBTuTpWwuzhsWbHEWkV1XVneGlTJi99cZS6M3YsFvhp/wgeGptIsI+H2fFERETECahcOYFBZ9fs2ZlVgmEY+kiUdCh2u8HbX+fy1CdpFFXWAU0TuTx+WxK9u/qanE5EREScicqVE0iO8MXd1YXi6nqOnqomLsjL7EgiLWLHsRJWvH+QfSfKAYgK8GTRj3owtleo/oggIiIi15zKlROwuVrpF+HHjqwSdmWVqFxJu5dTUkPqR4f4cF8BAF42V35zSzx3DYvB5qr7qkRERMQcKldOYlCsPzuySthxrJQpg6LMjiNyRSprG/jD5xm8sjmL+kY7Lha4c3AU80cnEOhlMzueiIiIODmVKycxMCYAyNRiwtIuNdoN1u7K4elP0zhVVQ/A8PhAHrutJz1CfUxOJyIiItJE5cpJDIj2x2KB7JIaCitqCdHsadJObMk4xYr3D3K4oBKAboGdefTWntzSI1j3VYmIiEibonLlJHw83OgZ6sPB/Ap2ZpVwW3K42ZFEvtfRk1X87sPDrD9UCICPhysPjEpg+pBo3F1dTE4nIiIicj6VKycyODagqVwdU7mStqu8poFVG9J5dWsWZ+wGVhcL04dE88DI7vh3djc7noiIiMhFqVw5kYEx/qz5KosdWaVmRxE5T0Ojnde2Z/P79Ucoq2kA4JYewSz6UU/igzXDpYiIiLR9KldOZPDZxYQPF1RQUduAj4ebyYlEmnyeVsTK9w+SebIagIQQLx67NYkbEoJMTiYiIiJy6VSunEiwjwfRXTw5XlzD7uOl3JwYbHYkcXJHCitZ+cEhvjhyEoCAzu7MH53AnYMicbXqvioRERFpX1SunMzA6ACOF9ew81iJypWYpriqjt+vP8LrO3JotBu4WS38clgsc2+Ox7eTrqiKiIhI+6Ry5WQGx/rz9te5Wu9KTFF/xs5fvspi9WfpVNaeAWBsrxAeGd+TmMDOJqcTERERuToqV05m0Nn7rr7NKae2oREPN6vJicQZGIbBpwcL+d2HhzheXANAr3AfHrs1iaFxXUxOJyIiItIyVK6cTGxgZwK93DlVVc++E+WOsiXSWg7klbPi/YNsO9p0tTTI28ZDYxOZ1D8Cq4sWARYREZGOQ+XKyVgsFgZGB/DxgQJ2HCtRuZJWU1RZy9OfHGHt7hwMA2yuLtwzohv33RSHl03/6xEREZGOR7/hOKFBsU3lSvddSWuobWjk5c3HeP7zDKrrGwGY0Dech8clEuHvaXI6ERERkdajcuWEzq13tft4KY12Qx/NkhZhGAbv783niY8Oc6LsNAD9Iv1YfFsSA6L9TU4nIiIi0vpUrpxQzzBvOrtbqaw9Q1pBJUnhPmZHknZu9/FSfvfhIXYfLwUg3NeDh8f3YEJyOC4q7yIiIuIkTF2lc+nSpVgslmaPHj16XNJz33jjDSwWCxMnTmy2/a677jrvNceNG9cK6dsvV6sL/c9eSdBHA+VqHMgrZ9aanUx64St2Hy+lk5uV+aMT2PDbm7i9X1cVKxEREXEqpl+56tWrF+vXr3d87+r6w5GysrJYsGABI0aMuOD+cePG8corrzi+t9lsVx+0gxkUE8CX6afYkVXCzOtjzI4j7UxGUSW/X5fOB/vyAbC6WPhp/wjmj0kgxMfD5HQiIiIi5jC9XLm6uhIaGnrJxzc2NjJt2jSWLVvGl19+SVlZ2XnH2Gy2y3pNZ3RulsBdWSUYhoHFoisM8sOyi2t4ZsMR3v3mBHYDLBb4cd9w5o1KIFaLAIuIiIiTM71cpaenEx4ejoeHB0OHDiU1NZWoqKiLHr98+XKCg4OZPXs2X3755QWP2bhxI8HBwfj7+3PLLbewcuVKunS5+EKldXV11NXVOb6vqKi48jfUTvSL9MPNaqGwoo6cktNEddEsbnJx+eWnefazDNbuzOGM3QBgbK8QHhydQI9Q3bMnIiIiAiaXq5SUFNasWUNiYiL5+fksW7aMESNGsH//fry9vc87fvPmzbz88svs2bPnoq85btw47rjjDmJjY8nMzGTRokWMHz+erVu3YrVaL/ic1NRUli1b1lJvq13o5G6ld1dfvskuY0dWicqVXNCpqjpe2JjJ/2w7Tv0ZOwA3JASxYEwCyRF+5oYTERERaWMshmEYZoc4p6ysjOjoaP77v/+b2bNnN9tXWVlJcnIyzz//POPHjweaJq8oKyvj3XffvehrHj16lLi4ONavX8/IkSMveMyFrlxFRkZSXl6Oj0/H/at86oeHeOmLo9w5KJInJiWbHUfakPKaBv74ZSavbMmi5uxaVYNjAvjtmARSul38KrCIiIhIR1NRUYGvr+8ldQPTPxb4r/z8/EhISCAjI+O8fZmZmWRlZTFhwgTHNru96S/prq6upKWlERcXd97zunXrRmBgIBkZGRctVzabzSknvRgUE8BLXxxlh2YMlLOq687wypZj/PGLo1TUngEgOcKX345J5Ibugbo3T0REROR7tKlyVVVVRWZmJtOnTz9vX48ePdi3b1+zbY899hiVlZWsWrWKyMjIC75mbm4uxcXFhIWFtUrm9uzcwq5HT1ZzqqqOQC/nK5jSpLahkb9uO87zGzMpqa4HIDHEm/ljEhiTFKJSJSIiInIJTC1XCxYsYMKECURHR5OXl8eSJUuwWq1MnToVgBkzZtC1a1dSU1Px8PCgd+/ezZ7v5+cH4NheVVXFsmXLmDRpEqGhoWRmZrJw4ULi4+MZO3bsNX1v7YF/Z3cSQrw4UljFrqwSxvVWAXU29WfsrN2Vw7OfpVNY0fTR2NjAzswb1Z3bksOxap0qERERkUtmarnKzc1l6tSpFBcXExQUxPDhw9m2bRtBQUEAZGdn4+Jy6escW61W9u7dy1/+8hfKysoIDw9nzJgxrFixwik/9ncpBsUEcKSwirW7chmTFKpFX51Eo93gnW9OsGrDEXJKTgMQ7uvBA6O6M6l/BK5WU9cXFxEREWmX2tSEFm3F5dy01t7tyy1n0gtfUd9oZ85NcSwc18PsSNKK7HaDD/fn8/t1R8g8WQ1AoJeN+2+OY2pKFDbXC8+oKSIiIuKs2u2EFnLt9Ynw5YlJfZi/9lue35hJXJAXkwZEmB1LWphhGHx2uIinPz3Cwfymddz8PN2478Y4Zg6NoZO7SpWIiIjI1VK5Eu7oH0HmySqe+zyTR/6xj6gungyKCTA7lrQAwzDYdOQkz6xPZ09OGQBeNldmD49l9ohYfDzczA0oIiIi0oGoXAkAvx2dyNGT1Xy0v4Bf/c9u3p0zTAsLt2OGYbAx7STPbEjn27OlysPNhZnXx3DfDXH4d3Y3N6CIiIhIB6RyJQC4uFh4enJfckpr2H+igtl/2cnbc67XlY12xjAMPk8rYtX6dL7NLQeaStW0lGh+dWM3gr09TE4oIiIi0nFpQosLcKYJLf5dQXkttz+3mcKKOm5MCOLlmQM1c1w7YBgGGw4VsfqzdPb+S6maPiSae2+II8hbs2WKiIiIXInL6QYqVxfgzOUKmmYQ/NlLX1HbYOeu62NY+uNeZkeSizAMg/WHili14Qj7TzRNVNHJzcr0odHcM6KbSpWIiIjIVdJsgXJV+kT48syUftz3169Z81UWccFeTB8SbXYs+ReGYfDpwUJWb0jnQF5TqfJ0/65UBXqpVImIiIhcaypXckHjeofx0NhEnvokjaX/e4CYLp6M6B5kdiynZ7c3lapVG9I5lP9dqZoxNIZ7RsTSRaVKRERExDQqV3JRc26KI/NkFf/4+gRz/vY178wZRnywl9mxnJLdbvDJgQJWbUjncEElAJ3drcy8Poa7R3QjQLP/iYiIiJhO5UouymKxkHpHH7KLa9h1vJTZf9nJO3OG6Rf5a8huN/j4QAGr/6VUedlcmXl9NHcP76Yp1UVERETaEE1ocQHOPqHFvyuuqmPi81vIKTnN4NgA/jo7BXdXzSDYmux2gw/35/PshgzSCr8rVb8cFsPs4bH4eapUiYiIiFwLmi3wKqlcne9IYSWTnv+Kyroz/GxABE/+NBmLxWJ2rA6n0W7w4b58Vm9IJ72oCgDvs6VqlkqViIiIyDWn2QKlxSWEePPsz69j1pqdvLk7l/hgL351Y5zZsTqMRrvB+3vzePazDDLOlSoPV2YNi2XWsFh8PbWYs4iIiEhbp3Ill+ymxGAevy2Jpe8d5ImPDxMT2JmxvULNjtVuGYbB/hMVrDtYwHt78zl2qhoAHw9XZg2P5ZfDYvHtpFIlIiIi0l6oXMllmXl9DBknq/jrtmzmvbGHt349lF7hvmbHajfqz9jZdrSYdQcLWX+okPzyWsc+Hw9X7h7RjbuGxeDjoVIlIiIi0t6oXMllsVgsLJnQi+PFNXyZfoq7/7KLf84dRrCPh9nR2qyK2gY2pp1k3cFCNh4uorLujGNfJzcrNyYEMTophDG9QvBWqRIRERFptzShxQVoQosfVn66gTue30LmyWr6Rvjy918NxcPNanasNiOv7DTrDxWy7mAh244W09D43TAL9LIxqmcwo5NCGBYfqH83ERERkTZMswVeJZWrS5N1qpqJz2+hrKaBW5PDePbO63Bxcc4ZBA3D4HBBJesONhWqfSfKm+2PC+rM6KRQRieFcF2kn9P+O4mIiIi0N5otUK6JmMDOvPiLAUx/eTsf7M0nLsiL+aMTzI51zZxptLMzq5RPDxaw7mAhuaWnHfssFugf5c+YpBBGJ4XQLcjLxKQiIiIici2oXMlVGdKtC//3J31Y+NZeVm9IJy6oM7f362p2rFZTXXeGL4403T/1WVoRZTUNjn02VxdGdA9kdFIIt/QIIcjbZmJSEREREbnWVK7kqk0eGElmURUvfXGUh97aS4S/JwOi/c2O1WLKaur5aH/T1anNGaeoP2N37PP3dGNkz6arUyO6B+LpriElIiIi4qz0m6C0iIXjenD0VDXrDhbyq//ZxbtzhxHh72l2rCvWaDfYnHGKtbtyWHegkPrG7wpVdBdPRp8tVAOi/XG1upiYVERERETaCk1ocQGa0OLKVNed4acvbuVQfgWJId68Ped6vGztq79nnarmrd25vP11brM1qJLCfLg1OYzRSSF0D/bCYtGEFCIiIiLOQLMFXiWVqyuXV3aa25/bwsnKOm7pEcyfZgzE2sZnxqupP8OH+wpYuyuHHcdKHNv9PN2Y2K8rPx0QQe+uWihZRERExBmpXF0llaursyenjCkvbaXujJ27h8fy2G1JZkc6j2EYfJ1dypu7cnnv2zyq6xuBpln+bugexOSBkYxKCsbmqjWoRERERJyZpmIXU/WL9OPpyX25/7Vv+PPmY8QFezF1cJTZsQAoqqjlH9+cYO2uHI6erHZsj+7iyc8GRHBH/wjC/TqZmFBERERE2iuVK2kVtyWHk1lUze/XH2Hxu/txsUDfSD+iAzrTyf3aXg2qP2Pns8NFvLkrh41HTtJob7pY28nNyo/6hDF5YASDYwN0H5WIiIiIXBWVK2k1/2dkPJknq/jfb/N4+O19ju3B3jaiu3gSFdCZ6C6eZx+diQ7wxM/TrcVKTlpBJW/uyuGdb05QXF3v2D4g2p/JAyO4NTm83U24ISIiIiJtl36zlFZjsVh48qfJBHnb2JlVwvHiGspPN1BUWUdRZR07s0rPe463hysxXToT1cWT6ABPRwmLCfQkxNsDlx+YHKP8dAPvfZvHm7ty+Da33LE9yNvGHf278rMBkcQHe7X4exURERER0YQWF6AJLVpPWU09x4trOF5SQ3ZxddPXxTUcL6mmsKLue5/r7upCVEBT6Yrq4tmshOWX17J2Vw4f7y+g7uwiv64uFkb2DGbywEhuTAjSelQiIiIictk0oYW0WX6e7vh5utM30u+8fafrG8kprSHrVDXZJTWOEna8uJoTpaepP2Mno6iKjKKq7/0ZCSFeTB4YycTruhLoZWuldyIiIiIi0pzKlbQZndytJIR4kxDifd6+M4128spqOV5y7mpX03/PlTBXq4Uf9w1n8sBIkiN8NTmFiIiIiFxzKlfSLrhaXYjq0vRxwBHdm+8798lWFSoRERERMZPKlbR7KlUiIiIi0hboDn8REREREZEWoHIlIiIiIiLSAkwtV0uXLsVisTR79OjR45Ke+8Ybb2CxWJg4cWKz7YZh8PjjjxMWFkanTp0YNWoU6enprZBeRERERETkO6ZfuerVqxf5+fmOx+bNm3/wOVlZWSxYsIARI0act+/JJ59k9erVvPjii2zfvp3OnTszduxYamtrWyO+iIiIiIgI0AbKlaurK6GhoY5HYGDg9x7f2NjItGnTWLZsGd26dWu2zzAMnnnmGR577DFuv/12kpOTefXVV8nLy+Pdd99txXchIiIiIiLOzvRylZ6eTnh4ON26dWPatGlkZ2d/7/HLly8nODiY2bNnn7fv2LFjFBQUMGrUKMc2X19fUlJS2Lp1a4tnFxEREREROcfUqdhTUlJYs2YNiYmJ5Ofns2zZMkaMGMH+/fvx9j5/IdnNmzfz8ssvs2fPngu+XkFBAQAhISHNtoeEhDj2XUhdXR11dXWO7ysqKq7g3YiIiIiIiDMztVyNHz/e8XVycjIpKSlER0ezdu3a865MVVZWMn36dP70pz/94EcHL1dqairLli1r0dcUERERERHn0qYWEfbz8yMhIYGMjIzz9mVmZpKVlcWECRMc2+x2O9B031ZaWhqhoaEAFBYWEhYW5jiusLCQfv36XfTnPvLII8yfP9/xfUVFBZGRkVf7dkRERERExIm0qXJVVVVFZmYm06dPP29fjx492LdvX7Ntjz32GJWVlaxatYrIyEjc3NwIDQ1lw4YNjjJVUVHB9u3b+fWvf33Rn2uz2bDZbI7vDcNwPFdERERERJzXuU5wriN8H1PL1YIFC5gwYQLR0dHk5eWxZMkSrFYrU6dOBWDGjBl07dqV1NRUPDw86N27d7Pn+/n5ATTbPm/ePFauXEn37t2JjY1l8eLFhIeHn7ce1veprKwE0NUrEREREREBmjqCr6/v9x5jarnKzc1l6tSpFBcXExQUxPDhw9m2bRtBQUEAZGdn4+JyeRMaLly4kOrqau69917KysoYPnw4H3/8MR4eHpf8GuHh4eTk5ODt7Y3FYrmsn9/Szn1EMScnBx8fH1OzSOvRee74dI6dg85zx6dz7Bx0nju+yznHhmFQWVlJeHj4D76uxbiU61timoqKCnx9fSkvL9fg7sB0njs+nWPnoPPc8ekcOwed546vtc6x6etciYiIiIiIdAQqVyIiIiIiIi1A5aqNs9lsLFmypNlshtLx6Dx3fDrHzkHnuePTOXYOOs8dX2udY91zJSIiIiIi0gJ05UpERERERKQFqFyJiIiIiIi0AJUrERERERGRFqByJSIiIiIi0gJUrtq45557jpiYGDw8PEhJSWHHjh1mR5IWsnTpUiwWS7NHjx49zI4lV+mLL75gwoQJhIeHY7FYePfdd5vtNwyDxx9/nLCwMDp16sSoUaNIT083J6xckR86x3fdddd5Y3vcuHHmhJUrkpqayqBBg/D29iY4OJiJEyeSlpbW7Jja2lrmzp1Lly5d8PLyYtKkSRQWFpqUWK7EpZznm2666bzxfN9995mUWC7XCy+8QHJyMj4+Pvj4+DB06FA++ugjx/7WGMcqV23Y3//+d+bPn8+SJUv4+uuv6du3L2PHjqWoqMjsaNJCevXqRX5+vuOxefNmsyPJVaqurqZv374899xzF9z/5JNPsnr1al588UW2b99O586dGTt2LLW1tdc4qVypHzrHAOPGjWs2tl9//fVrmFCu1qZNm5g7dy7btm1j3bp1NDQ0MGbMGKqrqx3HPPjgg7z33nu8+eabbNq0iby8PO644w4TU8vlupTzDHDPPfc0G89PPvmkSYnlckVERPDEE0+we/dudu3axS233MLtt9/OgQMHgFYax4a0WYMHDzbmzp3r+L6xsdEIDw83UlNTTUwlLWXJkiVG3759zY4hrQgw3nnnHcf3drvdCA0NNZ566inHtrKyMsNmsxmvv/66CQnlav37OTYMw5g5c6Zx++23m5JHWkdRUZEBGJs2bTIMo2ncurm5GW+++abjmEOHDhmAsXXrVrNiylX69/NsGIZx4403Gg888IB5oaTF+fv7G3/+859bbRzrylUbVV9fz+7duxk1apRjm4uLC6NGjWLr1q0mJpOWlJ6eTnh4ON26dWPatGlkZ2ebHUla0bFjxygoKGg2rn19fUlJSdG47mA2btxIcHAwiYmJ/PrXv6a4uNjsSHIVysvLAQgICABg9+7dNDQ0NBvLPXr0ICoqSmO5Hfv383zO3/72NwIDA+nduzePPPIINTU1ZsSTq9TY2Mgbb7xBdXU1Q4cObbVx7NoSYaXlnTp1isbGRkJCQpptDwkJ4fDhwyalkpaUkpLCmjVrSExMJD8/n2XLljFixAj279+Pt7e32fGkFRQUFABccFyf2yft37hx47jjjjuIjY0lMzOTRYsWMX78eLZu3YrVajU7nlwmu93OvHnzGDZsGL179waaxrK7uzt+fn7NjtVYbr8udJ4Bfv7znxMdHU14eDh79+7l4YcfJi0tjX/84x8mppXLsW/fPoYOHUptbS1eXl688847JCUlsWfPnlYZxypXIiYZP3684+vk5GRSUlKIjo5m7dq1zJ4928RkInI17rzzTsfXffr0ITk5mbi4ODZu3MjIkSNNTCZXYu7cuezfv1/3xHZwFzvP9957r+PrPn36EBYWxsiRI8nMzCQuLu5ax5QrkJiYyJ49eygvL+ett95i5syZbNq0qdV+nj4W2EYFBgZitVrPm7GksLCQ0NBQk1JJa/Lz8yMhIYGMjAyzo0grOTd2Na6dS7du3QgMDNTYbofuv/9+3n//fT7//HMiIiIc20NDQ6mvr6esrKzZ8RrL7dPFzvOFpKSkAGg8tyPu7u7Ex8czYMAAUlNT6du3L6tWrWq1caxy1Ua5u7szYMAANmzY4Nhmt9vZsGEDQ4cONTGZtJaqqioyMzMJCwszO4q0ktjYWEJDQ5uN64qKCrZv365x3YHl5uZSXFyssd2OGIbB/fffzzvvvMNnn31GbGxss/0DBgzAzc2t2VhOS0sjOztbY7kd+aHzfCF79uwB0Hhux+x2O3V1da02jvWxwDZs/vz5zJw5k4EDBzJ48GCeeeYZqqur+eUvf2l2NGkBCxYsYMKECURHR5OXl8eSJUuwWq1MnTrV7GhyFaqqqpr9RfPYsWPs2bOHgIAAoqKimDdvHitXrqR79+7ExsayePFiwsPDmThxonmh5bJ83zkOCAhg2bJlTJo0idDQUDIzM1m4cCHx8fGMHTvWxNRyOebOnctrr73GP//5T7y9vR33X/j6+tKpUyd8fX2ZPXs28+fPJyAgAB8fH37zm98wdOhQhgwZYnJ6uVQ/dJ4zMzN57bXX+NGPfkSXLl3Yu3cvDz74IDfccAPJyckmp5dL8cgjjzB+/HiioqKorKzktddeY+PGjXzyySetN46vfkJDaU3PPvusERUVZbi7uxuDBw82tm3bZnYkaSFTpkwxwsLCDHd3d6Nr167GlClTjIyMDLNjyVX6/PPPDeC8x8yZMw3DaJqOffHixUZISIhhs9mMkSNHGmlpaeaGlsvyfee4pqbGGDNmjBEUFGS4ubkZ0dHRxj333GMUFBSYHVsuw4XOL2C88sorjmNOnz5tzJkzx/D39zc8PT2Nn/zkJ0Z+fr55oeWy/dB5zs7ONm644QYjICDAsNlsRnx8vPHQQw8Z5eXl5gaXSzZr1iwjOjracHd3N4KCgoyRI0can376qWN/a4xji2EYxpVXMxEREREREQHdcyUiIiIiItIiVK5ERERERERagMqViIiIiIhIC1C5EhERERERaQEqVyIiIiIiIi1A5UpERERERKQFqFyJiIiIiIi0AJUrERGRFrRx40YsFgtlZWVmRxERkWtM5UpERERERKQFqFyJiIiIiIi0AJUrERHpUOx2O6mpqcTGxtKpUyf69u3LW2+9BXz3kb0PPviA5ORkPDw8GDJkCPv372/2Gm+//Ta9evXCZrMRExPD008/3Wx/XV0dDz/8MJGRkdhsNuLj43n55ZebHbN7924GDhyIp6cn119/PWlpaa37xkVExHQqVyIi0qGkpqby6quv8uKLL3LgwAEefPBBfvGLX7Bp0ybHMQ899BBPP/00O3fuJCgoiAkTJtDQ0AA0laLJkydz5513sm/fPpYuXcrixYtZs2aN4/kzZszg9ddfZ/Xq1Rw6dIiXXnoJLy+vZjkeffRRnn76aXbt2oWrqyuzZs26Ju9fRETMYzEMwzA7hIiISEuoq6sjICCA9evXM3ToUMf2u+++m5qaGu69915uvvlm3njjDaZMmQJASUkJERERrFmzhsmTJzNt2jROnjzJp59+6nj+woUL+eCDDzhw4ABHjhwhMTGRdevWMWrUqPMybNy4kZtvvpn169czcuRIAD788ENuvfVWTp8+jYeHRyv/K4iIiFl05UpERDqMjIwMampqGD16NF5eXo7Hq6++SmZmpuO4fy1eAQEBJCYmcujQIQAOHTrEsGHDmr3usGHDSE9Pp7GxkT179mC1Wrnxxhu/N0tycrLj67CwMACKioqu+j2KiEjb5Wp2ABERkZZSVVUFwAcffEDXrl2b7bPZbM0K1pXq1KnTJR3n5ubm+NpisQBN94OJiEjHpStXIiLSYSQlJWGz2cjOziY+Pr7ZIzIy0nHctm3bHF+XlpZy5MgRevbsCUDPnj3ZsmVLs9fdsmULCQkJWK1W+vTpg91ub3YPl4iICOjKlYiIdCDe3t4sWLCABx98ELvdzvDhwykvL2fLli34+PgQHR0NwPLly+nSpQshISE8+uijBAYGMnHiRAB++9vfMmjQIFasWMGUKVPYunUrf/jDH3j++ecBiImJYebMmcyaNYvVq1fTt29fjh8/TlFREZMnTzbrrYuISBugciUiIh3KihUrCAoKIjU1laNHj+Ln50f//v1ZtGiR42N5TzzxBA888ADp6en069eP9957D3d3dwD69+/P2rVrefzxx1mxYgVhYWEsX76cu+66y/EzXnjhBRYtWsScOXMoLi4mKiqKRYsWmfF2RUSkDdFsgSIi4jTOzeRXWlqKn5+f2XFERKSD0T1XIiIiIiIiLUDlSkREREREpAXoY4EiIiIiIiItQFeuREREREREWoDKlYiIiIiISAtQuRIREREREWkBKlciIiIiIiItQOVKRERERESkBahciYiIiIiItACVKxERERERkRagciUiIiIiItICVK5ERERERERawP8HVg+T4iQe4MUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# excise 3.4 transformer neural network (optional)\n",
    "learning_rate = 0.002\n",
    "num_epochs = 30\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "\n",
    "for number_decoder_layers in range(0,1,3):\n",
    "    # Train model\n",
    "    model_6 = transformer(vocab_size, embedding_dim, hidden_dim, num_layers, number_decoder_layers).to(device)\n",
    "    train_losses_6, valid_losses_6, train_perplexities_6, valid_perplexities_6 = train(model=model_6,\n",
    "                                                                            model_type='LSTM',\n",
    "                                                                                train_data=train_data,\n",
    "                                                                                valid_data=valid_data,\n",
    "                                                                                vocab_size=vocab_size, \n",
    "                                                                                batch_size=batch_size,\n",
    "                                                                                seq_len=seq_len,\n",
    "                                                                                num_epochs=num_epochs, \n",
    "                                                                                learning_rate=learning_rate, \n",
    "                                                                                device=device,\n",
    "                                                                                clip=0.5\n",
    "                                                                                )\n",
    "    ax[0].plot(train_losses_6, label='train_loss'+str(number_decoder_layers))\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].legend(loc='upper right')\n",
    "\n",
    "    ax[1].plot(valid_losses_6, label='valid_loss'+str(number_decoder_layers))\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].legend(loc='upper right')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 4.4686 test_perplexity = 87.2302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'after years of a two dealers an the officers the ruling taxes a the what hurricane it his <unk> powerful at the a went <unk> town tuesday <unk> july the the oct. telling N commissioned initial speculation resigning revamping ford a two a being posting the each the salomon N'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_loss_6, test_perplexity_6 = evaluate(model_6, 'LSTM', test_data, vocab_size, batch_size, seq_len, device)\n",
    "print('test_loss =', '{:.4f}'.format(test_loss_6), 'test_perplexity =', '{:.4f}'.format(test_perplexity_6))\n",
    "\n",
    "# Generate text\n",
    "sample(model_6, 'LSTM', int_to_word, 50, vocab, device, start='after years of')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_losses_6, label='train_loss')\n",
    "plt.plot(valid_losses_6, label='valid_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
